{"pages":[],"posts":[{"title":"CHOMP_and_related_works","text":"​ 最近深感自己知识储备的不足，面对一个问题可能前人已经解决了，但是却自己根本摸不着头绪。准备好好开始看paper来增加自己的知识库储备。实验室的师兄师姐们都非常厉害，分分钟可以丢给我很多paper去看，希望借此机会可以增加自己研究的insight。 CHOMP​ CHOMP是一篇如雷贯耳的文章。刚进领域的时候就经常看见CHOMP,STOMP等planner，非常如雷贯耳，最近项目可能要添加进一些planner的因素，所以要先从这篇09年ICRA的祖师爷开始看起，注意同一批人在13年再次对CHOMP工作做了细致的讲解、提供了和RRT*的对比实验，并且提供了扩展到含额外约束的轨迹规划问题上。 ​ 其实CHOMP的思路比较直观，它有一个目标函数$U(\\xi)=f_{prior}(\\xi)+\\lambda f_{obstacle}(\\xi)$，最终目标就是优化这个目标函数得到轨迹$\\xi$。即有：$$\\xi^* = \\mathop{\\arg\\min}_{\\xi}U(\\xi)$$​ 其中，轨迹$\\xi(t)$可以认为是一个[0,1]$\\rightarrow R^m$的函数，我们把时间归一化到0~1之间，也就是说$\\xi(0)=q_{init}$以及$\\xi(1)=q_{goal}$。所以$U(\\xi)$就可以简单地认为是一个关于轨迹$\\xi$的损失函数。当它取到最小的时候我们即得到了一个最优的colision-free的轨迹。 动力学损失函数​ CHOMP的目标函数主要考虑了动力学带来的cost以及碰撞所带来的cost。我们可以先看动力学所带来的cost。$f_{prior}(\\xi)=\\displaystyle\\frac{1}{2}\\int^1_0 \\vert|\\frac{d}{dt}\\xi(t)||^2 dt$，这一项是直接对轨迹的速度的平方做积分，也可以扩展到加速度和加加速度的情况，是用来鼓励轨迹更加顺滑的。 碰撞损失函数​ 第二个损失函数就是说CHOMP希望规划出一条collision-free的轨迹，所以必须要把场景中的所有障碍物全部考虑进去。我们考虑机械臂外表面上的点集$B\\subset R^3$, u是其上一点。在某个构型下，我们显然可以通过构型的前向动力学信息以及机械臂上的相对坐标计算出其在欧式空间中的瞬时绝对坐标，我们记为$x(\\xi(t),u)$，有了这个坐标以后，那我们需要得到这一点离障碍物的距离，我们希望尽可能提升这个距离。所以我们引入了SDF距离（有向距离场），这个在图像渲染中用到的比较多，其实就是这个点到一个物体表面的最短距离。有了这个距离度量函数，我们可以构造出大于0的损失函数来尽可能最大化这个距离度量。所以我们有碰撞损失函数：$$\\displaystyle f_{obstacle}=\\int_0^1\\int_B c_{obstacle}(x(\\xi(t), u))|\\frac{d}{dt}x(\\xi(t),u)|dudt$$ ​ 在上式中，其实对机械臂的表面积做积分这件事情是比较显然的，但是之前一直很难理解为什么要乘上这个速度函数$x’(\\xi(t),u)=v(\\xi,u)$。理解起来也不复杂，因为我们的轨迹的时间归一化到了0~1之间。所以如果不乘上这个速度因子的话，就有可能导致两条运动速度不同而位置相同的轨迹算出来的损失函数是相等的，但是我们希望让以较快速度通过的有更大的损失，可能是因为较大的速度更容易碰撞时产生实际的负面影响，所以加上了速度因子后，就把时间归一化带来的多解问题消除掉了，可以理解为是一个保序变换。 优化过程​ 论文使用迭代的方式来更新轨迹$\\xi_k$，使用一阶泰勒展开$U(\\xi)\\approx U(\\xi_k)+g_k^T(\\xi-\\xi_k)$，其中$g_k=\\nabla U(\\xi_k)$。$$\\xi_{k+1}=\\xi_k-\\frac{1}{\\lambda}M^{-1}g_k$$ 推广到含约束的优化过程​ 有些轨迹创建过程可能需要额外的约束，比如传递水杯的时候我们可能希望水不要洒出来。所以在2013年的文章中，作者对把原先的CHOMP泛化到比较普遍的约束问题上。首先作者假设所有约束都可以按照轨迹在希尔伯特空间上非线性的可微分向量函数来定义：$H:\\Xi\\rightarrow R^k$，其中的$H(\\xi)=0$就是所有满足要求的约束的轨迹。 ​ 所以其实就是在我们之前的迭代过程中，我们此时迭代改写为：$$\\xi_{k+1}=\\xi_k-\\frac{1}{\\lambda}M^{-1}g_k \\\\s.t. H[\\xi]=0$$​ 为了得到一个具体的更新规则，我们在$\\xi_i$处对函数H进行一阶泰勒展开，也就是$H(\\xi)\\approx H(\\xi_i)+\\frac{\\partial}{\\partial\\xi}H(\\xi_i)(\\xi-\\xi_i)=C(\\xi-\\xi_i)+b$其中C是约束函数的Jacobian，而$b=H(\\xi_i)$，我们可以把带约束的迭代问题转化为拉格朗日约束下的梯度下降问题，具体过程不再描述。最终我们得到的更新规则为：$$\\xi_{k+1}=\\xi_k-\\frac{1}{\\lambda}A^{-1}g_k+\\frac{1}{\\lambda}A^{-1}C^T(CA^{-1}C^T)^{-1}CA^{-1}g_{k}-A^{-1}C^T(CA^{-1}C^T)^{-1}b$$ ​ 其实就是说在带约束的更新规则下，它先以无约束的方式优化一步，然后把A投影到穿过$\\xi_t$的超平面上，并且这个超平面与我们的约束函数在$\\xi_i$处的一阶泰勒展开的近似超平面$C(\\xi-\\xi_i)+b=0$平行。最终，它消除掉了两个超平面之间的平移量，使得下一步迭代$\\xi_{i+1}$更加接近于约束函数$H(\\xi)=0$上。 ​ 所以其实我们如果能够形式化定义出这个$H(\\xi)$的具体形式，那么找到满足这个约束的轨迹也只是一个优化的过程。 [RSS2020] 在线抓取合成和优化的轨迹创建​ 摘要的大致意思就是说轨迹创建和Grasp预测通常是分别处理的，这篇文章希望提出一个整合两个planning问题的方法。这篇文章整合出了一个轨迹优化方法和在线抓点合成/选取的一个结合办法，也就是在线学习并且挑选出最终抓点的6D pose。并且证明了在复杂环境中是可以鲁棒并且高效地创建出motion plan的。 ​ 避障自然就是使用了CHOMP算法，不再过多叙述。和这篇文章主打的Grasp相关的概念：给定一个场景中的多个物体，定义了可行的目标物体抓取集合$G\\subseteq Q\\subseteq R^d$。这样我们的优化问题就变成了：$$\\xi^* = \\mathop{\\arg\\min}_{\\xi}f_{motion}(\\xi) \\\\s.t.\\xi(1)\\in G$$​ 其中的损失函数$f_{motion}$自然就是CHOMP中的$U(\\xi)$，即$f_{motion}(\\xi)=f_{obstacle}(\\xi)+\\lambda f_{prior}(\\xi)$。所以这一步其实就是CHOMP含约束优化的一个实例。约束函数定义为$H(\\xi_i)=\\xi_i-g=0$，我们可以使用CHOMP含约束的更新规则来进行更新。我们可以定义$f_g(\\xi_i)$是在第i次迭代时，使用g作为目标抓点的损失函数。稍微有一点点不同的地方在于在不同迭代中选择的抓点g可能是不一样的。 在线学习抓点选择​ 如果我们直接使用CHOMP求解带约束g的优化函数，那么我们并没有考虑到一些轨迹上的特性，比如这篇ISRR2011所提到的损失函数可能陷入局部最优的情况。 ​ 如上图所示，这个例子如果我们梯度下降的起点选择的不好就很容易会陷入局部最优的情况，这是从机器学习角度是很容易理解的事情。不过，我们可以通过额外要求满足轨迹的一些Attributes来限定梯度下降的区域（起点）就是在那个可以达到局部最优的位置。 ​ 如上图所示，如果我们规定，所有轨迹都要满足“都从冰箱上方过去”，我们就可以把轨迹的优化范围限定在如右图所示的，可以到达全局最优点的位置。形式化地来说，我们可以定义一个好的轨迹应当满足的特质：$\\tau:S\\to A(\\Xi,S)$，其中S是抽象的任务描述，而$\\Xi$是可以解决任务的轨迹的集合，$A(\\Xi,S)$就代表了轨迹的属性，比如在上例中就是从冰箱门上方运动过去。我们就可以隐式地描述出原先轨迹集合$\\Xi$的一个子集：$\\Xi_A\\subseteq \\Xi$，我们从这个好轨迹集合中选取第二阶段优化的初始值，这样就可以避免局部最优的情况。所以总体流程就为$S\\to A(\\Xi,S)\\to \\xi\\in\\Xi_A\\to\\xi^*$。 ​ 我们继续回到这篇文章，我们如果想优化$f_g(\\xi_i)$，我们希望能够最大化motion generation success。文章使用了把第i次迭代时的Goal Set描述为了概率分布$p_i$，那么这一步其实就是在迭代计算$p_{i+1}$的过程，那么第i+1次迭代的时候，我们选取的目标抓点就是$g_{i+1}=\\arg\\max(p_{i+1})$，文章使用的赌博机算法（Bandit Algorithm）不再过多叙述，是一个简单的RL算法。 在线抓取合成​ 这一部分主要应用了CASE2018所提出的ISF算法，也就是从初始的有限抓取集合G出发，通过优化的方式在线合成更多质量更高的抓取点。优化的目标函数为：$$f_{grasp}(g)=f_{isf}(g)+\\gamma f_{collision}(g)$$​ ISF主要就是最大化夹爪和物体的接触面积，这件事情是通过对夹爪上的采样表面点和法向量，并且优化接触点位置处的法向量和距离得到的，ISF算法因为本身也设计到很多理解和推导，暂时不在这里描述。 ​ 并且此处有：$f_{collision}(g)=f_{hand_obstacle}+\\beta f_{obstacle}(g)$，这里的$f_{obstacle}$就是在CHOMP中的碰撞损失，而$f_{hand_obstacle}$指的就是在抓取的过程中，我们不希望robot的手部和待抓取的物体会出现任何的碰撞。 ​ 最终这篇文章提出的算法如下，每次迭代的时候，先用带约束的CHOMP创建轨迹，然后迭代更新Goal Set的分布，然后选取其中成功率最大的一个grasp作为goal，然后使用C-Space ISF方法来优化这个选取的grasp，使其抓取的成功率更大。","link":"/2021/12/09/CHOMP-and-related-works/"},{"title":"MoveIt Setup Assistant","text":"​ This article shows how to configure a custom arm in MoveIt. 1. Load the model​ Make sure that your moveit is correctly configured before moving on the turorial. And this article is mainly adapted from ROS MoveIt tutorial, but I configure a flexiv robot not a franka-panda. ​ First, use the following command to open the moveit setup assistant GUI. 1roslaunch moveit_setup_assistant setup_assistant.launch ​ Press “Create New MoveIt Configuration Package”, and choose the .urdf file. ​ After importing the urdf model, you can check the robot model in the right. If there something wrong, like missing part, or some red error shown in the bash, try to check the correctness of the urdf model. Those urdf models that are not following the ROS conventions can not be correctly processed by the MoveIt setup assistant. 2. Compute the self-collisions matrix​ Just press the button, if there are no special needs. 3. Define virtual joints​ Virtual joints are used primarily to attach the robot to the world. ​ For our flexiv model, we can find in urdf there is a joint0 serving as virtual joints in the urdf, ​ so we don’t add one more joint here, but if we fail at last, we may return back here to move the virtual joint in the urdf. 4. Define planning groups​ Planning groups are necessary for us to plan in MoveIt. Here we first define a planning group called arm, which use kdl_kinematics_plugin. ​ And we need to press “add joints” to configure the planning group. ​ Joint 0 to 7 are added into the planning group, because later the kinematic solver will need the 7 joints to calculate solutions. ​ Next, we should configure a planning group called “hand”, with no ik solvers and all links contained by the gripper. The following picture is the result when we successfully configure both. 5. Define robot poses​ We can add some useful configurations to use in future. 6. Define end effectors 7. Add passive joints​ The passive joints tab is meant to allow specification of any passive joints that might exist in a robot. These are joints that are unactuated on a robot (e.g. passive casters.) This tells the planners that they cannot (kinematically) plan for these joints because they can’t be directly controlled. The Panda does not have any passive joints so we will skip this step.[1] Not quiet sure, I leave empty here as well. 8. 3D perception​ Not used here. 9. Simulation with Gazebo​ Gazebo is not used here. 10. ROS control​ Clicking the “Auto add …”, then we have ​ We should edit both one. ​ Change the first to a joint position controller. 11. Add author information​ Do what it says. 12. Configuration Files​ Just generate the package. 13. Visualize the robot in RViz​ We can visualize the robot in RViz by the following command after building the newly created package. 1roslaunch panda_moveit_config demo.launch ​ Everything seems to be in the right place. You can refer to RViz tutorial for more details.","link":"/2021/11/05/MoveIt-Setup-Assistant/"},{"title":"SE3353-assignment10","text":"简单总结一下这次作业。使用Docker-Compose封装SpringBoot应用、Nginx、MySQL、MongoDB和Redis。 12345678910111213141516任务要求：请你根据上课内容，针对你在E-BookStore项目中的数据库设计，完成下列任务：1.请你参照课程样例，构建你的E-BookStore的集群，它应该至少包含 1 个nginx实例(负载均衡) + 1 个Redis实例(存储session) + 2 个Tomcat后端实例。(4分）2.所使用的框架不限，例如可以不使用nginx而选用其他负载均衡器，或不使用Redis而选用其他缓存工具。3.参照上课演示的案例，将上述系统实现容器化部署，即负载均衡器(或注册中心和Gateway)、缓存和服务集群都在容器中部署。 (1分）– 请提交一份Word文档，详细叙述你的实现方式；并提交你的工程代码。评分标准：– 能够正确地部署和运行上述系统，在验收时需当面演示。– 部署方案不满足条件或无法正确运行，则视情况扣分。 ​ 整体框架可以参考这个博客，但是真正debug起来前前后后也是花了一两周时间，只能说还是docker用得太少了，以及docker集群中的错误定位在没有经验的情况下还是非常痛苦的，包括思维的“连续性”这件事情，因为每次拾起来这个任务总需要花费一些建立去retrieve之前的记忆，但每次能够挤出来的时间又是碎片时间，导致retrieve记忆的效率很低。相关代码在这个仓库。 SpringBoot的Docker化​ 也就是把一个Spring Boot工程打包成Docker镜像，这件事情网上有各种各样奇奇怪怪的教程让我去安装什么插件，配置什么端口。这些都是不必要的事情。首先明确我们整个过程打包出来的就只有一个单纯的jar包，这个jar包是平台无关的，我们只需要根据这个jar包封装成一个docker镜像并且启动起来实例就可以了。所以其实我们只需要在bookstore_backend-1.0-SNAPSHOT.jar的相同目录下新建一个Dockerfile文件，写入如下代码： 1234567891011#基于哪个镜像FROM java:8 # 拷贝文件到容器，也可以直接写成ADD microservice-discovery-eureka-0.0.1-SNAPSHOT.jar /app.jarADD bookstore_backend-1.0-SNAPSHOT.jar /app.jar # 开放9090端口EXPOSE 9090 # 配置容器启动后执行的命令ENTRYPOINT [&quot;java&quot;,&quot;-Djava.security.egd=file:/dev/./urandom&quot;,&quot;-jar&quot;,&quot;/app.jar&quot;] MySQL的Docker部署​ 理论上这是一个非常非常非常常见的应用场景，但是就是在这一步卡了我非常久非常久的时间。 ​ 首先是，因为是从容器中启动MySQL，最开始容器中是没有数据库的，所以需要有一个初始化过程。所以先要从我们原先的生产环境中把数据库拿出来，这里就直接使用这篇博客提到的全量备份方法： 1mysqldump -uroot -p123123 --databases bookstore&gt;./init/init.sql ​ 但是根据教程把在docker-compose中绑定- ./init:/docker-entrypoint-initdb.d后，启动以后还是没有bookstore数据库。根据这篇回答，在command中添加–init-file /docker-entrypoint-initdb.d/init.sql，这才让我真正能够在启动容器的时候初始化我的数据库。 ​ 接下来就是需要让我们的SpringBook容器真正连上这个容器中的MySQL，这才算是打通了集群化部署的工作流程。 ​ 总体踩过的坑如下： 在docker-compose的MySQL的environment中，必须设置MYSQL_ROOT_HOST: ‘%’，根据MySQL官网的叙述，如果不设置这一条，MySQL容器只会允许‘root‘@’localhost’，也就是容器内部的查询请求通过，但是我们配置出来的MySQL显然是需要让内网的别的容器访问的。 在docker-compose的MySQL的environment的MYSQL_ROOT_PASSWORD中，似乎不支持纯数字密码。（至少我设置123456登录不上去，设置为纯字母密码才可以）。 在docker-compose的MySQL的environment中，可能需要设置MYSQL_DATABASE。这个变量指定了我们在创建镜像时需要创建的数据库，如果我们还设置了MYSQL_USER，会同时授予User访问这个数据库的权限。 在Spring Boot的application.properties中，如果我们之前设置的是访问路径是‘jdbc:mysql://localhost:3306/bookstore?autoReconnect=true&amp;useSSL=false‘，我们需要把localhost修改为docker-compose中service的名字，否则这样SpringBoot会调用localhost自己容器中的3306端口，但是它自己的容器里是没有MySQL的。 在Debug过程中，其实看到最多的是如下的报错信息： 123456java.sql.SQLNonTransientConnectionException: Could not create connection to database server. Attempted reconnect 3 times. Giving up.#此处省略几十行 at com.bookstore.Application.main(Application.java:123) ~[classes/:na]Caused by: com.mysql.cj.exceptions.CJException: Access denied for user 'root'@'localhost' (using password: YES)#此处省略几十行 ... 50 common frames omitted ​ 现在回过头来看其实当时主要还是看到那么多行报错一直在搜第一行的错误原因，而中间这一段才是真正的原因，也就是本机尝试登录root账户失败，那显然就是密码错误。 ​ 总体的和MySQL相关的配置文件如下： 123456789101112131415161718192021222324252627app_db: image: mysql:8.0.23 hostname: testt command: # MySQL8的密码验证方式默认是 caching_sha2_password，但是很多的连接工具还不支持该方式 # 就需要手动设置下mysql的密码认证方式为以前的 mysql_native_password 方式 --default-authentication-plugin=mysql_native_password --character-set-server=utf8mb4 --collation-server=utf8mb4_general_ci --init-file /docker-entrypoint-initdb.d/init.sql #attention here # docker的重启策略：在容器退出时总是重启容器，但是不考虑在Docker守护进程启动时就已经停止了的容器 restart: unless-stopped environment: MYSQL_ROOT_PASSWORD: root_password # root用户的密码 MYSQL_DATABASE: bookstore MYSQL_ROOT_HOST: '%' ports: - 3306:3306 expose: - 3306 volumes: - ./db:/var/lib/mysql - ./conf:/etc/mysql/conf.d - ./log:/logs - ./init:/docker-entrypoint-initdb.d 其余部分的容器化​ 其他的部分（包括MongoDB,Nginx,Redis,Neo4j）的容器部署大差不差，这些一共加起来可能也就花了半小时时间。主要还是配置第一个MySQL的时候踩了太多太多的坑。 ​ 最后放一些结果图。 ​ 我们可以看到Nginx确实把请求轮询分发给了两台Server。 ​ 并且在响应的结果中，我们确实看到了MySQL和MongoDB组装成功响应。","link":"/2021/12/18/SE3353-assignment10/"},{"title":"SE3353_assignment4","text":"简单总结一下这次作业。基于Lucene的全文搜索功能和WebService的封装。 在你的项目中增加基于Solr或Lucene的针对书籍简介的全文搜索功能，用户可以在搜索界面输入搜索关键词，你可以通过全文搜索引擎找到书籍简介中包含该关键词的书籍列表。为了实现起来方便，你可以自己设计文本文件格式来存储书籍简介信息。例如，你可以将所有书籍的简介信息存储成为JSON对象，包含书的ID和简介文本，每行存储一本书的JSON对象。 请将上述全文搜索功能开发并部署为Web Service。 一、开发基于Lucene的全文搜索功能我们实现一个ApplicationRunner接口，这样它会在SpringBoot启动的时候自动执行一次，把数据库中的书籍及其间接建立索引。 为了测试这个功能，我们添加了一个用于测试的Controller。 我们分别输入“你”和“奥秘”作为关键词，我们可以清楚地看见确实返回了所有包含关键词的书籍。 二、把全文搜索功能整合到WebService中 如果说第一部分在十分钟内就可以基本上完成的话，这部分就非常冗长了。首先是WebService的选型，我首先选了Jax-ws作为后端webservice的实现。后面实现起来一切正常，但是当我想整合前端时出现了巨大的问题。简而言之就是，前端因为要组装text/xml作为SOAP协议的body，但是在firefox和Chrome中，这属于“非简单请求”，会自动先发送一个preflight。 非简单请求的CORS请求，会在正式通信之前，增加一次HTTP查询请求，称为”预检”请求（preflight）”预检”请求用的请求方法是OPTIONS，表示这个请求是用来询问的。头信息里面，关键字段是Origin，表示请求来自哪个源。浏览器先询问服务器，当前网页所在的域名是否在服务器的许可名单之中，以及可以使用哪些HTTP动词和头信息字段。只有得到肯定答复，浏览器才会发出正式的XMLHttpRequest请求，否则就报错。 就是因为这个OPTION请求让Jax-ws用不了，发请求的时候SpringBoot会报出： com.sun.xml.internal.ws.server.http : 无法处理 HTTP 方法: OPTIONS 我们需要定义interceptor去拦截这个OPTIONS请求，并且返回一个200:OK，但是找了半天不知道这个接口在哪里。使用SpringBoot的HandlerInterceptor也拦不到这个OPTIONS请求，陷入僵局。 后来看到了这篇文章js调用webservice接口时后台无法处理OPTIONS请求的解决方法，只能弃用Jax-ws，启用cxf。 拦截器参考网上的抄了一份过来，简单而言就是如果拦截到的是preflight，那就设置对应的Header后返回200:OK。 具体的webservice倒显得很容易了，如下是Service层的代码，还有一个CXFConfig类。 我们可以在对应暴露出的url后添加?wsdl，来得到这个webservice对应的wsdl文件。 理论上前端应该是要parse这个wsdl文件然后生成对应的SOAP消息的，不过限于时间，和具体业务其实不需要这么大的灵活度，所以就没做，固定了前端的格式。 值得一提的是，前端的SOAP消息格式是使用了SOAPUI这个软件，传入wsdl文件后自动生成出来的，节省了我很多debug的时间。 剩下的无非就是前端的一些工作了，封装出对应的SOAP消息生成和解析器即可。 搜索“你”和“奥秘”，结果与之前我们在controller中测试的结果完全相同。任务结束。","link":"/2021/10/20/SE3353-assignment4/"},{"title":"SE3353-assignment8","text":"简单总结一下这次作业。基于MongoDB和Neo4j的存储功能。 ​ 1.将你认为合适的内容改造为在MongoDB中存储，例如书的产品评价或书评。你可以参照课程样例将数据分别存储在MySQL和MongoDB中，也可以将所有数据都存储在MongoDB中，如果采用后者，需要确保系统功能都能正常实现，包括书籍浏览、查询、下订单和管理库存等。​ 2.为你的每一本图书都添加一些标签，在Neo4J中将这些标签构建成一张图。在系统中增加一项搜索功能，如果用户按照标签搜索，你可以将Neo4J中存储的与用户选中的标签以及通过2重关系可以关联到的所有标签都选出，作为搜索的依据，在MySQL中搜索所有带有这些标签中任意一个或多个的图书，作为图书搜索结果呈现给用户。 ​ 最近有点眼高手低，有些技术以前用过就自以为可以不必记录下来了。要遏制住这股风气，因为我并不了解任何工具的底层、实现也全靠CSDN，哪怕为了第二次复现起来、第二次快速回忆起现在做的事情，也需要详尽地写下目前的操作。 PART1：MongoDB首先是安装并运行MongoDB，在安装目录下，先运行mongod.exe，再运行mongo.exe即可看到如下的命令行： 123456789101112131415161718192021222324---&gt; 2 + 24&gt; db.runoob.insert({x:10})WriteResult({ &quot;nInserted&quot; : 1 })&gt; db.runoob.find(){ &quot;_id&quot; : ObjectId(&quot;6190c5a1e7f64d1430e95cde&quot;), &quot;x&quot; : 10 }&gt; db.auth(&quot;root&quot;,&quot;123456&quot;)Error: Authentication failed.0&gt; use adminswitched to db admin&gt; db.createUser({user:&quot;root&quot;,pwd:&quot;1234&quot;,roles:[{&quot;role&quot;:&quot;readWrite&quot;,&quot;db&quot;:&quot;demo&quot;}]})Successfully added user: { &quot;user&quot; : &quot;root&quot;, &quot;roles&quot; : [ { &quot;role&quot; : &quot;readWrite&quot;, &quot;db&quot; : &quot;demo&quot; } ]}&gt; db.auth(&quot;root&quot;,&quot;1234&quot;)1 ​ 我们在书店的DAO层中添加如下代码： 123456789101112131415@Overridepublic Book addRemark(Long book_id, String remark) { Book book = findOne(book_id); Remark remarks = book.getRemark(); if (remarks == null) { remarks = new Remark(book.getId()); } else { remarkRepository.delete(book.getRemark()); } remarks.addRemark(remark); book.setRemark(remarks); save(book); return book;} ​ 因为MongoDB不支持相同id的document的覆盖操作，所以我们每次取出来就删除掉，然后再把新的评论加上后再save到collection中，唯一的问题可能就是在delete后save前如果server crash了，可能会导致这个书籍的评论消失。可以通过额外的数据备份机制来避免这个情况。 ​ 使用ROBO3T可以看到，确实我们每次添加一条评论，就自动添加到Collection对应book id的document下了。 PART2：Neo4j​ 按照Neo4j desktop很快，但是把它在SpringBoot中调通用了很大波折。我发现只要加入spring-boot-starter-data-neo4j这个依赖，启动后就会报如下的错。 12345678org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'reactiveNeo4jTemplate' defined in class path resource [org/springframework/boot/autoconfigure/data/neo4j/Neo4jReactiveDataAutoConfiguration.class]: Post-processing of merged bean definition failed; nested exception is java.lang.IllegalStateException: Failed to introspect Class [org.springframework.data.neo4j.core.ReactiveNeo4jTemplate] from ClassLoader ...Caused by: java.lang.ClassNotFoundException: reactor.util.context.ContextView at java.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[na:1.8.0_91] at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[na:1.8.0_91] at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) ~[na:1.8.0_91] at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[na:1.8.0_91] ... 26 common frames omitted ​ 依赖也没冲突，代码都没开始写，就出这个问题，真的卡了好几个小时。后来不抱希望地看到了这个issue，添加了依赖： 123456&lt;!-- https://mvnrepository.com/artifact/io.projectreactor.netty/reactor-netty --&gt;&lt;dependency&gt; &lt;groupId&gt;io.projectreactor.netty&lt;/groupId&gt; &lt;artifactId&gt;reactor-netty&lt;/artifactId&gt; &lt;version&gt;1.0.13&lt;/version&gt;&lt;/dependency&gt; ​ 居然就能用了，真是百思不得其解。 可见成功初始化，创建了由书籍和标签构成的一张图。 我们如下extend Neo4jRepository 123456public interface BookNodeRepository extends Neo4jRepository&lt;BookNode, Long&gt; { BookNode findByName(String name); BookNode findByTagNodesName(String name); @Query(&quot;MATCH (p:BookNode) -[:contain]-&gt;(p1:TagNode)&lt;-[:contain]- (p2:BookNode) where p.name=~ $n return p2&quot;) List&lt;BookNode&gt; findByTwoRelationship(@Param(&quot;n&quot;) String n);} 和我们在Neo4j Desktop中查询到的完全一样 Customizing Query by @Query annotation","link":"/2021/11/22/SE3353-assignment8/"},{"title":"SE335_Compliers_lab2","text":"总而言之前前后后好几天时间都在搞这个lab，不过主要花的时间可能是大半天时间。为了留下点东西，姑且总结一下这个lab。 其实这个lab可以说是烦而不难，基本上搞清楚flex c++的执行流，就完全可以干掉这个lab。 上面是一些无关痛痒的指令，算是第一步就可以很轻易写出来的东西。 一、处理嵌套注解 嵌套注解的处理其实从原理上就和括号匹配一样，至少要维护一个整形作为匹配的进度（/加一、/减一，最终为0时匹配完毕）。但是这样实现就要区分出在匹配完毕时（非注解情况下）和匹配进行时（注解情况下）的不同执行流，需要更改全部代码。其实，仔细查看lab2的提示和flex c++的文档就可以发现它提供了start condition来处理不同情况下的处理，这是一个很好用的工具。如上图所示，一旦匹配到了开始符号/*，那么自动push一个COMMENT状态即可；一旦匹配到了结束符号，那么就从栈中pop一个状态。 二、字符串的处理这部分是我debug了最久的地方，因为经常会出现匹配上了，但是开始的位置和标准答案差一个的情况、或者字符串后的各种符号平移了几位的情况，这是因为多种原因造成的： \\1. 比如A=”5”的情况，在匹配第一个”的时候，如果没有adjust，最终的开始位置就会在等号处。 \\2. 匹配转义字符时，不能根据最终转义完成的字符串的长度来计算adjust，这样会导致adjust数量过少。 首先是匹配普通字符串的情况，lex代码如下： 由于adjust函数会自动根据matched的字符串的长度来做调整位置，所以我们自己添加了一个adjustByLen，里面添加了对转义字符的offset。接下来是对转义字符的支持，主要可以分为四类： \\1. ^A, ^B, …., ^Z，具体其含义我们不细究，反正查ASCII表可知其值为1~26。 \\2. \\xxx，其中x为0~9的数字，也就是直接指定ASCII码。 \\3. \\n, \\t, \\”等，直接转义为对应字符即可。 \\4. \\跟了一个换行符的情况，我们需要支付字符串跨行输入，这个下一节再说。 总体逻辑就是trim我们的matched string，把转义使用的\\x都修剪掉，换成真的对应的转义字符。注意，我们需要累积计算我们的offset，这样我们才能知道最终我们需要移动多少位。 接下来是例子52中出现的这种情况： 我们自然也需要给予支持，也就是+一个换行符，自动进入ignore状态，等到读到下一个\\后，才退出到正常的字符串处理状态。注意ignore了多少个字符也需要添加进offset中。","link":"/2021/10/17/complier-lab2/"},{"title":"behavior_tree_pre","text":"A internal talk of MVIG lab","link":"/2021/10/20/behavior-tree-pre/"},{"title":"complier-lab3","text":"​ This lab comes from SE3355 *Compilers* lab3, which requires me to use Bisonc++ to implement a parser for the Tiger language. ​ The blog is written after the completion of the lab, so some details may be forgotten. Our main target of this lab is to use a sequence of tokens which the lab2’s lexical scanner gives, to parse them and create a AST(Abstract Syntax Tree). The AST representation is necessary for us to do the semantic type checking in the next lab. What’s more, we need to convert AST to a intermediate language then, for our final processing when trying to get assembly language and binary executable file. ​ For convenience and succinctness, I will not again to explain the basic structure and class of the AST in the code. The code will be provided in my github repo when all labs are finished. ​ If you are not familiar with the workflow of the Bison C++, refer to the documentation for more details. Part 1 Some Simple ProductionsAccording to Appendix A of tiger book, we can easily write down the following productions. 1.1 basic productions of expression​ The basic primitive expression constructor is necessary, which forms the lower-level nodes(mainly leaf nodes) of the AST. 123456exp : INT {$$ = new absyn::IntExp(scanner_.GetTokPos(), $1);} | MINUS exp {$$ = new absyn::OpExp(scanner_.GetTokPos(), absyn::MINUS_OP, new absyn::IntExp(scanner_.GetTokPos(), 0), $2);} | STRING {$$ = new absyn::StringExp(scanner_.GetTokPos(), $1);} | lvalue {$$ = new absyn::VarExp(scanner_.GetTokPos(), $1);} | NIL {$$ = new absyn::NilExp(scanner_.GetTokPos());}; 1.2 Operation productions of expression1234567891011121314exp : MINUS INT {$$ = new absyn::IntExp(scanner_.GetTokPos(), -1 * $2);} | exp PLUS exp {$$ = new absyn::OpExp(scanner_.GetTokPos(), absyn::PLUS_OP, $1, $3);} | exp MINUS exp {$$ = new absyn::OpExp(scanner_.GetTokPos(), absyn::MINUS_OP, $1, $3);} | exp TIMES exp {$$ = new absyn::OpExp(scanner_.GetTokPos(), absyn::TIMES_OP, $1, $3);} | exp DIVIDE exp {$$ = new absyn::OpExp(scanner_.GetTokPos(), absyn::DIVIDE_OP, $1, $3);} | exp AND exp {$$ = new absyn::IfExp(scanner_.GetTokPos(), $1, $3, new absyn::IntExp(scanner_.GetTokPos(), 0));} | exp OR exp {$$ = new absyn::IfExp(scanner_.GetTokPos(), $1, new absyn::IntExp(scanner_.GetTokPos(), 1), $3);} | exp EQ exp {$$ = new absyn::OpExp(scanner_.GetTokPos(), absyn::EQ_OP, $1, $3);} | exp NEQ exp {$$ = new absyn::OpExp(scanner_.GetTokPos(), absyn::NEQ_OP, $1, $3);} | exp LT exp {$$ = new absyn::OpExp(scanner_.GetTokPos(), absyn::LT_OP, $1, $3);} | exp LE exp {$$ = new absyn::OpExp(scanner_.GetTokPos(), absyn::LE_OP, $1, $3);} | exp GT exp {$$ = new absyn::OpExp(scanner_.GetTokPos(), absyn::GT_OP, $1, $3);} | exp GE exp {$$ = new absyn::OpExp(scanner_.GetTokPos(), absyn::GE_OP, $1, $3);}; ​ We need to highlight the implementation of the AND and OR operation. Since we don’t have a AND_OP and OR_OP in the predefined declarations of the AST. We can use the following conversion: 123exp1 AND exp2&lt;=&gt;if exp1 then exp2 else 0 123exp1 OR exp2&lt;=&gt;if exp1 then 1 else exp2 ​ Since in tiger language, the if-expression will return either the then-body or the else-body according to the condition, we can implement the conversion without any worry. 1.3 General Declarations123456789101112exp : LET decs IN expseq END {$$ = new absyn::LetExp(scanner_.GetTokPos(), $2, $4);};decs : decs_nonempty {$$ = $1;} | {$$ = new absyn::DecList();} //may be empty here;decs_nonempty : decs_nonempty_s decs {$$ = $2; $$-&gt;Prepend($1);};decs_nonempty_s : vardec {$$ = $1;} | tydec {$$ = new absyn::TypeDec(scanner_.GetTokPos(), $1);} | fundec {$$ = new absyn::FunctionDec(scanner_.GetTokPos(), $1);}; ​ In let-in-end format, tiger language allow empty declarations in production. So decs can be devided into empty one and non-empty one. For those are not empty, it can be forwardly inferred to a list of decs_nonempty_s. We use a DecList class to realize such behavior. For example, 1234some declaration;some declaration;some declaration;some declaration; will finally form a class of absyn::DecList with four declarations in its member variable dec_list_ whose type is std::List&lt;dec *&gt;. 1.4 Variable declaration12vardec : VAR ID ASSIGN exp {$$ = new absyn::VarDec(scanner_.GetTokPos(), $2, nullptr, $4);} | VAR ID COLON ID ASSIGN exp {$$ = new absyn::VarDec(scanner_.GetTokPos(), $2, $4, $6);}; ​ Since now, we can handle codes like 12var a := 3;var b : string := &quot;foo&quot;; 1.5 Type declaration1234567891011121314151617tydec : tydec_one tydec {$$ = $2; $$-&gt;Prepend($1);} | tydec_one {$$ = new absyn::NameAndTyList($1);};tydec_one : TYPE ID EQ ty {$$ = new absyn::NameAndTy($2, $4); };ty : ID {$$ = new absyn::NameTy(scanner_.GetTokPos(), $1);} | LBRACE tyfields RBRACE {$$ = new absyn::RecordTy(scanner_.GetTokPos(), $2);} | ARRAY OF ID {$$ = new absyn::ArrayTy(scanner_.GetTokPos(), $3);};tyfields : tyfields_nonempty {$$ = $1;} | {$$ = new absyn::FieldList();} //may be empty here;tyfields_nonempty : tyfield {$$ = new absyn::FieldList($1);} | tyfield COMMA tyfields_nonempty {$$ = $3; $$-&gt;Prepend($1);};tyfield : ID COLON ID {$$ = new absyn::Field(scanner_.GetTokPos(), $1, $3);}; ​ As we can see, the tydec also contains a list of tydec_one, so we can apply the same strategy when we handle decs. There are three type declaration in tiger language. 123type a = int;type b = array of a;type c = {name:string, score:b}; ​ So we can easily recognize the first and third line of ty production, but the nested one seems more difficult. Since the typefields is also a non-fixed size list, so we also need to split it into the empty one and non-empty one. Each production will add one element to the typefields. 1.7 Function declaration​ The function declaration consists of two types. 12function id (tyfields) = expfunction id (tyfields) : type-id = exp ​ So we can easily get the following code. 12345fundec : fundec_one {$$ = new absyn::FunDecList($1);} | fundec_one fundec {$$ = $2; $$-&gt;Prepend($1);};fundec_one : FUNCTION ID LPAREN tyfields RPAREN EQ exp {$$ = new absyn::FunDec(scanner_.GetTokPos(), $2, $4, nullptr, $7);} | FUNCTION ID LPAREN tyfields RPAREN COLON ID EQ exp {$$ = new absyn::FunDec(scanner_.GetTokPos(), $2, $4, $7, $9);}; 1.8 Nested type assignment​ To initialize those type are declared in nest as follows, 1list{first=i, rest=readlist()} ​ we can also have 123456789exp : ID LBRACE rec RBRACE {$$ = new absyn::RecordExp(scanner_.GetTokPos(), $1, $3);}rec : rec_nonempty {$$ = $1;} | {$$ = new absyn::EFieldList();};rec_nonempty : rec_one {$$ = new absyn::EFieldList($1);} | rec_one COMMA rec_nonempty {$$ = $3; $$-&gt;Prepend($1);};rec_one : ID EQ exp {$$ = new absyn::EField($1, $3);}; 1.9 Other productions​ According to Appendix A, it’s easy to understand the following code. 1234567891011121314151617181920212223exp : LPAREN sequencing RPAREN {$$ = new absyn::SeqExp(scanner_.GetTokPos(), $2);} | LPAREN exp RPAREN {$$ = $2;} | LPAREN RPAREN {$$ = new absyn::VoidExp(scanner_.GetTokPos());} | LET IN END {$$ = new absyn::VoidExp(scanner_.GetTokPos());} | ID LPAREN actuals RPAREN {$$ = new absyn::CallExp(scanner_.GetTokPos(), $1, $3);} | lvalue ASSIGN exp {$$ = new absyn::AssignExp(scanner_.GetTokPos(), $1, $3);} | WHILE exp DO exp {$$ = new absyn::WhileExp(scanner_.GetTokPos(), $2, $4);} | FOR ID ASSIGN exp TO exp DO exp {$$ = new absyn::ForExp(scanner_.GetTokPos(), $2, $4, $6, $8);} | BREAK {$$ = new absyn::BreakExp(scanner_.GetTokPos());}; actuals : nonemptyactuals {$$ = $1;} | {$$ = new absyn::ExpList();};nonemptyactuals : exp {$$ = new absyn::ExpList($1);} | exp COMMA nonemptyactuals {$$ = $3; $$-&gt;Prepend($1);};sequencing : exp sequencing_exps {$$ = $2; $$-&gt;Prepend($1); };sequencing_exps: SEMICOLON exp sequencing_exps {$$ = $3; $$-&gt;Prepend($2);} | {$$ = new absyn::ExpList();};expseq : {$$ = new absyn::VoidExp(scanner_.GetTokPos());} | sequencing {$$ = new absyn::SeqExp(scanner_.GetTokPos(), $1);}; Part 2 Handle shift-reduce conflict2.1 If-then-else conflict​ I solve the conflict according to blog. 12345%nonassoc THEN%nonassoc ELSE...exp : IF exp THEN exp ELSE exp {$$ = new absyn::IfExp(scanner_.GetTokPos(), $2, $4, $6);} | IF exp THEN exp {$$ = new absyn::IfExp(scanner_.GetTokPos(), $2, $4, nullptr);} 2.2 ID[exp] conflict​ I have spent a lot time on debugging this conflict. To be specific, the problem can be illustrated in the following sheet. 12345ID [exp] ·, OF shiftID [exp] ·, $ reduceID [exp] ·, DOT shiftID [exp] ·, [ shiftID [exp] ·, ASSIGN shift ​ Let’s take an example, 12345var b := intArray [N] of 0a = c[10]c[10].first = 5d[10][5] = 2row[7] := 1 ​ I have tried a lot of method from blog1 and blog2, but it seems neither of both are useful. To enforce every production do reduce first, I configure “special_one” to achieve my goal. 123456789101112131415exp : special_one OF exp {$$ = new absyn::ArrayExp(scanner_.GetTokPos(), ((absyn::SimpleVar *)(((absyn::SubscriptVar *)($1))-&gt;var_))-&gt;sym_, ((absyn::SubscriptVar *)($1))-&gt;subscript_, $3);}lvalue : oneormore DOT ID {$$ = new absyn::FieldVar(scanner_.GetTokPos(), $1, $3);} | oneormore LBRACK exp RBRACK {$$ = new absyn::SubscriptVar(scanner_.GetTokPos(), $1, $3);} | oneormore {$$ = $1;} | special_one DOT ID {$$ = new absyn::FieldVar(scanner_.GetTokPos(), $1, $3);} | special_one LBRACK exp RBRACK {$$ = new absyn::SubscriptVar(scanner_.GetTokPos(), $1, $3);} | special_one {$$ = $1;};special_one : one LBRACK exp RBRACK{$$ = new absyn::SubscriptVar(scanner_.GetTokPos(), $1, $3);};oneormore : one DOT ID {$$ = new absyn::FieldVar(scanner_.GetTokPos(), $1, $3);} | one {$$ = $1;};one : ID {$$ = new absyn::SimpleVar(scanner_.GetTokPos(), $1);}; ​ I configured special_one equals “id[exp]”, enforcing every “id[exp]” to be reduced to special_one.","link":"/2021/11/04/complier-lab3/"},{"title":"complier-lab4","text":"This lab comes from SE3355 *Compilers* lab4, which requires me to implement a type-checking module when scanning the AST. The AST is created by bisonc++ script implemented in lab3. To make the following statement more clear, we can take a see of the structure of the AST node. The venv contains the symbols of variables and functions and the tenv contains the symbols of all types. 123456789101112131415161718192021222324252627282930313233343536373839404142434445class Var {public: int pos_; virtual ~Var() = default; virtual void Print(FILE *out, int d) const = 0; virtual type::Ty *SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount, err::ErrorMsg *errormsg) const = 0;protected: explicit Var(int pos) : pos_(pos) {}};class Exp {public: int pos_; virtual ~Exp() = default; virtual void Print(FILE *out, int d) const = 0; virtual type::Ty *SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount, err::ErrorMsg *errormsg) const = 0;protected: explicit Exp(int pos) : pos_(pos) {}};class Dec {public: int pos_; virtual ~Dec() = default; virtual void Print(FILE *out, int d) const = 0; virtual void SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount, err::ErrorMsg *errormsg) const = 0;protected: explicit Dec(int pos) : pos_(pos) {}};class Ty {public: int pos_; virtual ~Ty() = default; virtual void Print(FILE *out, int d) const = 0; virtual type::Ty *SemAnalyze(env::TEnvPtr tenv, err::ErrorMsg *errormsg) const = 0;protected: explicit Ty(int pos) : pos_(pos) {}}; Part 1 Some Basic Type Checking1.1 some root and leavesWe want to get the type of every variable, expression and declaration, so we can easily do it from the root and leaves. 1234567891011121314151617181920212223void AbsynTree::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, err::ErrorMsg *errormsg) const { root_-&gt;SemAnalyze(venv, tenv, 0, errormsg);}type::Ty *VarExp::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount, err::ErrorMsg *errormsg) const { return var_-&gt;SemAnalyze(venv, tenv, labelcount, errormsg);}type::Ty *NilExp::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount, err::ErrorMsg *errormsg) const { return type::NilTy::Instance();}type::Ty *IntExp::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount, err::ErrorMsg *errormsg) const { return type::IntTy::Instance();}type::Ty *StringExp::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount, err::ErrorMsg *errormsg) const { return type::StringTy::Instance();}type::Ty *VoidExp::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount, err::ErrorMsg *errormsg) const { return type::VoidTy::Instance();} 1.2 VariablesThe type-checking code of three variables is as follows. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566type::Ty *SimpleVar::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv,int labelcount, err::ErrorMsg *errormsg) const { env::EnvEntry *entry = venv-&gt;Look(sym_); if (entry &amp;&amp; (typeid(*entry) == typeid(env::VarEntry))) { env::VarEntry* varEntry = static_cast&lt;env::VarEntry *&gt;(entry); type::Ty *type = varEntry-&gt;ty_; bool readonly = varEntry-&gt;readonly_; return type; } else { errormsg-&gt;Error(pos_, &quot;undefined variable %s&quot;, sym_-&gt;Name().data()); } return type::IntTy::Instance();}type::Ty *FieldVar::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv,int labelcount, err::ErrorMsg *errormsg) const { // for field var first check var need to be a RecordTy, then check the sym_ is in the recordTY type type::Ty * variable_type = var_-&gt;SemAnalyze(venv, tenv, labelcount, errormsg); if (variable_type == nullptr) { errormsg-&gt;Error(pos_, &quot;variable not defined.&quot;); } else if (typeid(*(variable_type-&gt;ActualTy())) != typeid(type::RecordTy)) { errormsg-&gt;Error(pos_, &quot;not a record type&quot;); } else { type::RecordTy * real_type = ((type::RecordTy *) (variable_type)); int matched = 0; for (type::Field* field:real_type-&gt;fields_-&gt;GetList()) { if (field-&gt;name_-&gt;Name() == sym_-&gt;Name()) { matched = 1; } } if (matched == 1) { return real_type; } else { errormsg-&gt;Error(pos_, &quot;field nam doesn't exist&quot;); return real_type; } }}type::Ty *SubscriptVar::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv,int labelcount, err::ErrorMsg *errormsg) const { //check the type of var_ is an array or not //if so, check subscript_ is an int or not, and check the range. type::Ty *var_type = var_-&gt;SemAnalyze(venv, tenv, labelcount, errormsg); //var_type is array of the type that we want to return if (var_type == nullptr) { errormsg-&gt;Error(pos_, &quot;variable not defined.&quot;); } else if (typeid(*(var_type-&gt;ActualTy())) != typeid(type::ArrayTy)) { errormsg-&gt;Error(pos_, &quot;array type required&quot;); } else { type::Ty *subscript_type = subscript_-&gt;SemAnalyze(venv, tenv, labelcount, errormsg); if (typeid(*(subscript_type-&gt;ActualTy())) != typeid(type::IntTy)) { errormsg-&gt;Error(pos_, &quot;subscribe is not a int type.&quot;); } else { return ((type::ArrayTy *) var_type)-&gt;ty_; } } return nullptr;} 1.3 Call expressionThe type-checking code of call expression is as follows. 12345678910111213141516171819202122232425262728293031323334353637383940414243type::Ty *CallExp::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount, err::ErrorMsg *errormsg) const { env::FunEntry *funEntry = static_cast&lt;env::FunEntry *&gt;(venv-&gt;Look(func_)); if (funEntry == nullptr) { errormsg-&gt;Error(pos_, &quot;undefined function &quot; + func_-&gt;Name()); return type::NilTy::Instance(); } std::list&lt;Exp *&gt; args_list = args_-&gt;GetList(); if (funEntry-&gt;formals_ == nullptr) { auto args_it = args_-&gt;GetList().begin(); for (; args_it != args_-&gt;GetList().end(); args_it++) { Exp *current_exp = *args_it; current_exp-&gt;SemAnalyze(venv, tenv, labelcount, errormsg); } } else { auto formal_it = funEntry-&gt;formals_-&gt;GetList().begin(); auto args_it = args_-&gt;GetList().begin(); for (; formal_it != funEntry-&gt;formals_-&gt;GetList().end() &amp;&amp; args_it != args_-&gt;GetList().end(); formal_it++, args_it++) { Exp *current_exp = *args_it; type::Ty *formal_type = *formal_it; if (typeid(*(current_exp-&gt;SemAnalyze(venv, tenv, labelcount, errormsg)-&gt;ActualTy())) != typeid(*formal_type)) { // type not match errormsg-&gt;Error(current_exp-&gt;pos_, &quot;para type mismatch&quot;); } } if (args_it != args_-&gt;GetList().end()) { // number does not match auto last_it = args_-&gt;GetList().end(); last_it--; errormsg-&gt;Error((*last_it)-&gt;pos_, &quot;too many params in function g&quot;); } else if (formal_it != funEntry-&gt;formals_-&gt;GetList().end()) { } } if (funEntry-&gt;result_ != nullptr) { return funEntry-&gt;result_; } else { return type::NilTy::Instance(); }} Part 2 Some Tricky Part2.1 the loop variable shouldn’t be assignedSince the loop variable in a for-loop can’t not be assigned because of the rule of the tiger language, when a for-loop declares a variable, the variable should be marked with “readonly = true”. In such way, when encountering a variable in assign exp, the module can check the variable is read-only or not. 1234567891011121314151617181920212223242526272829303132333435363738type::Ty *ForExp::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount, err::ErrorMsg *errormsg) const { venv-&gt;BeginScope(); type::Ty *lo_type = lo_-&gt;SemAnalyze(venv, tenv, labelcount, errormsg); venv-&gt;Enter(var_, new env::VarEntry(lo_type, true)); type::Ty *hi_type = hi_-&gt;SemAnalyze(venv, tenv, labelcount, errormsg); if (typeid(*(hi_type-&gt;ActualTy())) != typeid(type::IntTy)) { errormsg-&gt;Error(hi_-&gt;pos_, &quot;for exp's range type is not integer&quot;); } body_-&gt;SemAnalyze(venv, tenv, labelcount + 1, errormsg); venv-&gt;EndScope(); return type::NilTy::Instance();}type::Ty *AssignExp::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount, err::ErrorMsg *errormsg) const { type::Ty *var_type = var_-&gt;SemAnalyze(venv, tenv, labelcount, errormsg); type::Ty *exp_type = exp_-&gt;SemAnalyze(venv, tenv, labelcount, errormsg); if (typeid(*(exp_type-&gt;ActualTy())) == typeid(type::NilTy) &amp;&amp; typeid(*(var_type-&gt;ActualTy())) == typeid(type::RecordTy)) { return type::NilTy::Instance(); } else if (var_type &amp;&amp; typeid(*(var_type)-&gt;ActualTy()) == typeid(type::IntTy)) { absyn::SimpleVar *simpleVar = dynamic_cast&lt;SimpleVar *&gt;(var_); if (simpleVar != nullptr) { env::EnvEntry *envEntry = venv-&gt;Look(simpleVar-&gt;sym_); if (envEntry != nullptr) { bool readonly = envEntry-&gt;readonly_; if (readonly) { errormsg-&gt;Error(pos_, &quot;loop variable can't be assigned&quot;); } } } } else if (var_type &amp;&amp; exp_type &amp;&amp; typeid(*(var_type-&gt;ActualTy())) != typeid(*(exp_type-&gt;ActualTy()))) { errormsg-&gt;Error(pos_, &quot; unmatched assign exp&quot;); } return type::NilTy::Instance();} 2.2 The in-loop checking of break expression123456789101112131415161718192021type::Ty *ForExp::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount, err::ErrorMsg *errormsg) const { ... body_-&gt;SemAnalyze(venv, tenv, labelcount + 1, errormsg); ...}type::Ty *WhileExp::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv,int labelcount, err::ErrorMsg *errormsg) const { ... type::Ty *body_type = body_-&gt;SemAnalyze(venv, tenv, labelcount + 1, errormsg); ...}type::Ty *BreakExp::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount, err::ErrorMsg *errormsg) const { if (labelcount == 0) { errormsg-&gt;Error(pos_, &quot;break is not inside any loop&quot;); } return type::NilTy::Instance();} 2.3 handle the nested function declarationTiger supports the adjacent nested function, so we need to define all the function name first in order for all the function to find the reference function entry in the venv. 123456789101112131415161718192021222324252627282930void FunctionDec::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv,int labelcount, err::ErrorMsg *errormsg) const { absyn::FunDec *last_function = nullptr; for (absyn::FunDec *function:functions_-&gt;GetList()) { if (last_function &amp;&amp; function-&gt;name_-&gt;Name() == last_function-&gt;name_-&gt;Name()) { errormsg-&gt;Error(function-&gt;pos_, &quot;two functions have the same name&quot;); } venv-&gt;Enter(function-&gt;name_, new env::FunEntry(nullptr, nullptr)); last_function = function; } for (absyn::FunDec *function:functions_-&gt;GetList()) { type::Ty *result_ty = tenv-&gt;Look(function-&gt;result_); type::TyList *formals = function-&gt;params_-&gt;MakeFormalTyList(tenv, errormsg); venv-&gt;Set(function-&gt;name_, new env::FunEntry(formals, result_ty)); venv-&gt;BeginScope(); auto formal_it = formals-&gt;GetList().begin(); auto param_it = function-&gt;params_-&gt;GetList().begin(); for (; param_it != function-&gt;params_-&gt;GetList().end(); formal_it++, param_it++) { venv-&gt;Enter((*param_it)-&gt;name_, new env::VarEntry(*formal_it)); } type::Ty *ty = function-&gt;body_-&gt;SemAnalyze(venv, tenv, labelcount, errormsg); errormsg-&gt;Error(pos_, &quot;function body over&quot;); if (function-&gt;result_ == nullptr &amp;&amp; ty &amp;&amp; typeid(*(ty-&gt;ActualTy())) != typeid(type::NilTy)) { errormsg-&gt;Error(pos_, &quot;procedure returns value&quot;); } venv-&gt;EndScope(); }} 2.4 illegal cycle in nested type declaration12345678910111213// test.16.tig/* error: mutually recursive types thet do not pass through record or array */let type a=ctype b=atype c=dtype d=ain &quot;&quot;end The upper code is not allowed in tiger language, so we need to check if there is a cycle in type nested declaration. Each time we define a new type, we need to scan the defined type in the venv to make sure there is no cycle. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253void TypeDec::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount,err::ErrorMsg *errormsg) const { for (NameAndTy* current: types_-&gt;GetList()) { // put all the name into the env first sym::Symbol *symbol = current-&gt;name_; for (NameAndTy* current_2: types_-&gt;GetList()) { if (current_2 == current) break; if (current_2-&gt;name_-&gt;Name() == current-&gt;name_-&gt;Name()) { errormsg-&gt;Error(pos_, &quot;two types have the same name&quot;); } } tenv-&gt;Enter(symbol, new type::NameTy(symbol, nullptr)); } for (NameAndTy* current: types_-&gt;GetList()) { sym::Symbol *symbol = current-&gt;name_; errormsg-&gt;Error(pos_, &quot;define symbol &quot; + symbol-&gt;Name()); absyn::Ty* type = current-&gt;ty_; absyn::NameTy *changed_type1 = dynamic_cast&lt;absyn::NameTy *&gt;(type); absyn::ArrayTy *changed_type2 = dynamic_cast&lt;absyn::ArrayTy *&gt;(type); absyn::RecordTy *changed_type3 = dynamic_cast&lt;absyn::RecordTy *&gt;(type); if (changed_type1 != nullptr) { type::Ty *type1 = changed_type1-&gt;SemAnalyze(tenv, errormsg); tenv-&gt;Set(symbol, type1); try { type::Ty * test_type = tenv-&gt;Look(symbol); type::NameTy * change_one = dynamic_cast&lt;type::NameTy *&gt;(test_type); type::NameTy * origin_one = dynamic_cast&lt;type::NameTy *&gt;(test_type); while (change_one != nullptr) { errormsg-&gt;Error(pos_, &quot;change_one= &quot; + change_one-&gt;sym_-&gt;Name()); change_one = dynamic_cast&lt;type::NameTy *&gt;(change_one-&gt;ty_); if (change_one == origin_one) { errormsg-&gt;Error(pos_, &quot;eeee= &quot; + change_one-&gt;sym_-&gt;Name()); throw(0); } } } catch (...) { errormsg-&gt;Error(pos_, &quot;illegal type cycle&quot;); } } else if (changed_type2 != nullptr) { type::Ty *type2 = changed_type2-&gt;SemAnalyze(tenv, errormsg); tenv-&gt;Set(symbol, type2); } else if (changed_type3 != nullptr) { type::Ty *type3 = changed_type3-&gt;SemAnalyze(tenv, errormsg); tenv-&gt;Set(symbol, type3); } else { exit(0); } } return;}","link":"/2021/11/22/complier-lab4/"},{"title":"ikfast-configuration","text":"​ IKFAST is a very powerful tool when a robot requires inverse kinematic solutions, so it’s necessary for us to config a customized robot ikfast plugin from scratch, which is a basic skill for a robotic engineer, I deem. 1. Configure the ikfast for flexiv robot​ As first search shows, the ikfast is provided by OpenRAVE which is a very complex planning framework. OpenRAVE requires a tricky installing process, and using not supported environment to install it will cause a lot of frustration. Luckily, there is a docker image to help us get rid of such tedious and troublesome work. I fork the repo to better save the docker image. The docker image provides Ubuntu 14.04 with OpenRAVE 0.9.0 and ROS Indigo installed, which can be used to generate the solver code once. You can browse the docker image by the following command. 1docker pull kamicode/personalrobotics:ros-openrave ​ The script in moveit_kinematics which is called auto_create_ikfast_moveit_plugin.sh will automatically build the docker environment and prepare the OpenRAVE. ​ Since we have the robot’s urdf, we can use the following command to create the ikfast plugin. 12export MYROBOT_NAME=&quot;panda&quot; &amp;rosrun moveit_kinematics auto_create_ikfast_moveit_plugin.sh --iktype Transform6D $MYROBOT_NAME.urdf &lt;planning_group_name&gt; &lt;base_link&gt; &lt;eef_link&gt; ​ In my case, the command is 1rosrun moveit_kinematics auto_create_ikfast_moveit_plugin.sh --iktype Transform6D --name flexiv &lt;somewhere&gt;/ws_moveit/src/moveit_resources/flexiv_description/robot.urdf arm base link7 ​ I specify the –name attribute, because if I use the absolute path, the auto-extracted name may not be correct. ​ The result after several minute is as follows. 12345678910111213141516171819202122Running &lt;somewhere&gt;/ws_moveit/src/moveit/moveit_kinematics/ikfast_kinematics_plugin/scripts/create_ikfast_moveit_plugin.py &quot;flexiv&quot; &quot;arm&quot; &quot;flexiv_arm_ikfast_plugin&quot; &quot;base&quot; &quot;link7&quot; &quot;/tmp/ikfast.rn0uJk/.openrave/kinematics.f82de3cf02e1545e44ec052f9f4778ca/ikfast0x10000049.Transform6D.1_2_3_4_5_6_f0.cpp&quot;Creating IKFastKinematicsPlugin with parameters: robot_name: flexiv base_link_name: base eef_link_name: link7 planning_group_name: arm ikfast_plugin_pkg: flexiv_arm_ikfast_plugin ikfast_output_path: /tmp/ikfast.rn0uJk/.openrave/kinematics.f82de3cf02e1545e44ec052f9f4778ca/ikfast0x10000049.Transform6D.1_2_3_4_5_6_f0.cpp search_mode: OPTIMIZE_MAX_JOINT srdf_filename: flexiv.srdf robot_name_in_srdf: flexiv moveit_config_pkg: flexiv_moveit_configFound source code generated by IKFast version 73Failed to find package: flexiv_arm_ikfast_plugin. Will create it in &lt;somewhere&gt;/ws_moveit/src/moveit/moveit_kinematics/flexiv_arm_ikfast_plugin.Created ikfast header file at '&lt;somewhere&gt;/ws_moveit/src/moveit/moveit_kinematics/flexiv_arm_ikfast_plugin/include/ikfast.h'Created ikfast plugin file at '&lt;somewhere&gt;/ws_moveit/src/moveit/moveit_kinematics/flexiv_arm_ikfast_plugin/src/flexiv_arm_ikfast_moveit_plugin.cpp'Created plugin definition at '&lt;somewhere&gt;/ws_moveit/src/moveit/moveit_kinematics/flexiv_arm_ikfast_plugin/flexiv_arm_moveit_ikfast_plugin_description.xml'Created cmake file at '&lt;somewhere&gt;/ws_moveit/src/moveit/moveit_kinematics/flexiv_arm_ikfast_plugin/CMakeLists.txt'Wrote package.xml at '&lt;somewhere&gt;/ws_moveit/src/moveit/moveit_kinematics/flexiv_arm_ikfast_plugin/package.xml'Created update plugin script at '&lt;somewhere&gt;/ws_moveit/src/moveit/moveit_kinematics/flexiv_arm_ikfast_plugin/update_ikfast_plugin.sh'Modified kinematics.yaml at '&lt;somewhere&gt;/ws_moveit/src/flexiv_moveit_config/config/kinematics.yaml' ​ Notice the upper script has modified the kinematics.yaml in flexiv_moveit_config, which means the robot will use the ikfast to calculate ik. 1234arm: kinematics_solver: flexiv_arm/IKFastKinematicsPlugin kinematics_solver_search_resolution: 0.005 kinematics_solver_timeout: 0.005 ​ But if we using the following command to visualize the flexiv robot in RViz, we can find an error which means that the moveit can not load the flexiv-ikfast plugin. 1roslaunch flexiv_moveit_config demo.launch ​ The error is 12[ERROR] [1636036724.761299732]: The kinematics plugin (arm) failed to load. Error: According to the loaded plugin descriptions the class flexiv_arm/IKFastKinematicsPlugin with base class type kinematics::KinematicsBase does not exist. Declared types are cached_ik_kinematics_plugin/CachedKDLKinematicsPlugin cached_ik_kinematics_plugin/CachedSrvKinematicsPlugin kdl_kinematics_plugin/KDLKinematicsPlugin lma_kinematics_plugin/LMAKinematicsPlugin prbt_manipulator/IKFastKinematicsPlugin srv_kinematics_plugin/SrvKinematicsPlugin[ERROR] [1636036724.761371349]: Kinematics solver could not be instantiated for joint group arm. ​ It tells us that MoveIt can not find the flexiv-ikfast plugin. So we goto the /ws_moveit/src/moveit/moveit_kinematics to find out why. ​ There is flexiv_arm_ikfast_plugin indeed in the directory, but it seems not be complied. I try to add some lines to the CMakeList.txt in moveit_kinematics directory but failed in compiling. But I find if I copy the flexiv_arm…._plugin out as a single package to compile. Rviz works without informing “plugin not found”. 2. Validate the output ikfast file works​ Because RViz will pick available ik solver if it can not find configured ikfast plugin, even if the error message has been eliminated, we can not make sure that the RViz planing do use the ikfast. We need to find an another way. ​ An article proposes another way for us. That is, use the ikfastdemo.cpp to do the validation. We can simply put the ikfastdemo.cpp in the same directory with our ikfast cpp, and make ikfastdemo.cpp inlcude it. ​ So we add two lines to ikfastdemo.cpp. 12#define IK_VERSION 73#include &quot;flexiv_arm_ikfast_solver.cpp&quot; ​ And compile it. 1g++ ikfastdemo.cpp -lstdc++ -llapack -o compute -lrt ​ The usage is as follows. 123456789101112131415161718Usage: ./compute fk j0 j1 ... j6 Returns the forward kinematic solution given the joint angles (in radians). ./compute ik t0 t1 t2 qw qi qj qk free0 ... Returns the ik solutions given the transformation of the end effector specified by a 3x1 translation (tX), and a 1x4 quaternion (w + i + j + k). There are 1 free parameters that have to be specified. ./compute ik r00 r01 r02 t0 r10 r11 r12 t1 r20 r21 r22 t2 free0 ... Returns the ik solutions given the transformation of the end effector specified by a 3x3 rotation R (rXX), and a 3x1 translation (tX). There are 1 free parameters that have to be specified. ./compute iktiming For fixed number of iterations, generates random joint angles, then calculates fk, calculates ik, measures average time taken. ./compute iktiming2 For fixed number of iterations, with one set of joint variables, this finds the ik solutions and measures the average time taken. 2.1 Painful debug timeI use the following command to calculate a forward kinematic solution. 12345678910111213141516./compute fk 1.57 1.57 1.57 1.57 1.57 1.57 1.57Found fk solution for end frame: Translation: x: -0.484689 y: 0.475605 z: 0.400080 Rotation 0.001591 0.001596 -0.999997 Matrix: 0.999996 0.002387 0.001595 0.002390 -0.999996 -0.001592 Euler angles: Yaw: -1.572388 (1st: rotation around vertical blue Z-axis in ROS Rviz) Pitch: 1.568409 Roll: 0.001595 Quaternion: -0.500596 0.500199 0.500597 -0.498606 -0.500596 + 0.500199i + 0.500597j - 0.498606k (alternate convention) ​ To visualize the solution in the RViz, I set the flexiv robot at the same configuration, and add a box indicating the calculation of the ikfast forward kinematics. ​ It seems the base joint need to rotate 180 degree to fit the result of ikfast fk, which is very weird. I spend a lot of time debugging on the problem. What remains unknown is that the RViz motion planning plugin still works well though the ikfast is not giving the correct answer. So it may not use ikfast or experience some downgrade mode because the failure of finding a solution by ikfast plugin. ​ By chance, I find that the yaw angle of the first fixed joint is strange. 123456&lt;!-- ============ joint 0 =============== --&gt;&lt;joint name=&quot;joint0&quot; type=&quot;fixed&quot;&gt; &lt;origin rpy=&quot;0 0 -3.1415927&quot; xyz=&quot;0 0 0.0&quot;/&gt; &lt;parent link=&quot;world&quot;/&gt; &lt;child link=&quot;base&quot;/&gt;&lt;/joint&gt; ​ It is exactly 180 degree, which fits our guess. So I change the value of that yaw angle to zero. And everything works. But what cause the difference? Remember we do NOT set the virtual joint when configuring the flexiv in MoveIt Setup Assistant because we find the fixed joint in URDF ? If we did that as the tutorial, it will add following lines into our robot.srdf. 12&lt;!--VIRTUAL JOINT: Purpose: this element defines a virtual joint between a robot link and an external frame of reference (considered fixed with respect to the robot)--&gt;&lt;virtual_joint child_link=&quot;panda_link0&quot; name=&quot;virtual_joint&quot; parent_frame=&quot;world&quot; type=&quot;floating&quot;/&gt; ​ So BEAR IN MIND, IT IS A CONVENTION THAT THE VIRTUAL JOINT SHOULD BE WITH RPY=”0 0 0”!!! 2.2 Result ​ Finally, I find both the joint and the box overlap however I change the joint position, which means the forward kinematics of ikfast calculate the correct result ! 3. Add ikfast into pybullet-planning module​ Let’s first have a glance on the directory structure of the pybullet-planning/pybullet_tools/ikfast ​ We need to add flexiv directory which includes The ik.py defines some basic information of our new robot. 1234567from ..utils import IKFastInfofrom ..ikfast import * # For legacy purposesFLEXIV_URDF = &quot;models/flexiv_description/flexiv.urdf&quot;FLEXIV_INFO = IKFastInfo(module_name='flexiv.ikfast_flexiv', base_link='base', ee_link='link7', free_joints=['joint1']) The setup.py helps us to compile a pybind11 module, which a shared library .so file. The code is mainly modified from the existed ikfast robot directory. 1234567891011121314151617181920#!/usr/bin/env pythonfrom __future__ import print_functionimport sysimport ossys.path.append(os.path.join(os.pardir, os.pardir, os.pardir))from pybullet_tools.ikfast.compile import compile_ikfast# Build C++ extension by running: 'python setup.py'# see: https://docs.python.org/3/extending/building.htmldef main(): # lib name template: 'ikfast_&lt;robot name&gt;' sys.argv[:] = sys.argv[:1] + ['build'] robot_name = 'flexiv' compile_ikfast(module_name='ikfast_{}'.format(robot_name), cpp_filename='ikfast_{}.cpp'.format(robot_name))if __name__ == '__main__': main() ​ In the original ikfast_flexiv.cpp, to enable the pybind11, we have to add a lot of functions and include “Python.h”. Luckily the author of pybullet-planning has written it for us, so we only to add these lines at the end. ​ Then we simply use the command 1234$python setup.py build......ikfast module ikfast_flexiv imported successful ​ We can modify the test codes in pybullet-planning examples to check whether our flexiv robot can use ikfast to calculate now. ​ The result shows we did it !","link":"/2021/11/06/ikfast-configuration/"},{"title":"VoteNet_pre","text":"A class pre of SJTU SE-125 Machine-Learning. This is a improved version. You can retrieve the origin version from bilibili.","link":"/2022/01/17/machine-learning-pre/"},{"title":"mmdection_hammer_segmentation","text":"The first time for me to construct a custom dataset and apply it in the mmdection. The whole pipeline is as follows: I. Collect and prepare the raw dataset. II. Convert the raw dataset into COCO style. III. Add and movify the mask-rcnn configuration in mmdection API to fit our requirement. IV. Train the model by mmdection. V. Evaluate and visualize the test dataset. VI. Modify the result into mask image so that we can insert it smoothly in our pipeline. Because of the fact that the project is still ongoing, the part I and part VI will NOT be described detailedly. 1.The dataset As shown above, it’s a sample from my created dataset which contains a rgb image and corresponding mask. The foreground is a tool(the hammer), and the background contains other unrelated things. At the very beginning when there was no relationship to how to training, it is of top priority to create enough data and split them into train, validate and test dataset. We collected 4535 imgs, 464 imgs and 454 imgs for training, validating and evaluating. 2. The format of COCOAs said by a famous blog, we use COCO to reconstruct our data is not because it is the best format, but it is the most widely used and accepted format. So we need to arrange our dataset in that format. To our relief, there are a lot of tools to help us automatically conduct such procedure, for example, pycococreator. What we need to do is only to set the related INFO, LICENSES, CATEGORIES and corresponding directory. 1234567891011121314151617181920212223242526272829303132333435363738INFO = { &quot;description&quot;: &quot;Hammer Segmentation Dataset&quot;, &quot;url&quot;: &quot;https://github.com/Kami-code&quot;, &quot;version&quot;: &quot;0.1.0&quot;, &quot;year&quot;: 2022, &quot;contributor&quot;: &quot;Kami-code&quot;, &quot;date_created&quot;: datetime.datetime.utcnow().isoformat(' ')}LICENSES = [ { &quot;id&quot;: 1, &quot;name&quot;: &quot;Attribution-NonCommercial-ShareAlike License&quot;, &quot;url&quot;: &quot;http://creativecommons.org/licenses/by-nc-sa/2.0/&quot; }]CATEGORIES = [ { 'id': 1, 'name': 'hammer', 'supercategory': 'none', },]...def main(): ROOT_DIR = 'hammer_' + MODE IMAGE_DIR = os.path.join(ROOT_DIR, &quot;rgb&quot;) ANNOTATION_DIR = os.path.join(ROOT_DIR, &quot;mask&quot;) coco_output = { &quot;info&quot;: INFO, &quot;licenses&quot;: LICENSES, &quot;categories&quot;: CATEGORIES, &quot;images&quot;: [], &quot;annotations&quot;: [] }... The structure is from the blog. You can refer to it to create your custom dataset. What I want to mention is if we have a lot of mask of different objects on a single image(though not in our current settings), the annotation_id should start from 0 and increase for each mask, while the meaning of image_id is easily understood. By the way, it seems no need to make annotation_id the same as “id” in CATEGORIES, so making it start from zero is necessary. After running the script of pycococreator, we get a single json file named “instance_train2022.json”. We can use the visualizer script provided in that repo to visualize to check whether we successfully get a COCO json file. COCO will contain the information which provided in the mask.png file using contour algorithm.(Finally, they will be stored in polygons format.) Since now, we have successfully got our first COCO dataset! 3. MMDectionMMDetection is an open source object detection toolbox based on PyTorch. It is a part of the OpenMMLab project. It provides a abstraction on the PyTorch which enables me train my network without writing any code. Frankly speaking, I will not use it if I have enough time to dig deep in mask-rcnn, cause it makes me tightly rely on the API, which do no good for my career. For now, given limited time, I have to use it. To use it, I mainly refer to the proceduce in this zhihu blog. But it also contains some frustrating bug, which I will explain below. This is the files need to add and modify if we want to add a new dataset to train by mask_rcnn. (marked in green means the newly-added file and marked in brown means the file needed to be modified) Let’s find how exactly these files works. mmdection/configs/mask_rcnn/my_mask_rcnn.py 123456_base_ = [ '../_base_/models/my_mask_rcnn_r50_fpn.py', '../common/my_coco_instance.py','../_base_/schedules/schedule_2x.py' ,'../_base_/default_runtime.py',] The file just links all the files we added. Next, we should define our own dataset structure, since we’ve got the dataset in COCO format. We can replicate the coco.py and change a little to get our “keto_coco.py”. mmdection/mmdet/datasets/coco.py 123456789101112131415161718@DATASETS.register_module()class CocoDataset(CustomDataset): CLASSES = ('person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush') ... mmdection/mmdet/datasets/keto_coco.py 12345@DATASETS.register_module()class KpCocoDataset(CustomDataset): CLASSES = ['hammer',] ... As we can see, what we do is just set the CLASSES variable into a list only containing one class hammer. The reason that we change the tuple to list seems to be a version-related problem that the low-level function will misleadingly parse the wrong type. mmdection/mmdet/datasets/init.py 12345678910__all__ = [ 'CustomDataset', 'XMLDataset', 'CocoDataset', 'DeepFashionDataset', 'VOCDataset', 'CityscapesDataset', 'LVISDataset', 'LVISV05Dataset', 'LVISV1Dataset', 'GroupSampler', 'DistributedGroupSampler', 'DistributedSampler', 'build_dataloader', 'ConcatDataset', 'RepeatDataset', 'ClassBalancedDataset', 'WIDERFaceDataset', 'DATASETS', 'PIPELINES', 'build_dataset', 'replace_ImageToTensor', 'get_loading_pipeline', 'NumClassCheckHook', 'CocoPanopticDataset', 'MultiImageMixDataset', 'KpCocoDataset'] Since we defined our KpCocoDataset, to make it register into the dataset collection, we need to add the structure name into its init.py and recomplie the module. In class_names.py, we should also add a simple function to get the classes of our defined dataset. mmdection/mmdet/core/evaluation/class_name.py 1234...def kp_coco_classes(): return ['hammer']... Also, a change in init.py is necessary. But there is no explicit call of this function, I GUESS the function may not be called or is called by its name in some format. The my_mask_rcnn_r50_fpn.py is replicated from the mask_rcnn_r50_fpn.py. We just change the class_num parameter 80 to 1. In my_coco_instance.py, we defined the path of the three COCO json files, and train and test pipeline. mmdection/configs/common/my_coco_instance.py 123456789101112131415161718192021222324...data = dict( samples_per_gpu=2, workers_per_gpu=2, train=dict( type='RepeatDataset', times=4, dataset=dict( type=dataset_type, ann_file=data_root + 'annotations/instances_train2022.json', img_prefix=data_root + 'train2022/', pipeline=train_pipeline)), val=dict( type=dataset_type, ann_file=data_root + 'annotations/instances_validate2022.json', img_prefix=data_root + 'validate2022/', pipeline=test_pipeline), test=dict( type=dataset_type, ann_file=data_root + 'annotations/instances_test2022.json', img_prefix=data_root + 'test2022/', pipeline=test_pipeline),)... And we will train the data on a single GPU with 2 samples_per_gpu, so we should downsize the learning rate defined in schedule_2x.py 8 times, because it’s default value is assuming the training is on 8 GPUs with 2 samples_per_gpu. 4. Train our modelWe can use the following command line to start training process. Though I have spent much time in debugging in the process. If we correctly config the mentioned files, it will finally works. 1python tools/train.py configs/mask_rcnn/my_mask_rcnn.py When training, we can read the logs to make sure we are on the right track. 12021-10-27 15:16:22,077 - mmdet - INFO - Epoch [1][2400/9070] lr: 2.500e-03, eta: 17:30:32, time: 0.288, data_time: 0.010, memory: 7875, loss_rpn_cls: 0.0216, loss_rpn_bbox: 0.0081, loss_cls: 0.0925, acc: 97.2070, loss_bbox: 0.1120, loss_mask: 0.1732, loss: 0.4074 How are the batches each epoch 9070 calculated? Recall that we set the training set to be repeat dataset 4, which means the total training dataset is 4535 * 4 = 18140. And we configure the training process on a single with samples_per_gpu = 2. So we get batch_size = 18140 / 2 = 9070. 1234567891011121314151617181920212223242021-10-27 22:03:28,363 - mmdet - INFO - Evaluating segm...Loading and preparing results...DONE (t=0.04s)creating index...index created!Running per image evaluation...Evaluate annotation type *segm*DONE (t=0.46s).Accumulating evaluation results...DONE (t=0.05s). Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.367 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=1000 ] = 0.416 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=1000 ] = 0.415 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = -1.000 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.365 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.369 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.373 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=300 ] = 0.373 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=1000 ] = 0.373 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = -1.000 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.369 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.3752021-10-27 22:03:28,939 - mmdet - INFO - Exp name: my_mask_rcnn.py2021-10-27 22:03:28,939 - mmdet - INFO - Epoch(val) [9][464] bbox_mAP: 0.3800, bbox_mAP_50: 0.4210, bbox_mAP_75: 0.4090, bbox_mAP_s: -1.0000, bbox_mAP_m: 0.3880, bbox_mAP_l: 0.3760, bbox_mAP_copypaste: 0.380 0.421 0.409 -1.000 0.388 0.376, segm_mAP: 0.3670, segm_mAP_50: 0.4160, segm_mAP_75: 0.4150, segm_mAP_s: -1.0000, segm_mAP_m: 0.3650, segm_mAP_l: 0.3690, segm_mAP_copypaste: 0.367 0.416 0.415 -1.000 0.365 0.369 Some results are as shown in the sheet. In this article, we don’t detailedly explain what each statistic means. 5. Evaluate and visualize our modelWe can run the following script to visualize our model. 12345678910111213from mmdet.apis import init_detector, inference_detector, show_result_pyplotif __name__ == '__main__': config_file = 'mmdetection/configs/mask_rcnn/my_mask_rcnn.py' # url: https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth checkpoint_file = 'mmdetection/work_dirs/my_mask_rcnn/latest.pth' device = 'cpu' # init a detector model = init_detector(config_file, checkpoint_file, device=device) # inference the demo image img = '../tool_seg_data/neee/rgb/1906.jpeg' result = inference_detector(model, img) show_result_pyplot(model, img, result, score_thr=0.3) Thank God we finally successfully train the segmentation network.","link":"/2021/10/29/mmdection-hammer-segmentation/"},{"title":"MoveIt Configuration","text":"在配置ROS-noetic MoveIt的时候，不可避免地遇到了一些问题。在此记录一下。 官网配置教程地址： https://ros-planning.github.io/moveit_tutorials/doc/getting_started/getting_started.html Catkin是ROS官方的构建编译系统，是原先的ROS编译构建系统rosbuild的继承者，它组合了cmake宏和python脚本来在基本的cmake工作流之上提供一些额外的功能。Catkin设计得比rosbuild更加便捷，允许更好地分配package，支持更好的交叉编译以及更好的便捷性。Catkin的工作流和cmake的工作流很像，但是还添加了自动find package的基础结构，并且在同时构建多个相互依赖的项目。 catkin编译的工作流程如下： 首先在工作空间catkin_ws/src/下递归的查找其中每一个ROS的package。package中会有package.xml和CMakeLists.txt文件，Catkin(CMake)编译系统依据CMakeLists.txt文件,从而生成makefiles(放在catkin_ws/build/)。 然后make刚刚生成的makefiles等文件，编译链接生成可执行文件(放在catkin_ws/devel)也就是说，Catkin就是将cmake与make指令做了一个封装从而完成整个编译过程的工具。catkin有比较突出的优点，主要是： 1.操作更加简单 2.一次配置，多次使用 3.跨依赖项目编译 使用catkin_make进行编译 123$ cd ~/catkin_ws #回到工作空间,catkin_make必须在工作空间下执行$ catkin_make #开始编译$ source ~/catkin_ws/devel/setup.bash #刷新坏境 编译完成后，如果有新的目标文件产生（原来没有），那么一般紧跟着要source刷新环境，使得系统能够找到刚才编译生成的ROS可执行文件。这个细节比较容易遗漏，致使后面出现可执行文件无法打开等错误。 第一次catkin build的时候，我们遇到如下错误： 根据回答，我们修改原先的catkin_build指令，修改为catkin build -DPYTHON_EXECUTABLE=/usr/bin/python3 -DPYTHON_INCLUDE_DIR=/usr/include/python3.7m。换句话说，ROS的catkin默认的是用python2.7去编译的，哪怕我们安装了python3-empy它依然报错。告诉它使用python3后这个报错就解决了。 第二个问题是tinyxml.h找不到tinystr.h文件。我们找到对应位置后在源码中添加TIXML_USE_STL即可解决这个问题，使用标准的STL库来代替tinystr.h。 第三处build的问题是 它提示我们找不到pyconfig.h，根据这篇博客的提示，我先使用sudo find / -name pyconfig.h，发现在路径/usr/include/python3.8下存在pyconfig.h，原来是解决问题1的时候复制的路径/usr/include/python3.7m根本不存在，把命令改为 12catkin build -DPYTHON_EXECUTABLE=/usr/bin/python3 -DPYTHON_INCLUDE_DIR=/usr/include/python3.8source devel/setup.bash 即可正常编译成功。 接下来我们使用命令 1roslaunch panda_moveit_config demo.launch rviz_tutorial:=true 这里会再次遇到一个问题，根据此回答即可解决。 自此我们可以正常运行并且看到Rviz界面，添加MotionPlanning后就可以在RViz界面中见到我们的panda机器人。 我们首先需要知道4种重叠的可视化： robot在planning environment中的configuration（各个关节的当前角度）。 robot的plan出来的路径 绿色：robot motion planning的开始状态 橙色：robot motion planning的结束状态 这些展示的状态可以在上面的motionplanning的下拉框中勾选。Planning scene在show robot visual-&gt;Scene Robot中；planning path在show robot visual-&gt;Planned Path中；开始状态在Query Start State-&gt;Planning Request中；结束状态的设置在Query Start State-&gt;Planning Request。 我们可以拖动各个关节的orientation来设置开始状态和结束状态。然后点击plan可以查看整个的运动轨迹。我们勾选use collision-aware IK就可以使得求解器尝试找到一个collision-free的轨迹；如果没有勾选的话，求解器会允许在过程中碰到collision。不过在可视化的时候，存在碰撞的link无论我们有没有勾选use collision-aware IK，都会被标识成红色。 在MotionPlanning的joint窗口下，我们可以拖动nullspace exploration，可以看到在末端执行器的位姿不变的情况下，其他机械臂的configuration是可以变化的（满足连续关系）。 现在我们可以开始使用panda机器人在moveit rviz插件中进行运动规划。步骤如下： 把start state设置到期望的位置 把goal state设置到期望的位置 确保这两个state都没有自己和自己碰撞 确保planned path是可见的 按下plan键。 我们在Planning窗口中可以选择不同的start和goal states，eg：当前的状态，之前的状态（之前的planning attempt的start state），一个随机采样的configuration，一个有名字的在srdf中定义的state。 我们也可以查看轨迹路径点，只需要在导航栏中打开panels-&gt;motionplanning-slider，即可看到对应的路径点并拖动。 如果我们勾选use cartesian path的话，robot会尝试直线地移动end effector。 [1]Catkin工作原理 https://blog.csdn.net/qq_33876441/article/details/102958248 [2]Catkin conceptual http://wiki.ros.org/catkin/conceptual_overview [3]https://blog.csdn.net/num8owl/article/details/108689843","link":"/2021/10/20/moveit-configuration/"},{"title":"moveit_planning_scene_tutorial","text":"moveit_planning_scene_tutorial Part1 Add primitive shape into planning scene​ The planning scene is a very useful tool when we want to create a collision-free trajectory for our robot. The first thing is that whatever our target is, the custom mesh object need to be added into the scene for realistic planning. We first introduce the tutorial on the moveit website. 12345678910111213141516171819202122232425262728293031323334353637383940414243......// Now let's define a collision object ROS message for the robot to avoid.moveit_msgs::CollisionObject collision_object;collision_object.header.frame_id = move_group_interface.getPlanningFrame();// The id of the object is used to identify it.collision_object.id = &quot;box1&quot;;// Define a box to add to the world.shape_msgs::SolidPrimitive primitive;primitive.type = primitive.BOX;primitive.dimensions.resize(3);primitive.dimensions[primitive.BOX_X] = 0.1;primitive.dimensions[primitive.BOX_Y] = 1.5;primitive.dimensions[primitive.BOX_Z] = 0.5;// Define a pose for the box (specified relative to frame_id)geometry_msgs::Pose box_pose;box_pose.orientation.w = 1.0;box_pose.position.x = 0.5;box_pose.position.y = 0.0;box_pose.position.z = 0.25;collision_object.primitives.push_back(primitive);collision_object.primitive_poses.push_back(box_pose);collision_object.operation = collision_object.ADD;std::vector&lt;moveit_msgs::CollisionObject&gt; collision_objects;collision_objects.push_back(collision_object);// Now, let's add the collision object into the world// (using a vector that could contain additional objects)ROS_INFO_NAMED(&quot;tutorial&quot;, &quot;Add an object into the world&quot;);planning_scene_interface.addCollisionObjects(collision_objects);// Show text in RViz of status and wait for MoveGroup to receive and process the collision object messagevisual_tools.publishText(text_pose, &quot;Add object&quot;, rvt::WHITE, rvt::XLARGE);visual_tools.trigger();visual_tools.prompt(&quot;Press 'next' in the RvizVisualToolsGui window to once the collision object appears in RViz&quot;);// Now when we plan a trajectory it will avoid the obstaclesuccess = (move_group_interface.plan(my_plan) == moveit::planning_interface::MoveItErrorCode::SUCCESS);...... ​ We also can attach objects to the robot, so that it moves with the robot geometry. This simulates picking up the object for the purpose of manipulating it. The motion planning should avoid collisions between the two objects as well. 123456789101112131415161718192021222324252627282930313233343536373839moveit_msgs::CollisionObject object_to_attach;object_to_attach.id = &quot;cylinder1&quot;;shape_msgs::SolidPrimitive cylinder_primitive;cylinder_primitive.type = primitive.CYLINDER;cylinder_primitive.dimensions.resize(2);cylinder_primitive.dimensions[primitive.CYLINDER_HEIGHT] = 0.20;cylinder_primitive.dimensions[primitive.CYLINDER_RADIUS] = 0.04;// We define the frame/pose for this cylinder so that it appears in the gripperobject_to_attach.header.frame_id = move_group_interface.getEndEffectorLink();geometry_msgs::Pose grab_pose;grab_pose.orientation.w = 1.0;grab_pose.position.z = 0.2;// First, we add the object to the world (without using a vector)object_to_attach.primitives.push_back(cylinder_primitive);object_to_attach.primitive_poses.push_back(grab_pose);object_to_attach.operation = object_to_attach.ADD;planning_scene_interface.applyCollisionObject(object_to_attach);// Then, we &quot;attach&quot; the object to the robot. It uses the frame_id to determine which robot link it is attached to.// You could also use applyAttachedCollisionObject to attach an object to the robot directly.ROS_INFO_NAMED(&quot;tutorial&quot;, &quot;Attach the object to the robot&quot;);move_group_interface.attachObject(object_to_attach.id, &quot;panda_hand&quot;);visual_tools.publishText(text_pose, &quot;Object attached to robot&quot;, rvt::WHITE, rvt::XLARGE);visual_tools.trigger();/* Wait for MoveGroup to receive and process the attached collision object message */visual_tools.prompt(&quot;Press 'next' in the RvizVisualToolsGui window once the new object is attached to the robot&quot;);// Replan, but now with the object in hand.move_group_interface.setStartStateToCurrentState();success = (move_group_interface.plan(my_plan) == moveit::planning_interface::MoveItErrorCode::SUCCESS);ROS_INFO_NAMED(&quot;tutorial&quot;, &quot;Visualizing plan 7 (move around cuboid with cylinder) %s&quot;, success ? &quot;&quot; : &quot;FAILED&quot;);visual_tools.publishTrajectoryLine(my_plan.trajectory_, joint_model_group);visual_tools.trigger();visual_tools.prompt(&quot;Press 'next' in the RvizVisualToolsGui window once the plan is complete&quot;); ​ Now we can create the collision-free trajectory with our attached object. Since, we can not replace everything with basic primitive shape in this tutorial, because such a simplification will cause some difference and mismatch in our digital twin and the real environment setting. Part2 Add custom object into planning scene​ This tutorial demonstrates the way to add the custom object into the planning scene. To be more specific, we need a .stl or .dae model file. This difference between .obj, .stl and .dae is described in blog1 and blog2, from which we can know the interconversion between .obj and .stl file can be easily conducted. 12345678910111213141516171819202122232425262728293031323334//Vector to scale 3D file units (to convert from mm to meters for example)Vector3d vectorScale(0.001, 0.001, 0.001);// Define a collision object ROS message.moveit_msgs::CollisionObject collision_object;// The id of the object is used to identify it.collision_object.id = &quot;custom_object&quot;;//Path where the .dae or .stl object is locatedshapes::Mesh* m = shapes::createMeshFromResource(&quot;&lt;YOUR_OBJECT_PATH&gt;&quot;, vectorScale); ROS_INFO(&quot;Your mesh was loaded&quot;);shape_msgs::Mesh mesh;shapes::ShapeMsg mesh_msg; shapes::constructMsgFromShape(m, mesh_msg);mesh = boost::get&lt;shape_msgs::Mesh&gt;(mesh_msg);//Define a pose for your mesh (specified relative to frame_id)geometry_msgs::Pose obj_pose;obj_pose.position.x = 0.1;obj_pose.position.y = 0.1;obj_pose.position.z = 1.0;// Add the mesh to the Collision object message collision_object.meshes.push_back(mesh);collision_object.mesh_poses.push_back(obj_pose);collision_object.operation = geometry_msgs::Pose::ADD;// Create vector of collision objects to add std::vector&lt;moveit_msgs::CollisionObject&gt; collision_objects;collision_objects.push_back(collision_object);// Add the collision object into the worldmoveit::planning_interface::MoveGroupInterface move_group(&lt;YOUR_PLANNING_GROUP&gt;);planning_scene_interface.addCollisionObjects(collision_objects); ​ Adding static meshes in the Rviz will be omitted because we want the pipeline to be automatic. ​ to be continued","link":"/2021/12/08/moveit-planning-scene-tutorial/"},{"title":"openGL和openCV中的摄像机参数的转化","text":"本文章的主题为研究openGL和openCV中的摄像机参数的转化。 首先要搞清楚，坐标旋转和坐标系旋转的概念。 坐标旋转（点的运动） 如上图，二维坐标系中的绕原点逆时针旋转度，则得到的B点满足： 把上式写成矩阵乘法的形式： 这是点的运动，坐标系（参照系）并没有发生变化 坐标系旋转（基变换） 如上图，把原先的xy基逆时针旋转度到st基，p在xy基下的坐标为，p在st基下的坐标为，求变换关系。 我们有 即 2是坐标系的旋转，点是不动的，得到的是不动的点在变化了的坐标系下的表示 点旋转β相当于坐标系旋转了-β。所以可以直接在1的基础上，把角度反转，就成了坐标系的旋转。 基变换和坐标变换平面解析几何中的直角坐标系有时候需要坐旋转，这实际上是坐标向量绕原点坐旋转，设坐标轴逆时针旋转的角度为，那么不难有，新坐标向量和原坐标向量之间的关系为： OpenCV中的内参矩阵K（计算机视觉——算法和应用P41） 有几种方式来描述上三角矩阵K，一种是 上式使用相互独立的参数来描述x和y维度的焦距和，s项刻画任何可能的传感器轴间的倾斜，这由传感器的安装没有与光轴垂直所引起，而是以像素坐标表达的光心。 在实践中，通过设置和s=0，在很多应用中会使用如下更简单的形式： 通常情况下，通过将原点大致设置在图像的中心，即，其中W和H是图像的高和宽（用像素表示，如600*480），就可以得到仅含一个未知量焦距f的完全可用的摄像机模型。注意上式中的焦距都是以像素为单位表示的，要与现实相机中的毫米焦距等区分开来。 要转化到现实中的距离，我们首先需要知道图像的现实宽度，如，然后我们可以通过公式：，这里的就是视场角，也就是FOV，下图就展示了一个的情况。 为了更好地理解OpenGL中的透视投影，我们先来讨论函数glFrustum。根据OpenGL的官方文档，“glFrustum描述了一个透视矩阵，提供了一个透视投影。”这句话没错，但是只说出了一半实情。事实上，glFrustum做了两件事情，首先它进行透视投影，然后它把结果转换到归一化设备坐标系(NDC)上。前者是投影几何中的常规操作，但是后者是OpenGL中特有的实现细节。 为了讲明白这件事情，我们需要把投影矩阵（Projection Matrix）分成两部分，也就是透视矩阵(Prespective Matrix)和NDC矩阵。 我们的相机内参矩阵可以描述透视投影，所以它是求解出Perspective Matrix的关键。而对于NDC矩阵，我们会使用OpenGL的glOrtho。 第一步：投影变换 我们的3x3内参矩阵K为了能在OpenGL中使用需要两个小的变更，一个是为了正确的裁剪，位置3,3的元素必须为-1，因为OpenGL的摄像机是从原点向z的负半轴看的，所以如果位置3,3的元素为正，摄像机前方的顶点在投影后将具有负的w坐标。原则上，这是可以的，但是由于 OpenGL 执行裁剪的方式，所有这些点都会被裁剪。 所以我们现在有了 对于第二个更改，我们需要保护失去的Z轴的深度信息，所以我们会在内参矩阵的基础上添加一行和一列，即： ，其中 新的第三行保持了Z值的顺序的同时，把-near和-far映射到它们自己（在归一化了w后）。这个结果就是在裁剪平面之间的点依旧在乘上了Perspective Matrix后依旧保持在裁剪平面之间。 第二步：变换到NDC NDC矩阵可以通过glOrtho函数提供。Perspective Matrix把一个视锥空间转化为了一个长方体空间，而glOrtho把长方体空间转化为归一化设备空间。 调用glOrtho需要六个参数left,right,bottom,top,near,far ，其中 调用它的时候，far和near就和前述的一样。而top,bottom,left,right的裁剪平面的选取对应原图像的维度和标定时的坐标规范。 举个例子，如果你的摄像机用WxH的左上角为零点的图像标定了，那么就该使用left = 0, right = W, bottom = H, top = 0，注意到H作为了bottom参数而0作为了top参数，这意味着y轴正半轴是向下的规范。 如果标定时使用的是y轴向上的坐标系，并且原点在图像中心的话，那么就是left = -W/2, right = W/2, bottom = -H/2, top = H/2. 注意到其实glOrtho的参数和透视矩阵有很大的关系，比如说把视景体(viewing volume)向左平移X等价于把主轴向右平移X。而让翻倍就等于让left和right参数减半。很明显，用这两个矩阵来描述这个投影是冗余的，但是分别去考察这两个矩阵允许我们分离相机几何学和图像几何学。 根据文献 https://stackoverflow.com/questions/60430958/understanding-the-view-and-projection-matrix-from-pybullet 在pybullet中， 内参矩阵K为： 和是光心，通常是图像中心。不过有以下不同， \\1. 维度，pybullet保持了第三行和第四列来保持深度信息，这和之前提到的OpenGL相机相同。 \\2. 第四行第三列的元素不是1而是-1 \\3. Pybullet中s=0 \\4. Pybullet中 首先，pybullet使用OpenGL，所以它使用的是列优先的顺序，所以从pybullet中读到的真正的projection matrix应当转置，或者使用numpyarray的order=’F’。 其次，把FOV转化为f的方程如下： 和 因此，pybullet把焦距乘以了2/h，这是因为Pybullet使用归一化设备坐标系（也就是对x除以图像宽度，来归一化到01，再乘以2到02，所以如果我们的光心在图像中间x=1的位置时，那么裁剪平面就归一化到了-1~1）。因此，pybullet的焦距是使用NDC下的正确的焦距长度。 在内参矩阵K中，k和l是mm/px的比例，在使用pybullet时，我们可以认为k=l=1 mm/px，换句话说，在pybullet形式的内参矩阵中，和是以像素为单位的，而整个矩阵的每个元素都是以mm为单位的。 考虑到以上的所有条件，在pybullet中，内参矩阵为： ，其中 把h=1000和FOV=90代入， def computeProjectionMatrixFOV(*args, **kwargs): # real signature unknown “”” Compute a camera projection matrix from fov, aspect ratio, near, far values “”” pass def computeViewMatrix(*args, **kwargs): # real signature unknown “”” Compute a camera viewmatrix from camera eye, target position and up vector “”” pass http://ksimek.github.io/2013/06/03/calibrated_cameras_in_opengl/ https://amytabb.com/ts/2019_06_28/#conversion-corner-1 http://www.info.hiroshima-cu.ac.jp/~miyazaki/knowledge/teche0092.html https://zhuanlan.zhihu.com/p/339199471","link":"/2021/10/15/opencv-opengl-camera-conversion/"},{"title":"pybullet-planning源码解析","text":"我们尝试来阅读一下pybullet-planning中和ik、planning相关的源码。 我们尝试从test_franka.py入手 首先是start_pose是起始的关于tool_link的6D位姿，也就是得到pos, orn 同理，end_pose是对于start_pose施加了一个相对位移z=-distance(1m)以后的位置 我们可以发现，每一次移动的位置确实是沿着z轴相对位置后退0.1m(因为我们分了十步去处理后退1m的最终目标) 分析multiply的源码，有比较陌生的poses参数，单个的意思其实就是按照数组形式处理传入的不定长个参数 术语表： solve joints: 需要用来求解ik的关节。 free joints: 需要在ik计算前指定的关节，这些值在运行时是已知的，但是在ik创建的时候是未知。(not known at IK generation time) 以panda_franka为例，传入的ikfast_info为IKFastInfo(module_name=’franla_panda.ikfast_panda_arm’, base_link=’panda_link0’, ee_link=’panda_link8’, free_joints=[‘panda_joint7’])。因为panda是7个自由度的机器人，所以需要指定一个joint为free joint。 以UR5为例，传入的infast_info为IKFastInfo(module_name=’ur5.ikfast_ur5_arm’, base_link=’base_link’, ee_link=’ee_link’, free_joints=[])。因为UR5是6个自由度的机器人，所以不需要指定free joint。 link_from_name：根据name得到link id 换句话说，对于一个link，施加get_ordered_ancesters以后可以得到包括这个link在内的其祖先link 以panda为例： get_ordered_ancestors(panda, ee_link)：[0, 1, 2, 3, 4, 5, 6, 7] get_ordered_ancesters(panda, tool_link)：[0, 1, 2, 3, 4, 5, 6, 7, 8] first_joint：连接了base_link的joint prune_fixed_joints：去除掉固定的joint以后，需要纳入计算的joint。 ik_joints：[0, 1, 2, 3, 4, 5, 6] free_joints：[6] assert set(free_joints) &lt;= set(ik_joints)：保证需要设置的free_joint包含在ik_joints内。 assert len(ik_joints) == 6 + len(free_joints)：保证剔除掉free_joints后为6个自由度，比如7自由度关节的机械臂需要1个free_joint，6自由度关节的机械臂需要0个free_joint。 difference_fn是一个求差值的函数，对于普通的joint来说，是直接两个参数相减，对于circular joint会有一些特殊的计算方式，但是circular joint在代码中的定义是upper limit &lt; lower limit的情况，并且google也并没有找到什么有用的信息，考虑到我们目前使用的机器人都没有circular joint，就直接当做两个值相减就可以了。 get_length函数就是单纯的求n-范数，范数通过norm参数传进去。 其实closet_inverse_kinematics的后半部分只是在对求出来的解进行排序，找到一个各关节位姿变化的距离最小的解输出。而前半部分是通过generator = ikfast_inverse_kinematics(…..)求解得到了所有的可行解。 在研究ikfast_inverse_kinematics之前，我们先来搞懂interval_generator Np.random.uniform从一个均匀分布[0,1)中随机采样，获得d个数据 Halton sequence:Halton序列是一种为数值方法（如蒙特卡洛模拟算法）产生顶点的系列生成算法。虽然这些序列是以确定的方法算出来的，但它们的偏差很小。也就是说，在大多数情况下这些序列可以看成是随机的。Halton系列于1960年提出，当时是作为quasi-random 数字序列的一个例子。 所以unit_generator是从均匀分布[0,1)中采样d个数据。而在interval_generator这个函数中，也就是随机采样d个0~1之间的权重，传入convex_conbination中。 所以其实就是在lower和upper之间随机取d组点，如果joint lower limit = joint upper limit，那么自然就直接取相等。 所以，调用interval_generator会返回一个满足条件的joints的各个角度。 接下来，我们再来尝试搞懂ikfast_inverse_kinematics \\1. 首先import_ikfast函数会根据我们传入的ikfast_info去import对应机器人的cpp编译出来的模组，每次需要使用新的机器人时，都要准备好这件事情。 \\2. ik_joints和free_joints相对比较容易理解，上文中有提到过这件事情。 \\3. lower_limits和upper_limits就是joint的上下界，free_deltas不详。 \\4. Islice(generator, max_attempts)，迭代器generator生成max_attempt个proposal后结束 所以generator迭代器将原先的joint pose和随机数采样得到的joint pose可行解传入compute_inverse_kinematics中，会继续被传入到每个机械臂特有的ikfast cpp中，具体作用不详，在ur5的ikfast代码中，似乎是在某些条件下会使用采样数据作为某个joint的值。 在ikfast cpp的源码中，我们可以看到这样的表述Computes all IK solutions given a end effector coordinates and the free joints. pfree is an array specifying the free joints of the chain. 换句话说，我们传入的采样得到的sampled参数实际上是指定了对应free joint的本次求解中的值，因为free joint是需要我们指定的，这无可厚非。 \\5. 在传入对应参数以后，满足条件的解会yield出来，所以其实ikfast_inverse_kinematics生成了一个产生对应configuration下的ik解的迭代器。solutions = list(generator)，也就是把这些解都放进list中，因为实现传入的时候，我们保证了要么max_attempts不为INF，要么max_times不为INF，所以不会出现generator会成为一个无限长的迭代器的情况，如果max_times设大了，可能会影响实时性。 接下来，我们来考察pybullet_inverse_kinematics函数。 具体流程是调用multiple_sub_inverse_kinematics，然后最后可以得到一堆解。 它实现逻辑是这样的： 每次需要计算ik的时候，sample一些在各个joint limit范围内的整个机械臂的姿态，创建对应的subrobot，然后subrobot会以ik target pose为目标，迭代式地求解这个ik，也就是求解一次ik，set到对应位置，再求解ik……，迭代约200次，然后最后check最终的位姿和ik的target位姿的差值是不是小于某个阈值，如果小于则成功。 其中sub_robot其实就是在Pybullet环境中创建一个不考虑碰撞的、不可见的、相同pos的robot，只用来计算ik，在计算完成后，就会把sub_robot删除。 Motion-planning 首先是refine_path这个函数，这个函数比较简单，也就是传入一组waypoint（每个waypoint是joint的一组值），refine_path会在每相邻两个waypoint中插入num_steps个中间点，作为新的path。 get_extend_fn(resolution)这个函数也是返回一个函数是根据resolutions确定在两个waypoint之间需要插值几个waypoint点，resolution默认值为3度，也就是0.05弧度。然后调用之前所提到的get_refine_fn函数。resolution对于不同的joint可以设置不同的joint。 remove_redundant其实就是把位姿path中太近（2范数小于1e-3）的点移除。 模拟一下这个情况，首先把path[0]加入waypoint中，然后考察下一个path[i]。对于对于相邻的两次，考察相邻path[i - 1]-&gt;path[i]和path[i]-&gt;path[i+1]这两个向量，我们规范化后考察其方向，如果方向一致的话就不再加到waypoints中。所以相对于path是path中一些平均相距为resolution的位姿。 interpolate_joint_waypoints，也就是对给定的waypoint，通过不同joint的resolution来插值得到一个均匀resolution的waypoints的路径。并且如果其中插值的路径中检测到了碰撞，直接输出空路径。 其实也是带了碰撞检测的interpolate_joint_waypoint。碰撞检测发生在枚举waypoint的需要插值的节点上。注意和interpolate_joint_waypoints中碰撞检测的对象的区分（一个是需要插值的节点，一个是插值后的新增节点）。 可以看到机器学习分为两个阶段，比如倒数第三段，check_link_pair循环中，就是判断自己的link之间的碰撞，也就是self-collision。在倒数第二段，其实就是check我们的body和obstacles之间的collision。 关于这个函数中提供的碰撞检测，有aabb碰撞检测和pair_wise_collision碰撞检测。Aabb比较容易理解，而pair_wise_collision主要是通过枚举需要判断的两个物体之间的笛卡尔积，分别通过p.getClosestPoints是否等于0来判断。","link":"/2021/10/15/pybullet-planning/"},{"title":"pybind11初探","text":"最近项目过程中经常遇到底层为C++代码的情况，作为一个契机，详尽地研究一下pybind11。 虽然无关紧要，但是很在意的问题是11这个数字是哪来的，因为pybind11的介绍是 Seamless operability between C++11 and Python，据称其主要内核代码使用了C++11的语言特性，如匿名函数、元组等。 如上图所示，这是符合pybind11编程要求的c++代码。 图 1 https://zhuanlan.zhihu.com/p/92120645 根据pybind11官网教程，可以使用如下代码进行编译： c++ -O3 -Wall -shared -std=c++11 -fPIC $(python3 -m pybind11 –includes) example.cpp -o example$(python3-config –extension-suffix) -Wall 这个编译选项会强制输出所有警告，用于调试。 -o output_filename 确定输出文件的名称为output_filename。同时这个名称不能和源文件同名。如果不给出这个选项，gcc就给出默认的可执行文件a.out。 -fPIC 生成位置无关代码。 -I/home/baochen/anaconda3/include/python3.8 -I/home/baochen/anaconda3/lib/python3.8/site-packages/pybind11/include .cpython-38-x86_64-linux-gnu.so 案例分析：pybullet-planning中的ikfast编译件 我们考察如何将一个新的机械臂添加到pybullet-planning中。 首先考察已有的franka_panda机械臂，文件夹内容如下： setup.py内容如下： 调用的compile文件如下： 基本上就是传入模块名称和cpp文件的路径，然后构建出一个python可以调用的模块。 同目录的ik.py提供了一个向python暴露的接口，其中的IKFastInfo是一个namedtuple，其实就是指定了我们编译出来的模块名字(setup.py中的robot_name和compile_ikfast中的module_name)","link":"/2021/10/15/pybind11/"},{"title":"complier-lab5","text":"This lab comes from SE3355 *Compilers* lab5, which requires me to implement a complete runnable tiger compiler. ​ 致谢：Lab5的完成离不开Girafboy学长仓库所提供的参考和思路，为了避免有抄袭之嫌，我将重新以我的思路阐述清楚我的代码。 ​ 首先明确，我们这个Lab5的目标是把做完escape analysis的AST转化为tree language(middle IR)，再转化为assem(lower IR)。正如院长所说，前4个Lab其实是把代码文本转化成了AST，只生成了一个IR，但是我们需要在Lab5一下子完成两个IR的转换和生成，并且我们生成tree language后，其实非常不好Debug，因为有各种各样的嵌套语句，也没有解释器来解析tree language，我们必须等到生成了assem后才能够被汇编解释器所解释执行。 Part1 AST翻译成Tree Language​ 前半部分是根据AST翻译成tree language。这部分相对比较简单，但是一件很麻烦的事情就是translation和Lab4的semantic analysis是非常紧密地intertwine在一起的，如果我们希望在Translate中调用SemAnalyze，会导致大量的代码重构，基本上是一整个Lab4的工作量加上额外的Debug时间，并且还涉及到一个非常trivial，还是很麻烦的点： 12345type::Ty *SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount, err::ErrorMsg *errormsg) override;tr::ExpAndTy *Translate(env::VEnvPtr venv, env::TEnvPtr tenv, tr::Level *level, temp::Label *label, err::ErrorMsg *errormsg) override; ​ 我们可以看到，在模板所提供的的接口定义中，Translate函数是没有labelcount参数的，所以它并不能判断某个break语句是否在某些循环语句的里面，我们必须使用SemAnalyze来判定这个错误，所以哪怕我们把语义分析的语句全部迁移到Translate中，还是会有这个问题。为了避免过于丑陋的实现，我在这里纠结了一段时间，不过后来我仔细地查看了Lab5的测试代码（而不是Lab5-part1，Lab5-part1测试脚本没有Semantic analysis阶段，误导了我很久），如下所示： 123456789101112131415161718192021222324...{ // Lab 4: semantic analysis TigerLog(&quot;-------====Semantic analysis=====-----\\n&quot;); sem::ProgSem prog_sem(std::move(absyn_tree), std::move(errormsg)); prog_sem.SemAnalyze(); absyn_tree = prog_sem.TransferAbsynTree(); errormsg = prog_sem.TransferErrormsg();}{ // Lab 5: escape analysis TigerLog(&quot;-------====Escape analysis=====-----\\n&quot;); esc::EscFinder esc_finder(std::move(absyn_tree)); esc_finder.FindEscape(); absyn_tree = esc_finder.TransferAbsynTree();}{ // Lab 5: translate IR tree TigerLog(&quot;-------====Translate=====-----\\n&quot;); tr::ProgTr prog_tr(std::move(absyn_tree), std::move(errormsg)); prog_tr.Translate(); errormsg = prog_tr.TransferErrormsg();}... ​ 所以虽然上课说应该在translation的时候做同时做类型检查的，在Lab中评测中还是没有要求的这么严格。为了避免在Translate阶段又做一遍类型检查，我对AST上的所有节点添加了一个成员type::Ty *type，这样我们做完SemAnalyze后就可以缓存住每个节点的真正类型，然后在Translate阶段使用。这样我们基本上就完成了做Translate的准备动作。 ​ Translate接口的定义如下： 123tr::ExpAndTy *Translate(env::VEnvPtr venv, env::TEnvPtr tenv, tr::Level *level, temp::Label *label, err::ErrorMsg *errormsg) override; ​ 解决掉Translate的关键就是理解level和label参数的含义，理解了自然后面就都好做了。 ​ 我们先来看label，涉及到label修改和使用的只有两处： 12345678910111213141516tr::ExpAndTy *WhileExp::Translate(env::VEnvPtr venv, env::TEnvPtr tenv, tr::Level *level, temp::Label *label, err::ErrorMsg *errormsg) { tr::Exp *exp = nullptr; temp::Label *done_label = temp::LabelFactory::NewLabel(); tr::ExpAndTy *check_test = test_-&gt;Translate(venv, tenv, level, label, errormsg); tr::ExpAndTy *check_body = body_-&gt;Translate(venv, tenv, level, done_label, errormsg); ......}tr::ExpAndTy *BreakExp::Translate(env::VEnvPtr venv, env::TEnvPtr tenv, tr::Level *level, temp::Label *label, err::ErrorMsg *errormsg) { tree::Stm *stm = new tree::JumpStm(new tree::NameExp(label), new std::vector&lt;temp::Label *&gt;({label})); return new tr::ExpAndTy(new tr::NxExp(stm), type);} ​ 注意到for循环最终是规约到while的，所以for循环没有使用到label。这样我们就很清楚了，label就是为了让break语句跳出循环使用的。换句话说，就是循环中done:的位置。这样看来，如果我们希望在translation中一遍同时做类型检查，没有labelcount也无伤大雅，我们只需要判断label是不是等于外面传入的默认值即可。 ​ 接下来是level函数，它主要的意义就是负责static link。换句话说，如果我们在$level=k_1$的函数体中引用到了$level=k_2&lt;k_1$的变量，那么我们需要通过$k_1-k_2$次static link跳转，找到这个变量所在的frame，并且通过其InReg还是InFrame来获取到具体的值。 ​ 所以，level的修改只会出现在FunctionDec的时候： 12345678910tr::Exp *FunctionDec::Translate(env::VEnvPtr venv, env::TEnvPtr tenv, tr::Level *level, temp::Label *label, err::ErrorMsg *errormsg) { for (FunDec *fundec : functions_-&gt;GetList()) { type::TyList *formaltys = make_formal_tylist(tenv, fundec-&gt;params_); tr::Level *new_level = tr::Level::NewLevel(level, fundec-&gt;name_, make_formal_esclist(fundec-&gt;params_)); ...... tr::ExpAndTy *entry = fundec-&gt;body_-&gt;Translate(venv, tenv, funentry-&gt;level_, funentry-&gt;label_, errormsg); }} StaticLink的处理​ static link的核心逻辑如下所示： 123456789tree::Exp *StaticLink(tr::Level *target, tr::Level *level) { tree::Exp *staticlink = new tree::TempExp(reg_manager-&gt;FramePointer()); while(level != target){ frame::Access *sl = level-&gt;frame_-&gt;formals.back(); staticlink = sl-&gt;ToExp(staticlink); level = level-&gt;parent_; } return staticlink;} ​ 当我们在某个函数中尝试使用一个变量的时候，我们需要在venv中找到此变量的定义层数，并且调用StaticLink函数找到对应的层数。 123456789101112131415161718192021222324tr::Exp *TranslateSimpleVar(tr::Access *access, tr::Level *level) { tree::Exp *real_fp = StaticLink(access-&gt;level_, level); return new tr::ExExp(access-&gt;access_-&gt;ToExp(real_fp));}tr::ExpAndTy *CallExp::Translate(env::VEnvPtr venv, env::TEnvPtr tenv, tr::Level *level, temp::Label *label, err::ErrorMsg *errormsg) { ...... auto *list = new tree::ExpList(); for (Exp* args_p:args_-&gt;GetList()) { tr::ExpAndTy *check_arg = args_p-&gt;Translate(venv, tenv, level, label, errormsg); list-&gt;Append(check_arg-&gt;exp_-&gt;UnEx()); } if(!fun_entry-&gt;level_-&gt;parent_) { exp = new tr::ExExp(frame::externalCall(func_-&gt;Name(), list)); } else { list-&gt;Append(StaticLink(fun_entry-&gt;level_-&gt;parent_, level)); exp = new tr::ExExp(new tree::CallExp(new tree::NameExp(func_), list)); } return new tr::ExpAndTy(exp, type);} ​ 注意在CallExp中，如果我们是外部调用的话，不需要添加static link参数，因为外部调用要求不传入static link。 外部调用的处理123tree::Exp *externalCall(std::string s, tree::ExpList *args) { return new tree::CallExp(new tree::NameExp(temp::LabelFactory::NamedLabel(s)), args);} 关系运算的翻译123456789101112131415161718192021222324case EQ_OP:case NEQ_OP:{ tree::CjumpStm *stm; switch (oper_) { case EQ_OP: if (dynamic_cast&lt;type::StringTy *&gt;(leftExpAndTy-&gt;ty_) != nullptr) { auto expList = new tree::ExpList({leftExp, rightExp}); stm = new tree::CjumpStm(tree::RelOp::EQ_OP, frame::externalCall(&quot;string_equal&quot;, expList), new tree::ConstExp(1), nullptr, nullptr); } else stm = new tree::CjumpStm(tree::RelOp::EQ_OP, leftExp, rightExp, nullptr, nullptr); break; case NEQ_OP: stm = new tree::CjumpStm(tree::RelOp::NE_OP, leftExp, rightExp, nullptr, nullptr); break; } auto trues = new temp::Label*[2]; auto falses = new temp::Label*[2]; trues[0] = stm-&gt;true_label_; trues[1] = nullptr; falses[0] = stm-&gt;false_label_; falses[1] = nullptr; exp = new tr::CxExp(trues, falses, stm); break;} ​ 其实就是一个CxExp的构造，我们需要把true_label和false_label的地址传入，以便之后回填。 Part2 Tree Language翻译成Assem基本数据结构12345678910class Frame {public: temp::Label *label; std::list&lt;Access*&gt; formals; int s_offset; Frame() {}; Frame(temp::Label *name, std::list&lt;bool&gt; *escapes) : label(name) {} virtual frame::Access *allocLocal(bool escape) = 0;}; ​ Frame主要是存放函数的名字、寄存器的escape信息、以及栈上的分配offset。 1234567891011121314151617181920212223class X64RegManager : public RegManager {public : X64RegManager() { …… caller_saved_regiters = new temp::TempList({rax, rdi, rsi, rdx, rcx, r8, r9, r10, r11}); callee_saved_registers = new temp::TempList({rbx, rbp, r12, r13, r14, r15}); registers = new temp::TempList({rax, rdi, rsi, rdx, rcx, r8, r9, r10, r11, rbx, rbp, r12, r13, r14, r15, fp, rsp}); args_registers = new temp::TempList({rdi, rsi, rdx, rcx, r8, r9}); ret_sink_registers = new temp::TempList({rsp, rax, rbx, rbp, r12, r13, r14, r15}); allregs_noRSP = new temp::TempList({rax, rdi, rsi, rdx, rcx, r8, r9, r10, r11, rbx, rbp, r12, r13, r14, r15, fp}); } //caller-saved registers temp::Temp *rax, *rdi, *rsi, *rdx, *rcx, *r8, *r9, *r10, *r11; //callee-saved registers temp::Temp *rbx, *rbp, *r12, *r13, *r14, *r15; temp::Temp *fp; temp::Temp *rsp; temp::TempList *caller_saved_regiters, *callee_saved_registers, *registers, *args_registers, *ret_sink_registers, *allregs_noRSP;}; 其他情况​ 其他情况不再赘述，详见之后公开的Github仓库。 ProcEntryExit1​ 主要是用来做view shift的，也就是把传入的寄存器参数存放到函数内来看它的位置。强调我的实现只是完成了功能，并没有性能上的考虑。所以，考虑到多个参数的情况，我先把所有传入的寄存器上的值和6个参数以外的栈上的值全部移到临时寄存器中，再重新根据我们的需要安排到栈上，此处的多余move应当是不必要的。 1234567891011121314151617181920212223tree::Stm *procEntryExit1(frame::Frame *frame, tree::Stm *stm) { int num = 1; tree::Stm *viewshift = new tree::ExpStm(new tree::ConstExp(0)); int total_size = frame-&gt;formals.size(); std::vector&lt;temp::Temp *&gt; tempList; for (frame::Access *formal : frame-&gt;formals) { temp::Temp *reg = temp::TempFactory::NewTemp(); if (num &lt;= 6) { viewshift = new tree::SeqStm(viewshift, new tree::MoveStm(new tree::TempExp(reg), new tree::TempExp(reg_manager-&gt;ARG_nth(num)))); } else { viewshift = new tree::SeqStm(viewshift, new tree::MoveStm(new tree::TempExp(reg), tree::NewMemPlus_Const(new tree::TempExp(reg_manager-&gt;FramePointer()), -1 * (num - 6) * reg_manager-&gt;WordSize()))); } tempList.push_back(reg); num++; } num = 1; for (frame::Access *formal : frame-&gt;formals) { viewshift = new tree::SeqStm(viewshift, new tree::MoveStm(formal-&gt;ToExp(new tree::TempExp(reg_manager-&gt;FramePointer())), new tree::TempExp(tempList[num - 1]))); num++; } return new tree::SeqStm(viewshift, stm);} ProcEntryExit2​ ProcEntryExit2是要在翻译完的body instruction后添加一条空指令，告诉我们函数结束的时候哪些寄存器是要被用到的。 12345assem::InstrList *procEntryExit2(assem::InstrList *body) { temp::TempList *returnSink = reg_manager-&gt;ReturnSink(); body-&gt;Append(new assem::OperInstr(&quot;&quot;, nullptr, returnSink, nullptr)); return body;} ProcEntryExit3​ ProcEntryExit3生成过程入口处理和出口处理的汇编语言代码，主要是对栈指针的更改，以及设置framesize变量。 1234567891011121314151617assem::Proc *procEntryExit3(frame::Frame *frame, assem::InstrList * body) { static char instr[256]; std::string prolog; int size = -frame-&gt;s_offset - 8; sprintf(instr, &quot;.set %s_framesize, %d\\n&quot;, frame-&gt;label-&gt;Name().c_str(), size); prolog = std::string(instr); sprintf(instr, &quot;%s:\\n&quot;, frame-&gt;label-&gt;Name().c_str()); prolog.append(std::string(instr)); sprintf(instr, &quot;subq $%d, %%rsp\\n&quot;, size); prolog.append(std::string(instr)); sprintf(instr, &quot;addq $%d, %%rsp\\n&quot;, size); std::string epilog = std::string(instr); epilog.append(std::string(&quot;retq\\n&quot;)); return new assem::Proc(prolog, body, epilog);} 处理传入大于6个参数的函数调用的情况首先，我们在tigermain中存在一个返回值-1，所以当我们主程序结束后，会根据这个-1跳转到run函数的循环。 123456# interpreter.pydef run(self): pc = self._state_table.get_pc() while pc &gt;= 0: self._instructions[pc].execute(self._state_table) pc = self._state_table.get_pc() ​ 如下图所示，我们目前创造了A的栈帧，并且把传入的参数都根据其是否逃逸放到了对应的寄存器和栈上。 ​ 我们假设此时A希望调用函数B，并且需要传入8个参数+1个static link，由于传参寄存器只有6个，所以后两个参数（记作23和45）和static link都需要放在栈上。 ​ 注意此时，在%rsp-8处我们需要填充一个空的八字节，然后再依次放入23,45和static link。上图为call(B)前的栈上状态。 ​ 在call(B)后，解释器会帮我们填入A的返回地址pc的位置，所以这八个字节是要空出来的，如上图所示。 12345678910111213# state_table.pydef call(self, label): # print(self._reg_table) label = label.replace('@PLT', '') if label in self._label_table: self._func_name_stack.append(self._current_func) self._current_func = label rsp = self.load_reg('%rsp') - 8 self.store_mem(rsp, self._pc) self.store_reg('%rsp', rsp) self._pc = self._label_table[label] self._func_temp_stack.append(self._temp_table.copy()) self._temp_table.clear() ​ 如下图所示，这是我们调用call(B)瞬间，还没有对传入参数做寄存器和栈分配时，栈上的情况。 ​ 接下来，我们继续对寄存器做escape allocation，最终得到一个完整的B运行时栈。 ​ 故代码如下： 12345678910111213141516171819202122232425temp::TempList *ExpList::MunchArgs(assem::InstrList &amp;instr_list, std::string_view fs) { auto tempList = new temp::TempList(); int num = 1; int out_formal = exp_list_.size() - 6; if (out_formal &gt; 0) instr_list.Append(new assem::OperInstr(&quot;subq $&quot; + std::to_string(reg_manager-&gt;WordSize()) + &quot;,%rsp&quot;, nullptr, nullptr, nullptr)); for (tree::Exp *exp : exp_list_) { temp::Temp *arg = exp-&gt;Munch(instr_list, fs); tempList-&gt;Append(arg); if (reg_manager-&gt;ARG_nth(num)) { instr_list.Append(new assem::MoveInstr(&quot;movq `s0, `d0&quot;, new temp::TempList(reg_manager-&gt;ARG_nth(num)), new temp::TempList(arg))); } else { instr_list.Append(new assem::OperInstr(&quot;subq $&quot; + std::to_string(reg_manager-&gt;WordSize()) + &quot;,%rsp&quot;,nullptr, nullptr, nullptr)); instr_list.Append(new assem::MoveInstr(&quot;movq `s0, (`d0)&quot;, new temp::TempList(reg_manager-&gt;StackPointer()), new temp::TempList(arg))); } num++; } if (out_formal &gt; 0) instr_list.Append(new assem::OperInstr(&quot;addq $&quot; + std::to_string(reg_manager-&gt;WordSize() * (out_formal + 1)) + &quot;,%rsp&quot;,nullptr, nullptr, nullptr)); return tempList;}","link":"/2021/12/14/complier-lab5/"},{"title":"pointnet_and_related_works","text":"​ 最近一门机器学习的课有一个阅读论文的作业，我选了老本行votenet，不过发现自己一些实现细节还是不过关。借着这个机会再从pointnet开始梳理一下思路并且解读一下源码。讲解视频 PointNet​ PointNet一定是3D pointcloud perception的里程碑，保留了大道至简的美感的同时，给出了严格的证明。是从每个角度都应该称赞的作品。 ​ 正如原文所说，又是空间中的n个点的集合有如下特性： 无序性（输出应当与输入的n个点的排序无关） 点和其邻居之间是有一些依赖关系的，我们设计的网络需要能够从邻居点中提取出局部特征 旋转和位移不变性（不过pointnet和pointnet++对旋转的处理都不是很好） 所以pointnet使用了max-pooling层来作为对称函数解决点云无序性的问题。 PointNet的证明​ 我们令$\\chi={S:S\\subseteq[0,1]^m\\text{and}|S|=n}$，此处的S就是我们的input，即一个欧式空间中的点集实例。此处做了归一化处理，所以它是模长恒定的m维向量。而$\\chi$自然就是欧式空间中的点集的集合。我们希望拟合一个定义在集合上的连续函数$f:\\chi\\rightarrow\\mathbb{R}$，注意此处的$\\mathbb{R}$是指实数空间，而不是实数。 ​ 因为这个函数是连续函数，所以在点集$S,S’\\in\\chi$上有如下性质：$$\\forall\\varepsilon&gt;0,\\exist\\delta&gt;0 \\\\\\textbf{if } d_h(S,S’)&lt;\\delta \\\\\\textbf{then} |f(S)-f(S’)|&lt;\\varepsilon$$​ 其中的$d_h$其实就是集合距离（豪斯多夫距离），我们只需要把上述认为是普通函数连续性的推广即可。 ​ 对于PointNet来说，它得到的拟合出来的函数的形式是：$\\gamma(\\mathop{MAX}_{x_i\\in S}{h(x_i)})$。其中，我们令$S={x_1,x_2,…,x_n}$，其中$x_i\\in R^N$，比如说我们单纯的空间位置的话，那就是$N=3$，而$h(x_i)$其实就是对这n个点做shared-MLP，得到的结果做一个maxpooling出一个$1\\times1024$维的global feature。然后$\\gamma$其实就是最后处理global feature的MLP。 ​ 我们需要证明的就是$$\\forall\\varepsilon&gt;0,\\exist \\text{ such a function} \\\\\\textbf{then} |f(S)-\\gamma(\\mathop{MAX}_{x_i\\in S}{h(x_i)})|&lt;\\varepsilon$$ 相对直观的分析​ 我们可以简单地认为$h(·)$就是把$x_i$映射到空间网格中的一个格子里，我们记这个空间网格的大小为$M\\times M\\times M$。那么因为点只会映射到一个网格中，我们可以得到$h(x_i)$就是一个大小为$1\\times M^3$的向量，并且只有一个值为1，其余值为0。 而MAX函数我们可以认为是使用空间网格来重建我们的输入点云，我们可以令这个网格足够密集（M足够大），使得每个网格至多包含一个原先的点，这样就等于我们做完max-pooling后，得到了一个$1\\times M^3$的向量，其中有n个点为1，其余为0。 ​ 因为网格的密度可以足够大，所以我们可以使用网格模型以任意精度去近似我们原先的点云集合$S$。接下来的$\\gamma$其实就是对这个新的表示形式的$1\\times M^3$向量(global feature)去做一个MLP。因为MLP可以近似任意函数，那它自然也能近似$f$函数，得证。本证明过程参考了深蓝学院的点云课程。 论文中的纯数学分析参考CSDN。知乎。 PointNet++​ PointNet只使用一个max-pooling层来整合全局信息，而PointNet++使用层级结构来逐层提取特征，并且不断地从层级中抽象出更大的局部区域。在PointNet++中，主要是通过set abstraction layer来实现的，它包含了sampling layer（最远点采样）, grouping layer和PointNet layer。 Sampling Layer​ 其实这一部分就是一个最远点采样的工作。也就是输入的维度是$B\\times N \\times(C+D)$，通常情况下$C=3$，而$D$就是特征维度，并且我们已知当前层需要采样到$\\text{npoints}$个点。那么其实就是$B\\times N \\times (C+D)\\rightarrow B\\times\\text{npoints}\\times(C+D)$。 Grouping Layer​ 这个层的目标是找到每个点的邻居，其实就是从半径为$r$的球中找到至多$\\text{nsample}$个元素。 PointNet Layer​ 这个其实就是拿原先的PointNet来提取特征。 Set Abstraction Layer的MSG优化 ​ 因为原先的set abstraction layer是在固定的一个半径上去做的，感受野是固定大小。而MSG就是在多个不同的半径上去提取特征，最后组合在一起。 Point Feature Propagation​ 在set abstraction layer中，原先的点集被降采样了。然而在点分割任务中，我们需要对每个点获取到一个点的种类标签，所以我们希望得到原先所有点的一个特征。一个方法就是在set abstraction layer中，我们永远采样所有的点作为中心点，但是这会导致计算消耗非常大，另一个方法就是使用point feature propagation。 ​ 在feature propagation layer中，它的输入是$N_i\\times(d+C)$，而它的输出的$N_{i-1}\\times(d+C)$，注意到其中的$N_i$就是在每层的set abstraction layer中的大小。所以其实这就是一个Decoder，并且在每一层Decode得到了全局特征后，再拼上了原先set abstraction layer的局部特征。 代码​ 接下来我们尝试来理解PointNet++的源码，PointNet++提供了三种任务的代码：classification、part segmentation和semantic segmentation，而set abstraction layer分为了SSG(single-scale grouping)和MSG(multi-scale grouping)。通常有纯Python版本的Pytorch实现和带有Cuda实现功能函数的Pytorch实现，因为最原先开源的版本是Tensorflow版本，这两个Pytorch版本在变量命名上都借鉴了原先版本的命名，但是这和论文中的参数命名是一个都对不上。并且个别出出现了破坏软件抽象规则的地方，如在PointNetSetAbstractionMsg中调用了类似于sample_and_group的结构但并没有复用代码，亦或者sample_and_group_all其实可以规约到sample_and_group，但是又重新写了一个函数等一系列问题。所以总体代码看起来比较痛苦，不过当我们彻底搞清楚代码以后，我们就可以把PoineNet++当做开箱即用的东西，再也不管它的底层实现了。 sample_and_group的实现12345678910111213141516171819202122232425262728293031323334def sample_and_group(npoint, radius, nsample, xyz, features, returnfps=False): &quot;&quot;&quot; Input: npoint: N_{i+1} radius: 查询半径 nsample: 考察至多几个邻域中的点 xyz: input points position data, [B, N, 3] features: input points feature data, [B, N, C] Return: new_xyz: sampled points position data, [B, npoint, nsample, 3] new_FEATURES: sampled points feature data, [B, npoint, nsample, 3+D] &quot;&quot;&quot; B, N, d = xyz.shape # d = 3 fps_idx = farthest_point_sample(xyz, npoint) # 最远点采样，从N_{i}个点中选出N_{i+1}个作为中心点 new_xyz = index_points(xyz, fps_idx) # [B, N_i, 3] -&gt; [B, N_{i+1}, 3] idx = query_ball_point(radius, nsample, xyz, new_xyz) grouped_xyz = index_points(xyz, idx) # [B, N_{i+1}, nsample, 3] grouped_xyz_norm = grouped_xyz - new_xyz.view(B, npoint, 1, d) # 此处的new_xyz先变为[B, N_{i+1}, 1, d]，然后加减法自动repeat，最终得到 [B, N_{i+1}, nsample, 3] # 此处就是把所有得到的点都转化为了相对于中心点的位置。 if features is not None: grouped_features = index_points(features, idx) # [B, N_{i+1}, nsample, C_i] new_features = torch.cat([grouped_xyz_norm, grouped_features], dim=-1) # [B, N_{i+1}, nsample, 3+C_i] else: new_features = grouped_xyz_norm if returnfps: return new_xyz, new_features, grouped_xyz, fps_idx else: return new_xyz, new_features # new_xyz = [B, N_{i+1}, K_{i+1}, 3] # new_features = [B, N_{i+1}, K_{i+1}, 3+C_i] query_ball_point的实现12345678910111213141516171819202122232425def query_ball_point(radius, K, xyz, new_xyz): &quot;&quot;&quot; Input: radius: local region radius nsample: max sample number in local region，注意这个邻居是要在原先的N_i个点中找的。 xyz: all points, [B, N_i, 3] new_xyz: query points, [B, N_{i+1}, 3] Return: group_idx: grouped points index, [B, N_{i+1}, K] &quot;&quot;&quot; device = xyz.device B, N, C = xyz.shape _, npoint, _ = new_xyz.shape group_idx = torch.arange(N, dtype=torch.long).to(device).view(1, 1, N).repeat([B, npoint, 1]) # 每个S（中心点）都有一个向量[1, 2, 3, ..., N]，如果这个向量里的值为N，那么就是不在中心点的邻域里；如果向量值等于自己，就是在中心点的邻域里 # 此时我们得到group_idex = [B * N_{i+1} * N_{i}] sqrdists = square_distance(new_xyz, xyz) # 输出的大小为 [B, N_{i+1}, N_{i}]，每个[N_{i+1}, N_{i}]里记录了两个点之间的距离 group_idx[sqrdists &gt; radius ** 2] = N # 那些半径不满足条件的索引值都设置为N group_idx = group_idx.sort(dim=-1)[0][:, :, :K] # 按照序号从小到大排，eg:[1, 3, 4, 5, ..., N, N, N] # 排完序后取前K个，那也就是[B, N_{i+1}, K]，意思就是每个中心点都对应了K个邻居的序号。 group_first = group_idx[:, :, 0].view(B, S, 1).repeat([1, 1, K]) mask = group_idx == N group_idx[mask] = group_first[mask] # 对于每个中心点的满足半径的邻居可能少于K个的情况，全部用第一个邻居的序号来替代。 return group_idx Classificaction Task​ 这一部分其实就是论文中的这个结构： ​ 我们注意到在实现中有$N_1=512,N_2=128$，$d=3$是点云的欧式空间坐标，如果点云输入有法向量数据，那么$C=3$，否则$C=0$。并且中间层有$C_1=128,C_2=256，C_4=1024,k=\\text{num_class}$，在数据集modelnet40下，$k=\\text{num_class}=40$。 1234567891011121314151617181920212223242526272829303132333435class get_model(nn.Module): def __init__(self,num_class,normal_channel=True): super(get_model, self).__init__() in_channel = 6 if normal_channel else 3 self.normal_channel = normal_channel self.sa1 = PointNetSetAbstraction(npoint=512, radius=0.2, K=32, in_channel=in_channel, mlp=[64, 64, 128], group_all=False) self.sa2 = PointNetSetAbstraction(npoint=128, radius=0.4, K=64, in_channel=128 + 3, mlp=[128, 128, 256], group_all=False) self.sa3 = PointNetSetAbstraction(npoint=None, radius=None, K=None, in_channel=256 + 3, mlp=[256, 512, 1024], group_all=True) self.fc1 = nn.Linear(1024, 512) self.bn1 = nn.BatchNorm1d(512) self.drop1 = nn.Dropout(0.4) self.fc2 = nn.Linear(512, 256) self.bn2 = nn.BatchNorm1d(256) self.drop2 = nn.Dropout(0.4) self.fc3 = nn.Linear(256, num_class) def forward(self, xyz): #xyz = [B, 3+ C(0 or 3), N_0] B, _, _ = xyz.shape if self.normal_channel: norm = xyz[:, 3:, :] # norm = [B, 0 or 3, N_0] (如果用到了法向量) xyz = xyz[:, :3, :] # xyz = [B, 3, N_0] else: norm = None l1_xyz, l1_features = self.sa1(xyz, norm) # l1_xyz = [B, 3, 512], l1_features = [B, 128, 512] l2_xyz, l2_features = self.sa2(l1_xyz, l1_features) # l2_xyz = [B, 3, 128], l1_features = [B, 256, 128] l3_xyz, l3_features = self.sa3(l2_xyz, l2_features) # l3_xyz = [B, 1, 3], l3_features = [B, 1, 1024] # 如果只是利用PointNet++提取特征的话，这样就可以了 x = l3_features.view(B, 1024) x = self.drop1(F.relu(self.bn1(self.fc1(x)))) # [B, 1024] -&gt; [B, 512] x = self.drop2(F.relu(self.bn2(self.fc2(x)))) # [B, 512] -&gt; [B, 256] x = self.fc3(x) # [B, 256] -&gt; [B, 40] x = F.log_softmax(x, -1) return x, l3_features ​ 接下来其实就是看set abstraction模块了，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class PointNetSetAbstraction(nn.Module): def __init__(self, npoint, radius, K, in_channel, mlp, group_all): super(PointNetSetAbstraction, self).__init__() self.npoint = npoint self.radius = radius self.K = K self.mlp_convs = nn.ModuleList() self.mlp_bns = nn.ModuleList() last_channel = in_channel for out_channel in mlp: self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1)) # 卷积核大小为1 self.mlp_bns.append(nn.BatchNorm2d(out_channel)) last_channel = out_channel # 对于sa1来说，就是有 3个MLP层，每层的卷积核大小都是1*1 # 第一层 [B, 3+C_i, K_{i+1}, N_{i+1}] -&gt; [B, 64, K_{i+1}, N_{i+1}] # 第二层 [B, 64, K_{i+1}, N_{i+1}] -&gt; [B, 64, K_{i+1}, N_{i+1}] # 第三层 [B, 64, K_{i+1}, N_{i+1}] -&gt; [B, 128, K_{i+1}, N_{i+1}] self.group_all = group_all def forward(self, xyz, points): &quot;&quot;&quot; Input: xyz: input points position data, [B, 3, N_i] features: input points feature data, [B, C_i, N_i] Return: new_xyz: sampled points position data, [B, 3, N_{i+1}] new_features: sample points feature data, [B, C_{i+1}, N_{i+1}] &quot;&quot;&quot; xyz = xyz.permute(0, 2, 1) if features is not None: features = features.permute(0, 2, 1) if self.group_all: new_xyz, new_features = sample_and_group_all(xyz, features) # new_xyz = [B, 1, 3] # new_features = [B, 1, K_{i+1}, 3+C_i] else: new_xyz, new_features = sample_and_group(self.npoint, self.radius, self.K, xyz, features) # new_xyz: sampled points position data, [B, N_{i+1}, 3] # new_features: sampled points position and feature data, [B, N_{i+1}, K_{i+1}, 3+C_i] new_features_concat = new_points.permute(0, 3, 2, 1) # [B, 3+C_i, K_{i+1}, N_{i+1}] for i, conv in enumerate(self.mlp_convs): bn = self.mlp_bns[i] new_features_concat = F.relu(bn(conv(new_features_concat))) # [B, C_{i+1}, K_{i+1}, N_{i+1}] new_features_concat = torch.max(new_features_concat, 2)[0] # [B, C_{i+1}, K_{i+1}, N_{i+1}] -&gt; [B, C_{i+1}, N_{i+1}] # 其实就是说从邻域特征中找到一个最大响应的值，因为每次处理的邻域的半径不同，所以每次提取特征响应的感受野也不同，实现了不同尺度下的特征提取 new_xyz = new_xyz.permute(0, 2, 1) # [B, N_{i+1}, 3] -&gt; [B, 3, N_{i+1}] return new_xyz, new_features_concat ​ 对于set abstraction layer来说，它的输入是$B\\times N_{i} \\times (3+C_i)$，而它的输出是$B\\times N_{i+1}\\times(3+C_{i+1})$。 ​ 理论上这样我们的分类任务已经可以完成了，这也是SSG(single-scale grouping)的情况。我们需要再看一下论文所提出的MSG的set abstraction module是如何实现的。 ​ 我们可以看到models里总体架构没有变，唯一有区别的就是两个sa层变为了SA_MSG。 123self.sa1 = PointNetSetAbstractionMsg(npoint=512, radius_list=[0.1, 0.2, 0.4], K_list=[16, 32, 128], in_channel=in_channel, mlp_list=[[32, 32, 64], [64, 64, 128], [64, 96, 128]])self.sa2 = PointNetSetAbstractionMsg(npoint=128, radius_list=[0.2, 0.4, 0.8], K_list=[32, 64, 128], in_channel=320, mlp_list=[[64, 64, 128], [128, 128, 256], [128, 128, 256]])self.sa3 = PointNetSetAbstraction(npoint=None, radius=None, K=None, in_channel=640 + 3, mlp=[256, 512, 1024], group_all=True) ​ 其实我们只要搞明白上述的in_channel各自是怎么来的。以sa2的in_channel为例：$$\\text{in_channel}=320=\\Sigma_j(\\text{out_channel}_j)=64+128+128$$​ 这样我们就明白了，其实就是把不同的radius所提取的不同尺度的特征拼接在一起得到320维的特征向量。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667class PointNetSetAbstractionMsg(nn.Module): def __init__(self, npoint, radius_list, K_list, in_channel, mlp_list): super(PointNetSetAbstractionMsg, self).__init__() self.npoint = npoint self.radius_list = radius_list self.nsample_list = nsample_list self.conv_blocks = nn.ModuleList() self.bn_blocks = nn.ModuleList() for i in range(len(mlp_list)): convs = nn.ModuleList() bns = nn.ModuleList() last_channel = in_channel + 3 for out_channel in mlp_list[i]: convs.append(nn.Conv2d(last_channel, out_channel, 1)) bns.append(nn.BatchNorm2d(out_channel)) last_channel = out_channel self.conv_blocks.append(convs) self.bn_blocks.append(bns) def forward(self, xyz, points): &quot;&quot;&quot; Input: xyz: input points position data, [B, 3, N_i] points: input points data, [B, C_i, N_i] Return: new_xyz: sampled points position data, [B, 3, N_{i+1}] new_points_concat: sample points feature data, [B, C_{i+1}, N_{i+1}] &quot;&quot;&quot; xyz = xyz.permute(0, 2, 1) # [B, N_i, 3] if points is not None: points = points.permute(0, 2, 1) # [B, N, C_i] ,C_i就是每个点额外的特征向量 B, N, C = xyz.shape new_xyz = index_points(xyz, farthest_point_sample(xyz, self.npoint)) # [B, N_{i+1}, C_i] new_points_list = [] for j, radius in enumerate(self.radius_list): # 在不同的尺度下找ball query，枚举radius_list K = self.nsample_list[j] # 当前半径下，中心节点的邻居的数量 # ======================以下开始其实是sample_and_group的逻辑==================== group_idx = query_ball_point(radius, K, xyz, new_xyz) grouped_xyz = index_points(xyz, group_idx) # [B, 3, npoints, nsample] grouped_xyz -= new_xyz.view(B, S, 1, C) # ??? 转换成相对坐标 if points is not None: grouped_points = index_points(points, group_idx) grouped_points = torch.cat([grouped_points, grouped_xyz], dim=-1) else: grouped_points = grouped_xyz # =========================================================================== grouped_points = grouped_points.permute(0, 3, 2, 1) # [B, N_{i+1}, K_{j}, (3+D)] -&gt; [B, (3+D), K_{j}, N_{i+1}] for k in range(len(self.conv_blocks[j])): # 所以mlp_list是一个二维数组，每一行都代表在对应半径下使用的input_channels conv = self.conv_blocks[j][k] bn = self.bn_blocks[j][k] grouped_points = F.relu(bn(conv(grouped_points))) # 得到了 [B, out_channel_j, K_{j}, N_{i+1}] # 最终我们得到一个group_points, [B, out_channel_j, K_{j}, N_{i+1}] new_points = torch.max(grouped_points, 2)[0] # [B, out_channel_j, N_{i+1}] new_points_list.append(new_points) # 最终new_points_list为 len_radius_list 个 [B, out_channel_j, N_{i+1}] new_xyz = new_xyz.permute(0, 2, 1) new_points_concat = torch.cat(new_points_list, dim=1) # 最终得到 [B, sigma{out_channel_j}, N_{i+1}] print(&quot;new_points_concat.size = &quot;, new_points_concat) return new_xyz, new_points_concat Q：PointNet++梯度是如何回传的？？？ A：PointNet++ fps实际上并没有参与梯度计算和反向传播。 可以理解成是PointNet++将点云进行不同规模的fps降采样，事先将这些数据准备好，再送到网络中去训练。 VoteNet​ VoteNet是基于end-to-end的3D目标检测网络，它基于3D的深度点云网络和霍夫投票。 ​ 可以从讲解视频和PPT获得相关内容。","link":"/2021/12/22/pointnet-and-related-works/"},{"title":"(ICRA2021)ScrewNet","text":"ScrewNet论文阅读和源码分析。 核心表述​ 其实ScrewNet的目的很简单，希望从深度图中直接估计出物体的关节模型及其位形信息，而之前的工作都需要引入额外的关节体的纹理信息、或者指定关节体的类型(rigid、revolute、prismatic，helical)。 ​ ScrewNet的核心结构如下： ​ 对于N帧深度图，每一帧都先使用ResNet-18作为backbone来提取2D图像特征，然后提取出来的N个特征向量传入到LSTM层中计算，然后输出N-1 X 8个相对的Screw Parameter。此处提供一个复习LSTM的博客。 Screw Theory​ 这篇文章能中ICRA的点就在于此。“空间中任何一个物体的位移都可以通过绕着一条直线的旋转以及绕着这条线的平行移动解决。”这条线叫做screw axis of displacement $S$。在普吕克坐标系下，直线可以表示成$(\\textbf{l}, \\textbf{m})$的形式，并且满足$||\\textbf{l}||=1$和$\\textbf{l}\\cdot\\textbf{m}=0$这两个约束条件。其实也很容易理解，在欧式坐标下，我们取直线的一段线段，有端点x和y。我们令$$\\textbf{l}=\\text{normalize} (y-x) \\\\textbf{m}=x\\times y$$​ 这就是普吕克坐标下表示直线的方法。 ​ 上图中的$(d,m)$就是我们的$(\\textbf{l}, \\textbf{m})$。 ​ 所以ScrewNet就使用了$(\\textbf{l}, \\textbf{m}, \\theta, d)$来表示在SE(3)中的刚体运动。其中$d$是沿着轴的线性平移，而$\\theta$是绕着轴的旋转，并且满足$d=h\\theta$。 Loss Function​ Screw位移包括了两部分：screw轴$S$，以及对应的位形$q_i$。所以ScrewNet希望同时优化如下的多个目标损失：$$L=\\lambda_1L_{S_{ori}}+\\lambda_2L_{S_{dist}}+\\lambda_3L_{S_{cons}}+\\lambda_4{L_q}$$​ 其中$L_{S_{ori}}$惩罚的是screw轴的偏差，所以通过GT轴和screw轴的角度偏差来计算，而$L_{S_{dist}}$是惩罚的是预测的screw轴和GT轴的空间距离，通过普吕克坐标系下的直线距离来表示。即： $$d((\\textbf{l}_1,\\textbf{m}_1),(\\textbf{l}_2,\\textbf{m}_2))=\\begin{cases}0, &amp; \\text{if $\\textbf{l}_1$ and $\\textbf{l}_2$ intersect} \\\\||\\textbf{l}_1\\times(\\textbf{m}_1-\\textbf{m}_2)||, &amp; \\text{else if $\\textbf{l}_1$ and $\\textbf{l}_2$ are parallel, i.e.$||\\textbf{l}_1\\times \\textbf{l}_2||=0$} \\\\\\frac{|\\textbf{l}_1\\cdot \\textbf{m}_2+\\textbf{l}_2 \\cdot \\textbf{m}_1|}{||\\textbf{l}_1\\times \\textbf{l}_2||}, &amp; else,\\textbf{l}_1\\ and\\ \\textbf{l}_2\\ are\\ skew\\ lines\\end{cases}\\\\L_{S_{dist}}=d((\\textbf{l}_{GT},\\textbf{m}_{GT}),(\\textbf{l}_{pred},\\textbf{m}_{pred}))$$​ 而$L_{S_cons}$强迫其满足预测出来的直线满足普吕克约束$(\\textbf{l}\\cdot\\textbf{m}=0)$和$||\\textbf{l}||=1$；$L_q$是位形损失。 ​ 其中$L_q$可以由两部分组成：旋转误差$L_{\\theta}$和平移误差$L_d$，如下计算：$$L_q=\\alpha_1L_{\\theta}+\\alpha_2L_d \\\\L_{\\theta}=I_{3\\times3}-R(\\theta_{GT},\\textbf{l}_{GT})R(\\theta_{pred},\\textbf{l}_{pred})^T \\\\L_d=||d_{GT}\\cdot\\textbf{l}_{GT}-d_{pred}\\cdot\\textbf{l}_{pred}||$$​ 其中的$R(\\theta,\\textbf{l})$就是沿着轴$\\textbf{l}$旋转$\\theta$角度的旋转矩阵$R$。之所以不直接对$q_{GT}$和$q_{pred}$施加$L_2$损失是因为这个损失函数的构成确保了其物理的含义，因为这个损失函数是基于旋转矩阵正交的性质而设计的，所以它可以确保学出来的$\\theta_{pred}$和$\\textbf{l}_{pred}$是满足$R(\\theta_{pred},\\textbf{l}_{pred})\\in SO(3)$。类似的，损失函数$L_d$也计算了沿着两根不同的轴$\\textbf{l}_{GT}$和$\\textbf{l}_{pred}$的平移误差，如果我们只是计算$d_{GT}$和$d_{pred}$的范数的话，就等于我们默认了它们是沿着同一个轴平移的，这就不合理。 ​ 综上所述，我们Loss函数的选取遵循了我们所提出的Screw理论。 打标签方法​ ScrewNet的训练集包括了一系列的深度图像，并且需要有对应的screw displacement。使用Mujoco来渲染仿真中的关节体并且记录深度图像。使用了数据集中的柜子、抽屉、微波炉、烤箱等。 ​ 为了创建screw displacement标签，我们考虑$o_i$作为基物体，然后我们计算后面的$o_j$相对于基物体的相对screw displacement。具体来说，就是给定一个N帧图片的视频流$I_{1:N}$，我们首先选定视频的第一帧是物体的基础位姿，然后计算出n-1帧的相对的screw displacement。 ​ 也就是我们有相对于坐标系$F_{O_j^1}$的n-1个位移了，我们可以通过在普吕克坐标下做变换把这n-1个相对位移全部转换到相对于基坐标轴$O_i$下。具体的普吕克坐标系下的变换形式可以参考原文。 代码实现1.models.py​ 在代码中，ScrewNet提供了三个模型：ScrewNet、ScrewNet_2imgs、ScrewNet_NoLSTM，在实际训练和测试中，和数据集的对应关系如下： 123456789101112if args.model_type == '2imgs': print(&quot;Testing Model: ScrewNet_2imgs&quot;) best_model = ScrewNet_2imgs(n_output=8) testset = RigidTransformDataset(args.ntest, args.test_dir)elif args.model_type == 'noLSTM': print(&quot;Testing Model: ScrewNet_noLSTM&quot;) best_model = ScrewNet_NoLSTM(seq_len=16, fc_replace_lstm_dim=1000, n_output=8) testset = ArticulationDataset(args.ntest, args.test_dir)else: print(&quot;Testing ScrewNet&quot;) best_model = ScrewNet(lstm_hidden_dim=1000, n_lstm_hidden_layers=1, n_output=8) testset = ArticulationDataset(args.ntest, args.test_dir) ​ 其中ScrewNet_2imgs似乎是一个降级版本，我们先从这个模型的源码开始读起： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class ScrewNet_2imgs(nn.Module): def __init__(self, n_output=8): super(ScrewNet_2imgs, self).__init__() self.fc_mlp_dim_1 = 2000 self.fc_mlp_dim_2 = 512 self.fc_mlp_dim_3 = 256 self.n_output = n_output self.resnet = models.resnet18() self.bn_res_1 = nn.BatchNorm1d(1000, momentum=0.01) #需要归一化的维度为1000 self.fc_mlp_1 = nn.Linear(self.fc_mlp_dim_1, self.fc_mlp_dim_1) self.bn_mlp_1 = nn.BatchNorm1d(self.fc_mlp_dim_1, momentum=0.01) self.fc_mlp_2 = nn.Linear(self.fc_mlp_dim_1, self.fc_mlp_dim_2) self.bn_mlp_2 = nn.BatchNorm1d(self.fc_mlp_dim_2, momentum=0.01) self.fc_mlp_3 = nn.Linear(self.fc_mlp_dim_2, self.fc_mlp_dim_3) self.bn_mlp_3 = nn.BatchNorm1d(self.fc_mlp_dim_3, momentum=0.01) self.fc_mlp_4 = nn.Linear(self.fc_mlp_dim_3, self.n_output) def forward(self, X_3d): # X shape: Batch x Sequence x 3 Channels x img_dims # Run resnet sequentially on the data to generate embedding sequence cnn_embed_seq = [] for t in range(X_3d.size(1)): #从第一帧开始枚举 x = self.resnet(X_3d[:, t, :, :, :]) #resnet的输入B * 1 * 3 * W * H x = x.view(x.size(0), -1) #拉伸为B * vector_size(每一个vector为CNN隐变量) x = self.bn_res_1(x) #归一化为B * 1000 cnn_embed_seq.append(x) # 此时我们得到了cnn_embed_seq是大小为N的一个list，其中每个元素为B * 1000的格式 # 首先我们把它变为torch.tensor，并且交换sample dim和time dim cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1) # 此时我们有cnn_embed_seq = (B * N * 1000) # 因为在transpose后，虽然tensor的shape改变了，但是它在内存中的存储位置并没有改变，如果我们直接调用view会出错，所以我们需要先做.contiguous()，然后再使用view x_rnn = cnn_embed_seq.contiguous().view(-1, self.fc_mlp_dim_1) # 注意到我们此时view成了(B * 2000)，所以我们可以反推原先输入的N=2，所以这个模型的dataset都是2帧的视频 # FC layers x_rnn = self.fc_mlp_1(x_rnn) # B * 2000 =&gt; B * 2000 x_rnn = F.relu(x_rnn) x_rnn = self.bn_mlp_1(x_rnn) x_rnn = self.fc_mlp_2(x_rnn) # B * 2000 =&gt; B * 512 x_rnn = F.relu(x_rnn) x_rnn = self.bn_mlp_2(x_rnn) x_rnn = self.fc_mlp_3(x_rnn) # B * 512 =&gt; B * 256 x_rnn = F.relu(x_rnn) x_rnn = self.bn_mlp_3(x_rnn) x_rnn = self.fc_mlp_4(x_rnn) # B * 256 =&gt; B * 8（其中8维就是screw parameter) return x_rnn.view(X_3d.size(0), -1) #返回 B * 8 ​ 其中的models.resnet18()其实是torchvision.models.resnet.py中帮我们实现好的resnet，我们可以单纯地认为输入B * 3 * W * H，输出一个B * 1000的ResNet特征。 ​ 接下来是No_LSTM版本的ScrewNet： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263class ScrewNet(nn.Module): def __init__(self, lstm_hidden_dim=1000, n_lstm_hidden_layers=1, drop_p=0.5, n_output=8): super(ScrewNet, self).__init__() self.fc_res_dim_1 = 512 self.lstm_input_dim = 1000 self.lstm_hidden_dim = lstm_hidden_dim self.n_lstm_hidden_layers = n_lstm_hidden_layers self.fc_lstm_dim_1 = 256 self.fc_lstm_dim_2 = 128 self.n_output = n_output self.drop_p = drop_p self.resnet = models.resnet18() self.fc_res_1 = nn.Linear(self.lstm_input_dim, self.fc_res_dim_1) self.bn_res_1 = nn.BatchNorm1d(self.fc_res_dim_1, momentum=0.01) self.fc_res_2 = nn.Linear(self.fc_res_dim_1, self.lstm_input_dim) self.LSTM = nn.LSTM( input_size=self.lstm_input_dim, hidden_size=self.lstm_hidden_dim, num_layers=self.n_lstm_hidden_layers, batch_first=True, ) self.fc_lstm_1 = nn.Linear(self.lstm_hidden_dim, self.fc_lstm_dim_1) self.bn_lstm_1 = nn.BatchNorm1d(self.fc_lstm_dim_1, momentum=0.01) self.fc_lstm_2 = nn.Linear(self.fc_lstm_dim_1, self.fc_lstm_dim_2) self.bn_lstm_2 = nn.BatchNorm1d(self.fc_lstm_dim_2, momentum=0.01) self.dropout_layer1 = nn.Dropout(p=self.drop_p) self.fc_lstm_3 = nn.Linear(self.fc_lstm_dim_2, self.n_output) def forward(self, X_3d): # 输入的大小 B * N * 3 * W * H cnn_embed_seq = [] for t in range(X_3d.size(1)): #枚举N帧 x = self.resnet(X_3d[:, t, :, :, :]) # B * 1 * 1000 x = x.view(x.size(0), -1) # B * 1000 x = self.bn_res_1(self.fc_res_1(x)) # B * 1000 =&gt; B * 512 x = F.relu(x) x = self.fc_res_2(x) # B * 512 =&gt; B * 1000 cnn_embed_seq.append(x) cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1) # 此时我们有cnn_embed_seq = (B * N * 1000) # 为了提高内存的利用率和效率，调用flatten_parameters让parameter的数据存放成contiguous chunk(连续的块)。类似我们调用tensor.contiguous self.LSTM.flatten_parameters() RNN_out, (h_n, h_c) = self.LSTM(cnn_embed_seq, None) # h_c shape (n_layers, B, hidden_size)，默认值为(1, B, 1000) # h_n shape (n_layers, B, hidden_size)，默认值为(1, B, 1000) # RNN_out = (B * N * 1000) # None represents zero initial hidden state # FC layers x_rnn = RNN_out.contiguous().view(-1, self.lstm_hidden_dim) # BN * 1000 x_rnn = self.bn_lstm_1(self.fc_lstm_1(x_rnn)) # BN * 1000 =&gt; BN * 256 x_rnn = F.relu(x_rnn) x_rnn = self.bn_lstm_2(self.fc_lstm_2(x_rnn)) # BN * 256 =&gt; BN * 128 x_rnn = F.relu(x_rnn) x_rnn = self.fc_lstm_3(x_rnn) # BN * 8 return x_rnn.view(X_3d.size(0), -1) # return B * 8N ​ 这里涉及到LSTM的输入和输出，可以参考官网上的参数介绍。注意到最后的全连接层的维度变化，最后之所以预测B * 8N，是因为每个样本都有N帧，我们需要预测出每一帧的关节体参数，至于为什么不是$B \\times N \\times8$，这倒不是很重要，反正在back propagation的时候我们只需要准确地实现loss function，都能回归出来。 ​ 代码中还提供了一个no_lstm版本， 12345678910111213141516class ScrewNet_NoLSTM(nn.Module): def __init__(self, seq_len=16, fc_replace_lstm_dim=1000, n_output=8): super(ScrewNet_NoLSTM, self).__init__() self.fc_replace_lstm_seq_dim = fc_replace_lstm_dim * seq_len ... self.fc_replace_lstm = nn.Linear(self.fc_replace_lstm_seq_dim, self.fc_replace_lstm_seq_dim) def forward(self, X_3d): ... # FC replacing LSTM layer # cnn_embed_seq = (B * N * 1000) cnn_embed_seq = cnn_embed_seq.contiguous().view(cnn_embed_seq.size(0), -1) # (B * N * 1000) =&gt; (B * 1000N) x_rnn = F.relu(self.fc_replace_lstm(cnn_embed_seq)) #(B * 1000N) =&gt; (B * 1000N) x_rnn = x_rnn.view(-1, self.fc_replace_lstm_dim) #(BN * 1000) #后面就继续连接FC层和上面一模一样了 ... 2.dataset.py​ 我们先从简单的两张图片的数据集RigidTransformDataset入手，注意到数据集只需要override __len__和 __getitem__。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&quot;&quot;&quot;Data loader class for the 2-imgs ablated version &quot;&quot;&quot;class RigidTransformDataset(Dataset): def __init__(self, ntrain, root_dir, n_dof=1, norm_factor=1., transform=None): super(RigidTransformDataset, self).__init__() self.root_dir = root_dir self.labels_data = None self.length = ntrain self.n_dof = n_dof self.normalization_factor = norm_factor self.transform = transform self.augmentation_factor = 15 def __len__(self): return self.length def __getitem__(self, idx, imgs_per_object=16): if self.labels_data is None: self.labels_data = h5py.File(os.path.join(self.root_dir, 'complete_data.hdf5'), 'r') obj_idx = int(idx / self.augmentation_factor) obj_data_idx = idx % self.augmentation_factor + 1 obj_data = self.labels_data['obj_' + str(obj_idx).zfill(6)] #编号补齐0到6位 # Load depth image depth_imgs = torch.tensor([obj_data['depth_imgs'][0], obj_data['depth_imgs'][obj_data_idx]]) #只取开始和最后的两张图片 #此时为 N * W * H depth_imgs.unsqueeze_(1).float() #使用unsqueeze_添加一维，变成 N * 1 * W * H depth_imgs = torch.cat((depth_imgs, depth_imgs, depth_imgs), dim=1) # 深度通道复制三份，变成N * 3 * W * H（这真的会有用吗???） # # Load labels pt1 = obj_data['moving_frame_in_world'][0, :] # 世界系下的四元数1 pt2 = obj_data['moving_frame_in_world'][obj_data_idx, :] # 世界西夏的四元数2 pt1_T_pt2 = change_frames(pt1, pt2) #计算出相对pose # Object pose in world obj_pose_in_world = np.array(obj_data['embedding_and_params'])[-7:] # obj_pose, obj_quat_wxyz obj_T_pt1 = change_frames(obj_pose_in_world, pt1) #也就是论文中提到的向着base object frame转换 # 用screw参数创建标签，label := &lt;l_hat, m, theta, d&gt; = &lt;3, 3, 1, 1&gt; l_hat, m, theta, d = transform_to_screw(translation=pt1_T_pt2[:3], quat_in_wxyz=pt1_T_pt2[3:]) # Convert line in object_local_coordinates new_l = transform_plucker_line(np.concatenate((l_hat, m)), trans=obj_T_pt1[:3], quat=obj_T_pt1[3:]) label = np.concatenate((new_l, [theta], [d])) # This defines frames wrt pt 1 # Normalize labels label[3:6] /= self.normalization_factor # Scaling m appropriately label = torch.from_numpy(label).float() sample = {'depth': depth_imgs, 'label': label} #最终一个GT以dict的形式打包传出 return sample ​ 多张图片的其实就大差不差了，不过里面有一个细节我们需要深究一下。 123456789101112131415161718192021222324252627282930313233343536class ArticulationDataset(Dataset): def __getitem__(self, idx): ... pt1 = moving_body_poses[0, :] # Fixed common reference frame for i in range(len(moving_body_poses) - 1): pt2 = moving_body_poses[i + 1, :] pt1_T_pt2 = change_frames(pt1, pt2) # Generating labels in screw notation: label := &lt;l_hat, m, theta, d&gt; = &lt;3, 3, 1, 1&gt; l_hat, m, theta, d = transform_to_screw(translation=pt1_T_pt2[:3], quat_in_wxyz=pt1_T_pt2[3:]) # Convert line in object_local_coordinates new_l = transform_plucker_line(np.concatenate((l_hat, m)), trans=obj_T_pt1[:3], quat=obj_T_pt1[3:]) label[i, :] = np.concatenate((new_l, [theta], [d])) # This defines frames wrt pt 1 def transform_to_screw(translation, quat_in_wxyz, tol=1e-6): dq = dq3d.dualquat(dq3d.quat(quat_as_xyzw(quat_in_wxyz)), translation) screw = dual_quaternion_to_screw(dq, tol) return screwdef dual_quaternion_to_screw(dq, tol=1e-6): l_hat, theta = tf3d.quaternions.quat2axangle(np.array([dq.real.w, dq.real.x, dq.real.y, dq.real.z])) if theta &lt; tol or abs(theta - np.pi) &lt; tol: t_vec = dq.translation() l_hat = t_vec / (np.linalg.norm(t_vec) + 1e-10) theta = tol # This makes sure that tan(theta) is defined else: t_vec = (2 * tf3d.quaternions.qmult(dq.dual.data, tf3d.quaternions.qconjugate(dq.real.data)))[ 1:] # taking xyz from wxyz d = t_vec.dot(l_hat) m = (1 / 2) * (np.cross(t_vec, l_hat) + ((t_vec - d * l_hat) / np.tan(theta / 2))) return l_hat, m, theta, d ​ 只有理解了transform_to_screw这个函数在做什么，我们才真正摸索到了Screw Theory的实质。主要可以参考这篇文献。总体逻辑就是四元数可以表示三维旋转，而对偶四元数可以同时表示三维旋转和平移，所以使用对偶四元数来表示Screw Parameter就是很合理的事情，满足以下推导： ​ 空间任意刚体运动，可分解为刚体上某一点的平移，以及绕经过此点的旋转轴的转动，我们令这个点为连体基坐标原点，我们记作$R$和$t$，旋转矩阵$R$对应的四元数为$p$，由$R$和$t$可以计算出对偶四元数$q$。根据Chasles theorem(Screw theory，沙勒定理)我们又知道：空间任意刚体运动，均可看作有限螺旋运动，即均可表示为绕一轴的旋转和沿该轴的平移，参数可以记为$(\\textbf{l},\\textbf{m}, \\theta, d)$。 ​ 首先，四元数$p$转化为轴角表达$p=(cos(\\frac{\\theta}{2}),\\textbf{l}sin(\\frac{\\theta}{2}))$就可以直接得到$\\textbf{l}$和$\\theta$，参数物理意义完全相同。其余参数满足下式：$$d=\\textbf{t}\\cdot\\textbf{l}=(2qp^*)\\cdot\\textbf{l} \\\\m=\\frac{1}{2}(\\textbf{t}\\times\\textbf{l}+(\\textbf{t}-d\\textbf{l})\\cot\\frac{\\theta}{2})$$ 3.train_model.py​ 里面涉及到三种不同的模型的定义，封装地也很好，总体逻辑还是非常简单易懂的。 12345678910111213141516171819202122232425262728293031323334trainset = ...testset = ...loss_fn = ...network = ...testloader = torch.utils.data.DataLoader(testset, batch_size=args.batch, shuffle=True, num_workers=args.nwork, pin_memory=True)trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch, shuffle=True, num_workers=args.nwork, pin_memory=True)# Load Saved weightsif args.load_wts: network.load_state_dict(torch.load(args.wts_dir + args.prior_wts + '.net'))# setup trainerif torch.cuda.is_available(): device = torch.device(args.device)else: device = torch.device('cpu')optimizer = torch.optim.Adam(network.parameters(), lr=args.learning_rate, weight_decay=1e-2)scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_schedule, gamma=lr_gamma)trainer = ModelTrainer(model=network, train_loader=trainloader, test_loader=testloader, optimizer=optimizer, scheduler=scheduler, criterion=loss_fn, epochs=args.epochs, name=args.name, test_freq=args.val_freq, device=args.device)# trainbest_model = trainer.train() 4.model_trainer.py​ 其实这就没啥好说的了，无非就是训练（算loss，反向传播，画图，训练日志，保存模型）和测试（计算均值和方差）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142class ModelTrainer(object): def __init__(self, *kwargs): pass def train(self): best_tloss = 1e8 for epoch in range(self.epochs + 1): sys.stdout.flush() loss = self.train_epoch(epoch) self.losses.append(loss) self.writer.add_scalar('Loss/train', loss, epoch) if epoch % self.test_freq == 0: tloss = self.test_epoch(epoch) self.tlosses.append(tloss) self.plot_losses() self.writer.add_scalar('Loss/validation', tloss, epoch) if tloss &lt; best_tloss: print('saving model.') net_fname = os.path.join(self.wts_dir, str(self.name) + '.net') torch.save(self.model.state_dict(), net_fname) # 把表现更好的模型存到本地 best_tloss = tloss self.scheduler.step() # Visualize gradients total_norm = 0. nan_count = 0 for tag, parm in self.model.named_parameters(): if torch.isnan(parm.grad).any(): print(&quot;Encountered NaNs in gradients at {} layer&quot;.format(tag)) nan_count += 1 else: self.writer.add_histogram(tag, parm.grad.data.cpu().numpy(), epoch) param_norm = parm.grad.data.norm(2) total_norm += param_norm.item() ** 2 total_norm = total_norm ** (1. / 2) self.writer.add_scalar('Gradient/2-norm', total_norm, epoch) if nan_count &gt; 0: raise ValueError(&quot;Encountered NaNs in gradients&quot;) # plot losses one more time self.plot_losses() # re-load the best state dictionary that was saved earlier. self.model.load_state_dict(torch.load(net_fname, map_location='cpu')) # export scalar data to JSON for external processing self.writer.export_scalars_to_json(&quot;./all_scalars.json&quot;) self.writer.close() return self.model def train_epoch(self, epoch): start = time.time() running_loss = 0 batches_per_dataset = len(self.trainloader.dataset) / self.trainloader.batch_size self.model.train() # Put model in training mode for i, X in enumerate(self.trainloader): self.optimizer.zero_grad() depth, labels = X['depth'].to(self.device), \\ X['label'].to(self.device) y_pred = self.model(depth) loss = self.criterion(y_pred, labels) if loss.data == -float('inf'): print('inf loss caught, not backpropping') running_loss += -1000 else: loss.backward() # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs. torch.nn.utils.clip_grad_norm_(self.model.parameters(), 10.) self.optimizer.step() running_loss += loss.item() stop = time.time() print('Epoch %s - Train Loss: %.5f Time: %.5f' % (str(epoch).zfill(3), running_loss / batches_per_dataset, stop - start)) return running_loss / batches_per_dataset def test_epoch(self, epoch): start = time.time() running_loss = 0 batches_per_dataset = len(self.testloader.dataset) / self.testloader.batch_size self.model.eval() # Put batch norm layers in eval mode with torch.no_grad(): for i, X in enumerate(self.testloader): depth, labels = X['depth'].to(self.device), \\ X['label'].to(self.device) y_pred = self.model(depth) loss = self.criterion(y_pred, labels) running_loss += loss.item() stop = time.time() print('Epoch %s - Test Loss: %.5f Euc. Time: %.5f' % (str(epoch).zfill(3), running_loss / batches_per_dataset, stop - start)) return running_loss / batches_per_dataset def test_best_model(self, best_model, fname_suffix='', dual_quat_mode=False): best_model.eval() # Put model in evaluation mode ... with torch.no_grad(): for X in self.testloader: depth, all_labels, labels = X['depth'].to(self.device), \\ X['all_labels'].to(self.device), \\ X['label'].to(self.device) y_pred = best_model(depth, all_labels) y_pred = y_pred.view(y_pred.size(0), -1, 8) if dual_quat_mode: y_pred = dual_quaternion_to_screw_batch_mode(y_pred) labels = dual_quaternion_to_screw_batch_mode(labels) err = labels - y_pred all_l_hat_err = torch.cat( (all_l_hat_err, torch.mean(torch.norm(err[:, :, :3], dim=-1), dim=-1).cpu())) all_m_err = torch.cat((all_m_err, torch.mean(torch.norm(err[:, :, 3:6], dim=-1), dim=-1).cpu())) all_q_err = torch.cat((all_q_err, torch.mean(err[:, :, 6], dim=-1).cpu())) all_d_err = torch.cat((all_d_err, torch.mean(err[:, :, 7], dim=-1).cpu())) all_l_hat_std = torch.cat( (all_l_hat_std, torch.std(torch.norm(err[:, :, :3], dim=-1), dim=-1).cpu())) all_m_std = torch.cat((all_m_std, torch.std(torch.norm(err[:, :, 3:6], dim=-1), dim=-1).cpu())) all_q_std = torch.cat((all_q_std, torch.std(err[:, :, 6], dim=-1).cpu())) all_d_std = torch.cat((all_d_std, torch.std(err[:, :, 7], dim=-1).cpu())) # Plot variation of screw axis pass def plot_grad_flow(self, named_parameters): pass def plot_losses(self): pass 5.Other Modules​ 其他辅助模组就暂时不继续占据篇幅了。注意到还存在一个noisy_models.py，引用了这个仓库，可能是对应的paper的数据增强手段，此处不表。","link":"/2021/12/20/screwnet/"}],"tags":[{"name":"CHOMP","slug":"CHOMP","link":"/tags/CHOMP/"},{"name":"planning","slug":"planning","link":"/tags/planning/"},{"name":"lab","slug":"lab","link":"/tags/lab/"},{"name":"ROS","slug":"ROS","link":"/tags/ROS/"},{"name":"moveit","slug":"moveit","link":"/tags/moveit/"},{"name":"SE3353","slug":"SE3353","link":"/tags/SE3353/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"MySQL","slug":"MySQL","link":"/tags/MySQL/"},{"name":"WebService","slug":"WebService","link":"/tags/WebService/"},{"name":"Search","slug":"Search","link":"/tags/Search/"},{"name":"MongoDB","slug":"MongoDB","link":"/tags/MongoDB/"},{"name":"Neo4j","slug":"Neo4j","link":"/tags/Neo4j/"},{"name":"compliers","slug":"compliers","link":"/tags/compliers/"},{"name":"behavior tree","slug":"behavior-tree","link":"/tags/behavior-tree/"},{"name":"ikfast","slug":"ikfast","link":"/tags/ikfast/"},{"name":"pybullet","slug":"pybullet","link":"/tags/pybullet/"},{"name":"VoteNet","slug":"VoteNet","link":"/tags/VoteNet/"},{"name":"mmdection","slug":"mmdection","link":"/tags/mmdection/"},{"name":"segmentation","slug":"segmentation","link":"/tags/segmentation/"},{"name":"deeplearning","slug":"deeplearning","link":"/tags/deeplearning/"},{"name":"OpenCV","slug":"OpenCV","link":"/tags/OpenCV/"},{"name":"OpenGL","slug":"OpenGL","link":"/tags/OpenGL/"},{"name":"Camera","slug":"Camera","link":"/tags/Camera/"},{"name":"Pybullet","slug":"Pybullet","link":"/tags/Pybullet/"},{"name":"Pybind11","slug":"Pybind11","link":"/tags/Pybind11/"},{"name":"PointNet","slug":"PointNet","link":"/tags/PointNet/"},{"name":"PointNet++","slug":"PointNet","link":"/tags/PointNet/"},{"name":"ScrewNet","slug":"ScrewNet","link":"/tags/ScrewNet/"},{"name":"ICRA","slug":"ICRA","link":"/tags/ICRA/"}],"categories":[]}