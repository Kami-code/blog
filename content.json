{"pages":[{"title":"","text":"About MeMy name is Chen Bao, I am a undergraduate student in Shanghai Jiao Tong University, who are supposed to get my bachelor in Software Engineering in June, 2023. I work as an intern in MVIG Lab, Shanghai Jiao Tong University.","link":"/blog/about/index.html"}],"posts":[{"title":"(ICCV2021)A-SDF","text":"​ 最近的工作有很多利用implicit function来做3D刚体重建，但是很少工作关注通用关节体的建模，比起刚体，关节体的自由度更大，所以泛化到未见过的形状比较困难。为了解决形状的方差较大的问题，这篇文章引入了 Articulated Signed Distance Functions (A-SDF) 来使用隐空间来表示关节体的形状。我们假设没有对关节体部分的几何信息、关节类型、关节轴、关节位置的先验知识。 问题重述​ 我们考虑在一个关节体类别上的N个模型实例。每个实例都会有M个位形，这样我们就有了$N \\times M$个形状。我们令$\\chi_{n,m}$代表了第n个实例的第m个位姿的形状，其中$n\\in\\{1,…,N\\},m\\in\\{1,…,M\\}$。每个形状$\\chi_{n,m}$会有一个形状编码$\\phi_n\\in R^C$和关节编码$\\psi_m\\in R^D$，其中$C$代表了隐变量空间的维度、$D$代表了关节体的自由度。 形状编码​ 形状编码$\\phi_n$是在同一个关节体实例中共享的，也就是不同位姿有相同的形状编码。在训练过程中，我们对于每个实例维护并更新形状编码。 关节编码​ 我们使用关节角度来作为关节编码。比如说，2D的关节体（如：眼镜）且两个关节均打开45度的关节编码为$\\psi_m=(45^{\\circ},45^{\\circ})$，关节角度定义为物体标准位形的相对角度。 网络 ​ 如上图所示，A-SDF函数$f_{\\theta}$最终定义如下：$$f_{\\theta}(x,\\phi,\\psi)=f_a[f_s(x,\\phi),x,\\psi]=s$$​ 其中$f_s$是shape encoder，而$f_a$是articulation network，$\\textbf{x}\\in R^3$为形状中采样到的点，$s\\in R$是标量SDF函数了，SDF函数值的正负描述了是在物体外（正）还是物体内（负）。并且SDF函数等于0的等值面$f_{\\theta}(\\cdot)=0$隐式地描述了3D形状。SDF也就是有向距离场，之前的文章中也遇到过。 ​ 自此，其实我们就可以知道S这个网络就是用2个我们提的形状编码和关节编码，以及多个点的query来回归出整个SDF函数。并且当我们的网络训练完成，且我们通过某些办法得到了形状编码以后，我们就可以传入未训练过的关节值来得到合成的3D模型，也就是SDF函数场。这样看来SDF函数虽然是十年前就有的东西，但是借助着implicit function的东风，learning界重新发现了它可以3D模型建立映射关系，并且嵌入到网络中。 ​ 但是，形状编码它对于同一个关节体实例是共享的，所以它要在某种意义上能够描述出关节体的动力学模型，这样它才能够为我们infer unseen joints提供帮助。我认为这个相对困难，在下文中我将继续尝试探索它的隐变量空间的参数是怎么训练的。 训练过程​ 在训练过程中，给定GT关节编码$\\psi$，从形状中采样的点以及对应的SDF值，我们希望训练模型来优化形状编码和模型参数$\\theta$。网络的真正逻辑如下图所示，论文中的图的逻辑相对模糊一些。 ​ 那无非接下来就是去学这个网络的参数，我们使用如下的损失函数：$$L(\\chi,\\phi,\\psi)=L^s(\\chi,\\phi,\\psi)+\\lambda_pL^p(\\chi,\\phi,\\psi)+\\lambda_{\\phi}||\\phi||_2^2 \\\\L^s(\\chi,\\phi,\\psi)=\\frac{1}{K}\\sum_{k=1}^K||f_{\\theta}(x_k,\\phi,\\psi)-s_k||_1 \\\\L^p(\\chi,\\phi,\\psi)=\\frac{1}{K}\\sum_{k=1}^K[CE(f_{\\theta}(x_k,\\phi,\\psi),p_k)]$$​ 其中$L^s$就是逐点地去回归这个SDF函数，而$L^p$就是如果我们要同时预测关节体分割网络的话，对应的逐点的分类交叉熵损失，而第三部分的$\\lambda_{\\phi}||\\phi||_2^2$就是对我们的形状参数的正则化约束。 ​ 在训练过程中，形状编码最初初始化为正态分布，后面我们是要持续更新它的，因为它是在$N \\times M$个训练样本中共享的，所以它和网络参数$\\theta$是要一起训练的，也就是：$$\\theta^*,\\phi_n=\\arg\\min_{\\theta,\\phi_n}\\sum_{n=1}^N\\sum_{m=1}^ML(\\chi_{n,m},\\phi_n,\\psi_m)$$ 应用​ 这个工作的应用阶段比较有意思，对我来说比较崭新。 Basic Inference​ 给定一个关节体模型$\\chi$，我们现在的目标是恢复出对应的形状参数$\\phi$和关节角度$\\psi$。 我们可以在原先网络上使用反向传播来求得。也就是我们固定网络参数，然后随机初始化两个待求目标$\\phi,\\psi$。然后我们通过优化如下目标来求得最优情况：$$\\hat{\\phi},\\hat{\\psi}=\\arg\\min_{\\phi,\\psi} L(\\chi,\\phi,\\psi)$$​ 但是注意到待求问题无论是$\\phi$和$\\psi$都是非凸的，基于梯度的方法可能会导致到达局部最优的情况。在实际运用过程中，我们发现直接通过梯度来估计关节角度可以收敛到一个比较好的结果，但形状参数不行。所以我们先估计出关节角度，然后在固定关节角度和网络参数的情况下，再对形状参数做优化。 TTA(Test-Time Adapation Inference)​ 上述的Basic Inference的准确度极大地依赖于固定的网络参数是否符合我们测试集的分布了。如果网络并没有训练好，会导致很多问题，所以引入了TTA作为一个更新方法。伪代码如下： $\\textbf{Algorithm:}$Test-Time Adaption Inference Algorithm $\\textbf{Input:}$ 目标模型 $\\chi$ $\\textbf{Output:}$ 形状编码$\\hat{\\phi}$, 关节角度$\\hat{\\psi}$，更新后的编码器参数$\\hat{f_s}$1: 初始化$\\phi\\sim N(0,\\sigma), \\psi$2: $\\_, \\hat{\\psi}=\\arg\\min_{\\phi,\\psi}L(\\chi, \\phi, \\psi)$ //此处估计出了关节角度3: $\\hat{\\phi}=\\arg\\min_{\\phi}L(\\chi,\\phi,\\hat{\\psi})$ //代入估计出的关节角度，重新估计我们的形状编码4: $\\hat{f}_s=\\arg\\min_{f_s}L(\\chi,\\hat{\\phi}, \\hat{\\psi})$ //TTA更新 ​ 上述其实就是说，对于每一类，由于提出的shape-encoder有类间的共享的feature，所以可以通过额外观测的测试集数据继续调整学习到的分布。 ​ 这个算法其实有点模糊了训练和测试的边界，不知道这种思路是否是通用的。 Articulated Shape Synthesis​ 当我们有了形状参数以后，我们就可以合成没有见过的角度的实例。可以看做一个生成式网络。比起变分推断的input是高斯分布的采样，这里是把分布align到了joint range上来创建，然后创建的方式是基于sample的inference得到SDF value来创建整个模型。相对传统的网络我觉得还是比较新颖的。 实验实验部分略，等到有需要再看。不过值得一提的是，TTA在现实中的深度图上测试时效果很好，因为TTA意味着使用训练集分布作为初始值，在测试集上继续拟合其分布，所以可以克服一定的sim2real gap。","link":"/blog/2022/02/20/A-SDF/"},{"title":"CHOMP_and_related_works","text":"​ 最近深感自己知识储备的不足，面对一个问题可能前人已经解决了，但是却自己根本摸不着头绪。准备好好开始看paper来增加自己的知识库储备。实验室的师兄师姐们都非常厉害，分分钟可以丢给我很多paper去看，希望借此机会可以增加自己研究的insight。 CHOMP​ CHOMP是一篇如雷贯耳的文章。刚进领域的时候就经常看见CHOMP,STOMP等planner，非常如雷贯耳，最近项目可能要添加进一些planner的因素，所以要先从这篇09年ICRA的祖师爷开始看起，注意同一批人在13年再次对CHOMP工作做了细致的讲解、提供了和RRT*的对比实验，并且提供了扩展到含额外约束的轨迹规划问题上。 ​ 其实CHOMP的思路比较直观，它有一个目标函数$U(\\xi)=f_{prior}(\\xi)+\\lambda f_{obstacle}(\\xi)$，最终目标就是优化这个目标函数得到轨迹$\\xi$。即有：$$\\xi^* = \\mathop{\\arg\\min}_{\\xi}U(\\xi)$$​ 其中，轨迹$\\xi(t)$可以认为是一个[0,1]$\\rightarrow R^m$的函数，我们把时间归一化到0~1之间，也就是说$\\xi(0)=q_{init}$以及$\\xi(1)=q_{goal}$。所以$U(\\xi)$就可以简单地认为是一个关于轨迹$\\xi$的损失函数。当它取到最小的时候我们即得到了一个最优的colision-free的轨迹。 动力学损失函数​ CHOMP的目标函数主要考虑了动力学带来的cost以及碰撞所带来的cost。我们可以先看动力学所带来的cost。$f_{prior}(\\xi)=\\displaystyle\\frac{1}{2}\\int^1_0 \\vert|\\frac{d}{dt}\\xi(t)||^2 dt$，这一项是直接对轨迹的速度的平方做积分，也可以扩展到加速度和加加速度的情况，是用来鼓励轨迹更加顺滑的。 碰撞损失函数​ 第二个损失函数就是说CHOMP希望规划出一条collision-free的轨迹，所以必须要把场景中的所有障碍物全部考虑进去。我们考虑机械臂外表面上的点集$B\\subset R^3$, u是其上一点。在某个构型下，我们显然可以通过构型的前向动力学信息以及机械臂上的相对坐标计算出其在欧式空间中的瞬时绝对坐标，我们记为$x(\\xi(t),u)$，有了这个坐标以后，那我们需要得到这一点离障碍物的距离，我们希望尽可能提升这个距离。所以我们引入了SDF距离（有向距离场），这个在图像渲染中用到的比较多，其实就是这个点到一个物体表面的最短距离。有了这个距离度量函数，我们可以构造出大于0的损失函数来尽可能最大化这个距离度量。所以我们有碰撞损失函数：$$\\displaystyle f_{obstacle}=\\int_0^1\\int_B c_{obstacle}(x(\\xi(t), u))|\\frac{d}{dt}x(\\xi(t),u)|dudt$$ ​ 在上式中，其实对机械臂的表面积做积分这件事情是比较显然的，但是之前一直很难理解为什么要乘上这个速度函数$x’(\\xi(t),u)=v(\\xi,u)$。理解起来也不复杂，因为我们的轨迹的时间归一化到了0~1之间。所以如果不乘上这个速度因子的话，就有可能导致两条运动速度不同而位置相同的轨迹算出来的损失函数是相等的，但是我们希望让以较快速度通过的有更大的损失，可能是因为较大的速度更容易碰撞时产生实际的负面影响，所以加上了速度因子后，就把时间归一化带来的多解问题消除掉了，可以理解为是一个保序变换。 优化过程​ 论文使用迭代的方式来更新轨迹$\\xi_k$，使用一阶泰勒展开$U(\\xi)\\approx U(\\xi_k)+g_k^T(\\xi-\\xi_k)$，其中$g_k=\\nabla U(\\xi_k)$。$$\\xi_{k+1}=\\xi_k-\\frac{1}{\\lambda}M^{-1}g_k$$ 推广到含约束的优化过程​ 有些轨迹创建过程可能需要额外的约束，比如传递水杯的时候我们可能希望水不要洒出来。所以在2013年的文章中，作者对把原先的CHOMP泛化到比较普遍的约束问题上。首先作者假设所有约束都可以按照轨迹在希尔伯特空间上非线性的可微分向量函数来定义：$H:\\Xi\\rightarrow R^k$，其中的$H(\\xi)=0$就是所有满足要求的约束的轨迹。 ​ 所以其实就是在我们之前的迭代过程中，我们此时迭代改写为：$$\\xi_{k+1}=\\xi_k-\\frac{1}{\\lambda}M^{-1}g_k \\\\s.t. H[\\xi]=0$$​ 为了得到一个具体的更新规则，我们在$\\xi_i$处对函数H进行一阶泰勒展开，也就是$H(\\xi)\\approx H(\\xi_i)+\\frac{\\partial}{\\partial\\xi}H(\\xi_i)(\\xi-\\xi_i)=C(\\xi-\\xi_i)+b$其中C是约束函数的Jacobian，而$b=H(\\xi_i)$，我们可以把带约束的迭代问题转化为拉格朗日约束下的梯度下降问题，具体过程不再描述。最终我们得到的更新规则为：$$\\xi_{k+1}=\\xi_k-\\frac{1}{\\lambda}A^{-1}g_k+\\frac{1}{\\lambda}A^{-1}C^T(CA^{-1}C^T)^{-1}CA^{-1}g_{k}-A^{-1}C^T(CA^{-1}C^T)^{-1}b$$ ​ 其实就是说在带约束的更新规则下，它先以无约束的方式优化一步，然后把A投影到穿过$\\xi_t$的超平面上，并且这个超平面与我们的约束函数在$\\xi_i$处的一阶泰勒展开的近似超平面$C(\\xi-\\xi_i)+b=0$平行。最终，它消除掉了两个超平面之间的平移量，使得下一步迭代$\\xi_{i+1}$更加接近于约束函数$H(\\xi)=0$上。 ​ 所以其实我们如果能够形式化定义出这个$H(\\xi)$的具体形式，那么找到满足这个约束的轨迹也只是一个优化的过程。 [RSS2020] 在线抓取合成和优化的轨迹创建​ 摘要的大致意思就是说轨迹创建和Grasp预测通常是分别处理的，这篇文章希望提出一个整合两个planning问题的方法。这篇文章整合出了一个轨迹优化方法和在线抓点合成/选取的一个结合办法，也就是在线学习并且挑选出最终抓点的6D pose。并且证明了在复杂环境中是可以鲁棒并且高效地创建出motion plan的。 ​ 避障自然就是使用了CHOMP算法，不再过多叙述。和这篇文章主打的Grasp相关的概念：给定一个场景中的多个物体，定义了可行的目标物体抓取集合$G\\subseteq Q\\subseteq R^d$。这样我们的优化问题就变成了：$$\\xi^* = \\mathop{\\arg\\min}_{\\xi}f_{motion}(\\xi) \\\\s.t.\\xi(1)\\in G$$​ 其中的损失函数$f_{motion}$自然就是CHOMP中的$U(\\xi)$，即$f_{motion}(\\xi)=f_{obstacle}(\\xi)+\\lambda f_{prior}(\\xi)$。所以这一步其实就是CHOMP含约束优化的一个实例。约束函数定义为$H(\\xi_i)=\\xi_i-g=0$，我们可以使用CHOMP含约束的更新规则来进行更新。我们可以定义$f_g(\\xi_i)$是在第i次迭代时，使用g作为目标抓点的损失函数。稍微有一点点不同的地方在于在不同迭代中选择的抓点g可能是不一样的。 在线学习抓点选择​ 如果我们直接使用CHOMP求解带约束g的优化函数，那么我们并没有考虑到一些轨迹上的特性，比如这篇ISRR2011所提到的损失函数可能陷入局部最优的情况。 ​ 如上图所示，这个例子如果我们梯度下降的起点选择的不好就很容易会陷入局部最优的情况，这是从机器学习角度是很容易理解的事情。不过，我们可以通过额外要求满足轨迹的一些Attributes来限定梯度下降的区域（起点）就是在那个可以达到局部最优的位置。 ​ 如上图所示，如果我们规定，所有轨迹都要满足“都从冰箱上方过去”，我们就可以把轨迹的优化范围限定在如右图所示的，可以到达全局最优点的位置。形式化地来说，我们可以定义一个好的轨迹应当满足的特质：$\\tau:S\\to A(\\Xi,S)$，其中S是抽象的任务描述，而$\\Xi$是可以解决任务的轨迹的集合，$A(\\Xi,S)$就代表了轨迹的属性，比如在上例中就是从冰箱门上方运动过去。我们就可以隐式地描述出原先轨迹集合$\\Xi$的一个子集：$\\Xi_A\\subseteq \\Xi$，我们从这个好轨迹集合中选取第二阶段优化的初始值，这样就可以避免局部最优的情况。所以总体流程就为$S\\to A(\\Xi,S)\\to \\xi\\in\\Xi_A\\to\\xi^*$。 ​ 我们继续回到这篇文章，我们如果想优化$f_g(\\xi_i)$，我们希望能够最大化motion generation success。文章使用了把第i次迭代时的Goal Set描述为了概率分布$p_i$，那么这一步其实就是在迭代计算$p_{i+1}$的过程，那么第i+1次迭代的时候，我们选取的目标抓点就是$g_{i+1}=\\arg\\max(p_{i+1})$，文章使用的赌博机算法（Bandit Algorithm）不再过多叙述，是一个简单的RL算法。 在线抓取合成​ 这一部分主要应用了CASE2018所提出的ISF算法，也就是从初始的有限抓取集合G出发，通过优化的方式在线合成更多质量更高的抓取点。优化的目标函数为：$$f_{grasp}(g)=f_{isf}(g)+\\gamma f_{collision}(g)$$​ ISF主要就是最大化夹爪和物体的接触面积，这件事情是通过对夹爪上的采样表面点和法向量，并且优化接触点位置处的法向量和距离得到的，ISF算法因为本身也设计到很多理解和推导，暂时不在这里描述。 ​ 并且此处有：$f_{collision}(g)=f_{hand_obstacle}+\\beta f_{obstacle}(g)$，这里的$f_{obstacle}$就是在CHOMP中的碰撞损失，而$f_{hand_obstacle}$指的就是在抓取的过程中，我们不希望robot的手部和待抓取的物体会出现任何的碰撞。 ​ 最终这篇文章提出的算法如下，每次迭代的时候，先用带约束的CHOMP创建轨迹，然后迭代更新Goal Set的分布，然后选取其中成功率最大的一个grasp作为goal，然后使用C-Space ISF方法来优化这个选取的grasp，使其抓取的成功率更大。","link":"/blog/2021/12/09/CHOMP-and-related-works/"},{"title":"(ICCV2021)Hand-Object Contact Consistency Reasoning for Human Grasps Generation","text":"​ 这个工作的目标是给定一个待抓取物体的点云，创建出对应的抓取手型网格。创建出的hand mesh首先得是自然、真实的，其次需要物理上能够抓紧物体。确保物体和合成的手型之间的合理的接触是得到 high-quality 和 stable 的 grasp的关键。 网站 代码 ​ 为了解决这个问题，我们利用手和物体的接触信息，并且确保它们是一致的。我们提出了两个网络，一个是生成式的 GraspCVAE来合成 grasping hand mesh，一个判断式的网络 ContactNet 来对接触区域建模。 训练阶段​ 两个网络分别训练。一个学习创建grasp，一个预测object的contact map。对于GraspCVAE来说，要同时输入手型和物体点云，来合成 hand reconstruction paradiam。为了训练Grasp CVAE，我们提出了两种loss来保证 hand-object consistency：一个强制先验的 hand contact的顶点接近物体表面，另一个loss是促进object的contact region被手接触到的。 测试阶段​ 给定一个待测试的物体，首先我们通过 GraspCVAE decoder 来创建出初始的抓型。创建出的抓型和物体点云一起传进 ContactNet 中来预测目标的 contact map。因为此时 ContactNet 已经用GT训练完成了，且其 Loss的目标是 penetration-free、且手指紧贴物体，所以我们认为此时 ContactNet 预测出来的 contact map是理想的接触。 我们使用 ContactNet 预测出来的 contact map作为 GT，来继续优化我们 GraspCVAE 创建出来的 initial grasp。如果GraspCVAE所创建出来的抓型是比较完美的，那么它的抓型直接计算的 contact map应该和 ContactNet 预测出来的 contact map一致。我们使用这个一致性作为自监督的signal来在测试阶段优化 GraspCVAE创建出来的 grasps。 训练过程​ 在训练过程中，手是通过点云$P^h\\in R^{778\\times 3}$来建模的，物体是通过点云$P^o\\in R^{N\\times 3}$来建模的。我们分别使用2个PointNet来提取特征$F^h, F^o\\in R^{1024}$。然后把两个特征接在一起传入 Encoder中，输出$\\mu \\in R^{64}, \\sigma^2\\in R^{64}$。我们从高斯分布中采样隐变量$z$。 ​ Decoder输入是隐变量$z$和物体的特征$F^{o}$来重建 hand mesh。Hand mesh是通过可微分的 MANO 模型表示的。MANO模型通过形状参数$\\beta \\in R^{10}$来描述人的不同的手型，而$\\theta \\in R^{51}$ 来描述关节的旋转角和基关节的平移。给定 Decoder 预测出来的参数$(\\hat{\\beta},\\hat{\\theta})$，MANO模型可以创建出一个可微分层来创建出手的模型，即$\\hat{M}=(\\hat{V}, \\hat{F})$，其中$\\hat{V}\\in R^{778\\times 3}$代表网格的顶点，而$\\hat{F}$ 代表 mesh 的面。 Object-centric Loss​ 当我们关注被抓取物体的时候，它上面总有一些区域是和手接触的。我们希望我们的手和这些区域是尽可能近的，所以就有了 Object-centric Loss 。具体地来说，从GT的 hand-object interaction中，我们可以计算出 object contact map $\\Omega\\in R^N$，具体计算流程如下： ​ 我们计算每个物体点到达最近的 hand vertex的距离，记为$D(P^o)$，并且归一化，记为$\\Omega = f(D(P^o))$，且有$$f(x)=1-2(\\text{Sigmoid}(2x) - 0.5)$$ ​ 如上图所示，每个点都被赋予了一个0~1的值，作为我们的 Contact Map。归一化的目的是让网络关注那些接近 hand 的区域。这样我们就可以使用如下 Loss 来达到我们的目的：$$L_O=||\\hat{\\Omega}-\\Omega||_2^2$$ Hand-centric Loss​ 我们预先定义好了手上的一些接触顶点 $V^p$，如下图所示： ​ 给定hand contact vertices 的预测位置，我们认为每个手上 contact region vertex附近的物体点是一些潜在的接触点。对于物体点云上的每个点$P_i^o$，我们计算它到最近的接触顶点的距离，如果小于一个阈值，我们就认为它是物体上的接触点。我们 Hand-centric loss的目标就是让手上的接触顶点尽可能靠近物体：$$L_H=\\sum_iD(P^o_i), \\text{for all } D(P^o_i\\leq \\tau)$$​ 我们定义$\\tau=1\\text{cm}$，最终我们的loss的组合如下：$$L_{grasp}=L_{baseline}+\\lambda_OL_O+\\lambda_HL_H$$​ 简单的来说$L_O$回答了哪里是可以抓取的位置的问题，而$L_H$回答了哪些手指应当接触的问题。 ContactNet​ 输入手点云$P^h\\in R^{778\\times 3}$和物体点云$P^o\\in R^{N\\times 3}$。网络的目标是预测出 contact map $\\Omega^c\\in R^N$。 ​ 网络的Loss设定为预测出的 contact map 和 GT 的 $L_2$范数，即$$L_{cont}=||\\Omega^c-\\Omega||_2^2$$ 接触推理过程（TTA, Test Time Adaption）​ 在测试过程中，对于一个物体点云，我们先使用 CVAE 创建出一个手的mesh $\\hat{M}$，如下图所示： ​ 我们可以通过$\\hat{M}$ 手动计算出对应的 contact map $\\Omega_{\\hat{M}}$。然后我们继续把物体点云和 $\\hat{M}$ 传入到 ContactNet 中得到较好的 $\\Omega^c$。我们希望让我们的$\\Omega_{\\hat{M}}$尽可能接近$\\Omega^c$，所以这是一种自监督的方式继续优化GraspCVAE的参数，Loss如下：$$L_{TTA}=L_{refine}+\\lambda_HL_H+\\lambda_pL_{penetration}$$​ 我们使用这个Loss来更新 GraspCVAE decoder的参数，冻结两个网络的其余部分。 实验部分和代码部分​ 此处有需要再看。需要关注的是手和物体的数据集，如Obman Dataset。以及手的建模的MANO模型的细节，以及一些度量指标。还有4种TTA的具体逻辑和比较实验，TTA是一个比较新颖的逻辑，有时候看完TTA并不知道它能不能work，还是要加深这部分的理解。","link":"/blog/2022/03/13/GraspTTA/"},{"title":"(CVPR2019)NOCS","text":"​ 在阅读关节体位姿估计以及关节种类估计、screw parameter的估计中，很多论文都引用了这篇CVPR2019的论文。 ​ 这篇论文的全名叫做 Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation，也就是网络希望泛化到同一类的不同实例中，这就要求提一个类全局的特征。包括说6D Object Pose and Size Estimation其实就是在VoteNet的3D Bounding Box的基础上，解锁了全部旋转自由度（VoteNet用heading bin简化到了只对z轴的转角做分类估计）。 ​ 为了提取类全局特征，这篇工作提出了NOCS（Normalized Object Coordinate Space），它是一个类中所有实例的共享的表征。所以，网络希望从观测到的RGBD中提取出NOCS表征、instance mask等。 NOCS​ NOCS是一个3D的单位立方体空间，也就是$\\{x,y,z\\}\\in[0,1]$。我们把训练集中已有的同一类的CAD模型归一化其方向、尺寸，放进这个立方体空间中。 ​ 我们使用RGB值把上图可视化出来了。在训练过程中，我们尝试训练出输入的RGB图到这个NOCS Map的投影。在训练过程中，网络会尝试回归出NOCS Map并且和深度图一起来估计6D Pose和Size。 网络 Pose Fitting​ 当我们有了instance mask、depth和NOCS Map之后，我们可以通过深度图+ Mask得到对应点云$P_m$，并且由于我们的NOCS Map是有RGB颜色的，我们可以根据RGB的值映射回点云$P_n$。接下来就是使用点云配准算法来得到旋转矩阵、平移和缩放系数。 Umeyama Algorithm（点云配准）​ 主要可以参考这个博客。 ​ 使用条件要求点云$P_n$和$P_m$的点的数量是相同的，并且对应关系是一致的，其中的点不能共线。 ​ 该算法的目标是计算一组$R,t$使得目标函数最优：$$\\frac{1}{n}\\sum_{i=1}^n||q_i-(cR)||$$","link":"/blog/2022/03/02/NOCS/"},{"title":"MoveIt Setup Assistant","text":"​ This article shows how to configure a custom arm in MoveIt. 1. Load the model​ Make sure that your moveit is correctly configured before moving on the turorial. And this article is mainly adapted from ROS MoveIt tutorial, but I configure a flexiv robot not a franka-panda. ​ First, use the following command to open the moveit setup assistant GUI. 1roslaunch moveit_setup_assistant setup_assistant.launch ​ Press “Create New MoveIt Configuration Package”, and choose the .urdf file. ​ After importing the urdf model, you can check the robot model in the right. If there something wrong, like missing part, or some red error shown in the bash, try to check the correctness of the urdf model. Those urdf models that are not following the ROS conventions can not be correctly processed by the MoveIt setup assistant. 2. Compute the self-collisions matrix​ Just press the button, if there are no special needs. 3. Define virtual joints​ Virtual joints are used primarily to attach the robot to the world. ​ For our flexiv model, we can find in urdf there is a joint0 serving as virtual joints in the urdf, ​ so we don’t add one more joint here, but if we fail at last, we may return back here to move the virtual joint in the urdf. 4. Define planning groups​ Planning groups are necessary for us to plan in MoveIt. Here we first define a planning group called arm, which use kdl_kinematics_plugin. ​ And we need to press “add joints” to configure the planning group. ​ Joint 0 to 7 are added into the planning group, because later the kinematic solver will need the 7 joints to calculate solutions. ​ Next, we should configure a planning group called “hand”, with no ik solvers and all links contained by the gripper. The following picture is the result when we successfully configure both. 5. Define robot poses​ We can add some useful configurations to use in future. 6. Define end effectors 7. Add passive joints​ The passive joints tab is meant to allow specification of any passive joints that might exist in a robot. These are joints that are unactuated on a robot (e.g. passive casters.) This tells the planners that they cannot (kinematically) plan for these joints because they can’t be directly controlled. The Panda does not have any passive joints so we will skip this step.[1] Not quiet sure, I leave empty here as well. 8. 3D perception​ Not used here. 9. Simulation with Gazebo​ Gazebo is not used here. 10. ROS control​ Clicking the “Auto add …”, then we have ​ We should edit both one. ​ Change the first to a joint position controller. 11. Add author information​ Do what it says. 12. Configuration Files​ Just generate the package. 13. Visualize the robot in RViz​ We can visualize the robot in RViz by the following command after building the newly created package. 1roslaunch panda_moveit_config demo.launch ​ Everything seems to be in the right place. You can refer to RViz tutorial for more details.","link":"/blog/2021/11/05/MoveIt-Setup-Assistant/"},{"title":"SE3353-assignment10","text":"简单总结一下这次作业。使用Docker-Compose封装SpringBoot应用、Nginx、MySQL、MongoDB和Redis。 12345678910111213141516任务要求：请你根据上课内容，针对你在E-BookStore项目中的数据库设计，完成下列任务：1.请你参照课程样例，构建你的E-BookStore的集群，它应该至少包含 1 个nginx实例(负载均衡) + 1 个Redis实例(存储session) + 2 个Tomcat后端实例。(4分）2.所使用的框架不限，例如可以不使用nginx而选用其他负载均衡器，或不使用Redis而选用其他缓存工具。3.参照上课演示的案例，将上述系统实现容器化部署，即负载均衡器(或注册中心和Gateway)、缓存和服务集群都在容器中部署。 (1分）– 请提交一份Word文档，详细叙述你的实现方式；并提交你的工程代码。评分标准：– 能够正确地部署和运行上述系统，在验收时需当面演示。– 部署方案不满足条件或无法正确运行，则视情况扣分。 ​ 整体框架可以参考这个博客，但是真正debug起来前前后后也是花了一两周时间，只能说还是docker用得太少了，以及docker集群中的错误定位在没有经验的情况下还是非常痛苦的，包括思维的“连续性”这件事情，因为每次拾起来这个任务总需要花费一些建立去retrieve之前的记忆，但每次能够挤出来的时间又是碎片时间，导致retrieve记忆的效率很低。相关代码在这个仓库。 SpringBoot的Docker化​ 也就是把一个Spring Boot工程打包成Docker镜像，这件事情网上有各种各样奇奇怪怪的教程让我去安装什么插件，配置什么端口。这些都是不必要的事情。首先明确我们整个过程打包出来的就只有一个单纯的jar包，这个jar包是平台无关的，我们只需要根据这个jar包封装成一个docker镜像并且启动起来实例就可以了。所以其实我们只需要在bookstore_backend-1.0-SNAPSHOT.jar的相同目录下新建一个Dockerfile文件，写入如下代码： 1234567891011#基于哪个镜像FROM java:8 # 拷贝文件到容器，也可以直接写成ADD microservice-discovery-eureka-0.0.1-SNAPSHOT.jar /app.jarADD bookstore_backend-1.0-SNAPSHOT.jar /app.jar # 开放9090端口EXPOSE 9090 # 配置容器启动后执行的命令ENTRYPOINT [&quot;java&quot;,&quot;-Djava.security.egd=file:/dev/./urandom&quot;,&quot;-jar&quot;,&quot;/app.jar&quot;] MySQL的Docker部署​ 理论上这是一个非常非常非常常见的应用场景，但是就是在这一步卡了我非常久非常久的时间。 ​ 首先是，因为是从容器中启动MySQL，最开始容器中是没有数据库的，所以需要有一个初始化过程。所以先要从我们原先的生产环境中把数据库拿出来，这里就直接使用这篇博客提到的全量备份方法： 1mysqldump -uroot -p123123 --databases bookstore&gt;./init/init.sql ​ 但是根据教程把在docker-compose中绑定- ./init:/docker-entrypoint-initdb.d后，启动以后还是没有bookstore数据库。根据这篇回答，在command中添加–init-file /docker-entrypoint-initdb.d/init.sql，这才让我真正能够在启动容器的时候初始化我的数据库。 ​ 接下来就是需要让我们的SpringBook容器真正连上这个容器中的MySQL，这才算是打通了集群化部署的工作流程。 ​ 总体踩过的坑如下： 在docker-compose的MySQL的environment中，必须设置MYSQL_ROOT_HOST: ‘%’，根据MySQL官网的叙述，如果不设置这一条，MySQL容器只会允许‘root‘@’localhost’，也就是容器内部的查询请求通过，但是我们配置出来的MySQL显然是需要让内网的别的容器访问的。 在docker-compose的MySQL的environment的MYSQL_ROOT_PASSWORD中，似乎不支持纯数字密码。（至少我设置123456登录不上去，设置为纯字母密码才可以）。 在docker-compose的MySQL的environment中，可能需要设置MYSQL_DATABASE。这个变量指定了我们在创建镜像时需要创建的数据库，如果我们还设置了MYSQL_USER，会同时授予User访问这个数据库的权限。 在Spring Boot的application.properties中，如果我们之前设置的是访问路径是‘jdbc:mysql://localhost:3306/bookstore?autoReconnect=true&amp;useSSL=false‘，我们需要把localhost修改为docker-compose中service的名字，否则这样SpringBoot会调用localhost自己容器中的3306端口，但是它自己的容器里是没有MySQL的。 在Debug过程中，其实看到最多的是如下的报错信息： 123456java.sql.SQLNonTransientConnectionException: Could not create connection to database server. Attempted reconnect 3 times. Giving up.#此处省略几十行 at com.bookstore.Application.main(Application.java:123) ~[classes/:na]Caused by: com.mysql.cj.exceptions.CJException: Access denied for user 'root'@'localhost' (using password: YES)#此处省略几十行 ... 50 common frames omitted ​ 现在回过头来看其实当时主要还是看到那么多行报错一直在搜第一行的错误原因，而中间这一段才是真正的原因，也就是本机尝试登录root账户失败，那显然就是密码错误。 ​ 总体的和MySQL相关的配置文件如下： 123456789101112131415161718192021222324252627app_db: image: mysql:8.0.23 hostname: testt command: # MySQL8的密码验证方式默认是 caching_sha2_password，但是很多的连接工具还不支持该方式 # 就需要手动设置下mysql的密码认证方式为以前的 mysql_native_password 方式 --default-authentication-plugin=mysql_native_password --character-set-server=utf8mb4 --collation-server=utf8mb4_general_ci --init-file /docker-entrypoint-initdb.d/init.sql #attention here # docker的重启策略：在容器退出时总是重启容器，但是不考虑在Docker守护进程启动时就已经停止了的容器 restart: unless-stopped environment: MYSQL_ROOT_PASSWORD: root_password # root用户的密码 MYSQL_DATABASE: bookstore MYSQL_ROOT_HOST: '%' ports: - 3306:3306 expose: - 3306 volumes: - ./db:/var/lib/mysql - ./conf:/etc/mysql/conf.d - ./log:/logs - ./init:/docker-entrypoint-initdb.d 其余部分的容器化​ 其他的部分（包括MongoDB,Nginx,Redis,Neo4j）的容器部署大差不差，这些一共加起来可能也就花了半小时时间。主要还是配置第一个MySQL的时候踩了太多太多的坑。 ​ 最后放一些结果图。 ​ 我们可以看到Nginx确实把请求轮询分发给了两台Server。 ​ 并且在响应的结果中，我们确实看到了MySQL和MongoDB组装成功响应。","link":"/blog/2021/12/18/SE3353-assignment10/"},{"title":"SE3353_assignment4","text":"简单总结一下这次作业。基于Lucene的全文搜索功能和WebService的封装。 在你的项目中增加基于Solr或Lucene的针对书籍简介的全文搜索功能，用户可以在搜索界面输入搜索关键词，你可以通过全文搜索引擎找到书籍简介中包含该关键词的书籍列表。为了实现起来方便，你可以自己设计文本文件格式来存储书籍简介信息。例如，你可以将所有书籍的简介信息存储成为JSON对象，包含书的ID和简介文本，每行存储一本书的JSON对象。 请将上述全文搜索功能开发并部署为Web Service。 一、开发基于Lucene的全文搜索功能我们实现一个ApplicationRunner接口，这样它会在SpringBoot启动的时候自动执行一次，把数据库中的书籍及其间接建立索引。 为了测试这个功能，我们添加了一个用于测试的Controller。 我们分别输入“你”和“奥秘”作为关键词，我们可以清楚地看见确实返回了所有包含关键词的书籍。 二、把全文搜索功能整合到WebService中 如果说第一部分在十分钟内就可以基本上完成的话，这部分就非常冗长了。首先是WebService的选型，我首先选了Jax-ws作为后端webservice的实现。后面实现起来一切正常，但是当我想整合前端时出现了巨大的问题。简而言之就是，前端因为要组装text/xml作为SOAP协议的body，但是在firefox和Chrome中，这属于“非简单请求”，会自动先发送一个preflight。 非简单请求的CORS请求，会在正式通信之前，增加一次HTTP查询请求，称为”预检”请求（preflight）”预检”请求用的请求方法是OPTIONS，表示这个请求是用来询问的。头信息里面，关键字段是Origin，表示请求来自哪个源。浏览器先询问服务器，当前网页所在的域名是否在服务器的许可名单之中，以及可以使用哪些HTTP动词和头信息字段。只有得到肯定答复，浏览器才会发出正式的XMLHttpRequest请求，否则就报错。 就是因为这个OPTION请求让Jax-ws用不了，发请求的时候SpringBoot会报出： com.sun.xml.internal.ws.server.http : 无法处理 HTTP 方法: OPTIONS 我们需要定义interceptor去拦截这个OPTIONS请求，并且返回一个200:OK，但是找了半天不知道这个接口在哪里。使用SpringBoot的HandlerInterceptor也拦不到这个OPTIONS请求，陷入僵局。 后来看到了这篇文章js调用webservice接口时后台无法处理OPTIONS请求的解决方法，只能弃用Jax-ws，启用cxf。 拦截器参考网上的抄了一份过来，简单而言就是如果拦截到的是preflight，那就设置对应的Header后返回200:OK。 具体的webservice倒显得很容易了，如下是Service层的代码，还有一个CXFConfig类。 我们可以在对应暴露出的url后添加?wsdl，来得到这个webservice对应的wsdl文件。 理论上前端应该是要parse这个wsdl文件然后生成对应的SOAP消息的，不过限于时间，和具体业务其实不需要这么大的灵活度，所以就没做，固定了前端的格式。 值得一提的是，前端的SOAP消息格式是使用了SOAPUI这个软件，传入wsdl文件后自动生成出来的，节省了我很多debug的时间。 剩下的无非就是前端的一些工作了，封装出对应的SOAP消息生成和解析器即可。 搜索“你”和“奥秘”，结果与之前我们在controller中测试的结果完全相同。任务结束。","link":"/blog/2021/10/20/SE3353-assignment4/"},{"title":"SE3353-assignment8","text":"简单总结一下这次作业。基于MongoDB和Neo4j的存储功能。 12 1.将你认为合适的内容改造为在MongoDB中存储，例如书的产品评价或书评。你可以参照课程样例将数据分别存储在MySQL和MongoDB中，也可以将所有数据都存储在MongoDB中，如果采用后者，需要确保系统功能都能正常实现，包括书籍浏览、查询、下订单和管理库存等。​ 2.为你的每一本图书都添加一些标签，在Neo4J中将这些标签构建成一张图。在系统中增加一项搜索功能，如果用户按照标签搜索，你可以将Neo4J中存储的与用户选中的标签以及通过2重关系可以关联到的所有标签都选出，作为搜索的依据，在MySQL中搜索所有带有这些标签中任意一个或多个的图书，作为图书搜索结果呈现给用户。 ​ 最近有点眼高手低，有些技术以前用过就自以为可以不必记录下来了。要遏制住这股风气，因为我并不了解任何工具的底层、实现也全靠CSDN，哪怕为了第二次复现起来、第二次快速回忆起现在做的事情，也需要详尽地写下目前的操作。 PART1：MongoDB首先是安装并运行MongoDB，在安装目录下，先运行mongod.exe，再运行mongo.exe即可看到如下的命令行： 123456789101112131415161718192021222324---&gt; 2 + 24&gt; db.runoob.insert({x:10})WriteResult({ &quot;nInserted&quot; : 1 })&gt; db.runoob.find(){ &quot;_id&quot; : ObjectId(&quot;6190c5a1e7f64d1430e95cde&quot;), &quot;x&quot; : 10 }&gt; db.auth(&quot;root&quot;,&quot;123456&quot;)Error: Authentication failed.0&gt; use adminswitched to db admin&gt; db.createUser({user:&quot;root&quot;,pwd:&quot;1234&quot;,roles:[{&quot;role&quot;:&quot;readWrite&quot;,&quot;db&quot;:&quot;demo&quot;}]})Successfully added user: { &quot;user&quot; : &quot;root&quot;, &quot;roles&quot; : [ { &quot;role&quot; : &quot;readWrite&quot;, &quot;db&quot; : &quot;demo&quot; } ]}&gt; db.auth(&quot;root&quot;,&quot;1234&quot;)1 ​ 我们在书店的DAO层中添加如下代码： 123456789101112131415@Overridepublic Book addRemark(Long book_id, String remark) { Book book = findOne(book_id); Remark remarks = book.getRemark(); if (remarks == null) { remarks = new Remark(book.getId()); } else { remarkRepository.delete(book.getRemark()); } remarks.addRemark(remark); book.setRemark(remarks); save(book); return book;} ​ 因为MongoDB不支持相同id的document的覆盖操作，所以我们每次取出来就删除掉，然后再把新的评论加上后再save到collection中，唯一的问题可能就是在delete后save前如果server crash了，可能会导致这个书籍的评论消失。可以通过额外的数据备份机制来避免这个情况。 ​ 使用ROBO3T可以看到，确实我们每次添加一条评论，就自动添加到Collection对应book id的document下了。 PART2：Neo4j​ 按照Neo4j desktop很快，但是把它在SpringBoot中调通用了很大波折。我发现只要加入spring-boot-starter-data-neo4j这个依赖，启动后就会报如下的错。 12345678org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'reactiveNeo4jTemplate' defined in class path resource [org/springframework/boot/autoconfigure/data/neo4j/Neo4jReactiveDataAutoConfiguration.class]: Post-processing of merged bean definition failed; nested exception is java.lang.IllegalStateException: Failed to introspect Class [org.springframework.data.neo4j.core.ReactiveNeo4jTemplate] from ClassLoader ...Caused by: java.lang.ClassNotFoundException: reactor.util.context.ContextView at java.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[na:1.8.0_91] at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[na:1.8.0_91] at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) ~[na:1.8.0_91] at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[na:1.8.0_91] ... 26 common frames omitted ​ 依赖也没冲突，代码都没开始写，就出这个问题，真的卡了好几个小时。后来不抱希望地看到了这个issue，添加了依赖： 123456&lt;!-- https://mvnrepository.com/artifact/io.projectreactor.netty/reactor-netty --&gt;&lt;dependency&gt; &lt;groupId&gt;io.projectreactor.netty&lt;/groupId&gt; &lt;artifactId&gt;reactor-netty&lt;/artifactId&gt; &lt;version&gt;1.0.13&lt;/version&gt;&lt;/dependency&gt; ​ 居然就能用了，真是百思不得其解。 可见成功初始化，创建了由书籍和标签构成的一张图。 我们如下extend Neo4jRepository 123456public interface BookNodeRepository extends Neo4jRepository&lt;BookNode, Long&gt; { BookNode findByName(String name); BookNode findByTagNodesName(String name); @Query(&quot;MATCH (p:BookNode) -[:contain]-&gt;(p1:TagNode)&lt;-[:contain]- (p2:BookNode) where p.name=~ $n return p2&quot;) List&lt;BookNode&gt; findByTwoRelationship(@Param(&quot;n&quot;) String n);} 和我们在Neo4j Desktop中查询到的完全一样 Customizing Query by @Query annotation","link":"/blog/2021/11/22/SE3353-assignment8/"},{"title":"A Real-World Articulated Object Knowledge Base","text":"​ 关节体的属性包括外表、结构、物理属性和语义类别。但是目前的CAD模型通常缺失了物理属性。所以AKB-48是一个大规模的从物理世界中扫描的关节体数据集，每个物体都通过知识图谱ArtiKG（Articulation Knowledge Graph）来描述。为了构建AKB-48这个数据集，这个工作通过fast articulation knowledge modeling(FArM) pipeline来构建每个物体的ArtiKG，通常只需要10~15分钟。这个工作进一步使用数据集提出了AKBNet，它是用来做物体类别级别的视觉关节体操作的。 数据集部分​ 首先，我们讨论关节体的知识图谱到底需要包含哪些内容。 Appearance：mesh数据以及对应的纹理。 Structure：这是关节体和刚体最不一样的地方，也就是我们需要有关节体的动力学结构，包含关节的类型、对应的参数以及范围。 Semantics：这部分的定义我不太理解，文章中要通过WordNet来标注出对应的语义信息。换句话说，对于一个杯子来说，虽然从动力学结构上，杯柄和杯体是一体的，但是从语义的角度，杯柄是可以用来抓的，Semantics似乎是要把这部分标注出来。 Physics Property：真实世界中存在着的物体是有真实的物理属性的，而CAD建模相对就没有这么准确了，这也是这篇文章认为的扫描模型的优势之一。所以我们需要得到，每个关节部分的质量、转动惯量、材质和表面摩擦力。 FArM标注Pipeline Object Alignment：首先，如上图所示对物体进行多视角的扫描。扫描物体的坐标系原点是在扫描仪光圈位置（可以理解成相机围着物体转一圈获得3Dmesh），所以我们需要把同一类物体全部对齐到统一坐标系下，比如所有的水瓶应该瓶口向上方向为z轴，所以alignment就是将物体坐标系统一的。 Part Segmentation：我们需要对整体做分割，分割成几个动力学上分离的部分，如果现实中的物体就是可分离的话，我们也可以每部分扫描进去以后再拼在一起。 Joint Annotation：然后我们就是标注joint的位置。 Physics Annotation：然后还需要标注几个物理属性：如每部分的质量、惯性、材料等，细节不不再阐述。 AKBNet​ 在这个网络中，输入是单张RGBD图以及对应的2D边界框，我们的目标是估计每个关节体部分的6D位姿，重建关节体的几何数据，并且推理出交互的策略，如下图所示： ​ 首先是这个带Bounding Box的RGBD数据，我认为它只是单纯地想把物体分割出来，或者说是引导后续分割网络的剪枝，因为可想而知不分割出来还想做part-level 的segmentation的话，这些数据量和diversity是远远不够的。 Pose Module​ 我们先看中间，过了一个 PointNet++以后，对全局特征直接回归出关节类型和参数，得到关节类型以后，通过NOCS和Pose优化算法，得到每个部分的6D Pose。然后我们再看Pose Module的上半部分，是对每个点过了一个二分类，得到了Part的分割，然后对每个part点云，去预测它的质量、摩擦力和转动惯量。因为都是标好的数据，监督学习就行了，至于准确率我觉得更应当考虑标数据的时候那些算法是否靠谱。关节体估计的一些关键算法，如NOCS和ANCSH，之后会慢慢更新上来。 Shape Module​ 这部分相对比较陌生，这篇文章主要介绍了引用的论文。目标其实就是通过点云和估计出的关节角度$\\theta$来恢复出整个模型$M_{\\theta}$。 Manipulation Module​ 把训出来的网络的输出全部作为观测，然后传给RL去训就行了。这里唯一的一个问题就是，估计出来的A-SDF距离场$M_{\\theta}$以一种什么样的形式传入呢。因为A-SDF可以认为是一个$R^3\\rightarrow R$的一个距离场，维度还是挺高的，这部分细节还在等作者回复。具体实现的方法就是把还原的模型作为一个完整的观测加载到仿真器中，作为Agent要操作的模型。而我们估计出来的关节参数和6D Pose作为引导RL的一个观测量加入到observation中。","link":"/blog/2022/02/19/akb48/"},{"title":"(IJCAI2018)Behavior Clone from Observation","text":"​ 通过这篇论文继续补充自己的理论知识。 ​ 在智能体通过模仿others的任务完成过程从而进行模仿学习的时候，通常会遇到两个问题： 通常模仿的内容（示教）只有状态信息而没有显式的动作信息。 学习速度要非常快。 ​ 这篇工作提出了一个2阶段的自动模仿学习技术，叫做behavior cloning from observation。 模仿学习​ 本图来源于OpenDeepRL，是对模仿学习的系统解构，这样我们就知道了其实我们上次所看到的SOIL和本篇文章的BCO都是基于反向动态模型(inverse dynamics model)的模仿学习策略，看来要把整个领域完全涵盖还是需要很大功夫的，包括说手头上的代码能力也需要同步提升起来。 行为克隆(Behavior Clone)​ 这篇文章是搜Behavior Clone偶然搜到的高引，但由上图可知，其实BCO和BC并不属于同一个模仿学习分支，故此处先摘抄深度强化学习中对BC的定义。使用监督学习模仿专家示范的方法在文献中称为$\\textbf{行为克隆}$。 ​ 给定示教数据集$D=\\{(s_i,a_i)|i=1,…,N\\}$，我们可以训练出确定性的专家策略$\\pi_{\\theta}(s)$，优化目标如下：$$\\min_{\\theta}\\sum_{(s_i,a_i)\\sim D}||a_i-\\pi_\\theta(s_i)||_2^2$$​ 一些随机性策略$\\pi_{\\theta}(\\widetilde{a}|s)$的具体形式，如高斯策略等，可以通过再参数化技巧来处理：$$\\min_{\\theta}\\sum_{\\widetilde{a}_i\\sim\\pi(\\cdot|s_i),(s_i,a_i)\\sim D}||a_i,\\widetilde{a}_i||_2^2$$ 问题重述​ 马尔科夫决策过程（MDP）通过五元组$M=\\{S,A,T,r,\\gamma\\}$表述，其中S是状态空间，A是动作空间，$T_{s_{i+1}}^{s_i,a}=P(s_{i+1}|s_i,a)$代表了智能体在状态$s_i$下执行动作$a$转移到状态$s_{i+1}$的概率，$r:S\\times A\\rightarrow R$是描述立即奖励的函数，$\\gamma$是打折因子。智能体的行为为$\\pi: S\\rightarrow A$。一个智能体以策略$\\pi$所经历的一系列状态转换，在本文中用$T_{\\pi}=\\{(s_i,s_{i+1})\\}$来表示。 ​ 在这些转换中，我们关心的是反向动态模型。也就是我们仅仅能知道状态转移$(s_i,s_{i+1})$的时候，我们希望得到对应的$a_i$的概率分布。 ​ 我们继续把 state 划分为智能体特定的 state $S^a$和任务特定的 state $S^t$，即$S=S^a\\times S^t$。在这个划分下，我们定义智能体特定的 反向动态模型为$M_{\\theta}: S^a\\times S^a\\rightarrow p(A)$，它把 $(s_i^a,s_{i+1}^a)\\in T_{\\pi}^a$ 映射到对应的动作分布上。 ​ 模仿学习通常定义在没有显式 reward function 的MDP上，即 $M$\\ $r$。智能体目标就是学习像专家一样的策略$\\pi:S\\rightarrow A$，智能体有的可以学习的东西就是专家的示教集合，即$\\{\\xi_1,\\xi_2,…\\}$，其中每个$\\xi$是一条示教轨迹$\\{(s_0,a_0),(s_1,a_1),…,(s_N,a_N)\\}$。因此，智能体必须要得到示教集中的动作才能去学习。对于示教轨迹没有动作的情况，这就是一个从观察中进行模仿学习（Ifo）的场景。 本文中的问题定义​ 给定一系列只包含状态的示教轨迹$D$，使用最小的post-demonstration环境交互$|I^{post}|$来找到一个好的模仿学习策略。 BCO(0)​ 本文提出了两个基础模块：inverse dynamics model和behavior cloning模块。 反向动态模型(Inverse Dynamics Model)​ 我们可以让智能体使用随机策略$\\pi$收集一些先验的经验，记为$I^{pre}$。我们把反向动态模型用参数$\\theta$描述为$M_{\\theta}$。在这一阶段，我们简单地使用极大似然估计来得到$\\theta^*$，即：$$\\theta^*=\\arg\\max_{\\theta}\\prod_{i=0}^{|I_{pre}|}p_{\\theta}(a_i|, s_i^a,s_{i+1}^a)$$​ 其中$p_\\theta$是模型$M_\\theta$对于一个转移所估计出来的可能动作的分布。对于这个模型，我们使用随机梯度下降来进行训练。也就是当我们有样本$(s_{i},a_{i},s_{i+1})$的时候，我们会让梯度的目标为提高对应$a_i$的概率。 Behavior Cloning​ 我们从示教集合$D_{demo}$中提取出agent-specific的状态转移对，构成$T_{demo}^a=\\{(s_t^a,s_{t+1}^a)\\}$，并且使用我们刚才先验训练好的模型$M_{\\theta^*}$估计出$\\widetilde{a}_i$，这样我们就有了有对应估计出的动作的示教数据集。有了这个以后，我们就可以尝试对模仿策略$\\pi_\\phi$做估计了，也就是找到一个参数$\\phi$，使得产生出含估计动作的示教数据集的概率最大，也是极大似然估计的思想：$$\\phi^*=\\arg\\max_{\\phi}\\prod_{i=0}^N\\pi_{\\phi}(\\widetilde{a}_i|s_i)$$​ 我们使用神经网络把策略参数$\\phi^*$给学出来。 BCO($\\alpha$)算法​ 上述两个描述了BCO算法的基本模块。如果我们希望进一步从后验的环境交互中进一步提升我们的两个模型的话，本文提出了叫做$BCO(\\alpha)$的改进版本，它可以同时提升学习到的inverse dynamics模型和最终估计出来的模仿策略。 ​ 在每一步behavioral cloning后，智能体可以在环境中执行它所估计出来的imitation policy一段时间。然后，我们收集新的state-action序列来更新模型和对应的imitation policy。 ​ BCO($\\alpha$)算法 用随机参数来初始化$M_{\\theta}$用随机参数来初始化$\\pi_{\\phi}$设置$I= ​ 其中的$\\alpha$是用来控制pose-demonstration环境交互和pre-demonstration的比例，当我们指定非零的$\\alpha$的时候，我们就可以迭代地改进我们的反向动态模型和学习到的imitation policy。 实验 ​ 其中的GAIL和FEM都是IRL的baseline，注意到因为我们在训练之前已经收集并且训练了pre-demonstration experience并且训练完了反向动态模型，所以我们最终的BCO(0)的动作是固定的反向动态模型所输出的模型。 ​ 具体细节等到需要了再回来看。 比较一下SOIL和这篇文章​ 因为SOIL提到了behavior cloning，所以搜到了这篇文章，并且看下来感觉两篇文章还挺像的，所以还是对比一下两者。 更新模型的角度​ SOIL使用了改进的模仿学习策略梯度，有一个模仿学习项来更新策略，即$$g_{soil}=g+\\lambda_0\\lambda_1^k\\sum_{(s,a’)\\in D’}\\nabla_{\\theta}\\log\\pi_{\\theta}(a’|s)$$​ 而这篇文章只是简单地用神经网络做了极大似然估计尝试把策略的概率分布估计出来。 算法的角度​ 在$BCO(0)$中，因为它pre-demonstration使用的是随机策略，如果动作空间维度很大，那么可能根本学不到一个好的inverse dynamics模型，所以基本上$BCO(\\alpha)$是标配。$BCO(\\alpha)$和SOIL的区别其实不大，感觉SOIL只是在灵巧手这个特定的角度切入去做了一个work的工作，所以从创新性来说，SOIL最终投到了IROS是合理的。","link":"/blog/2022/02/17/behavior-clone/"},{"title":"behavior_tree_pre","text":"A internal talk of MVIG lab","link":"/blog/2021/10/20/behavior-tree-pre/"},{"title":"SE335_Compliers_lab2","text":"总而言之前前后后好几天时间都在搞这个lab，不过主要花的时间可能是大半天时间。为了留下点东西，姑且总结一下这个lab。 其实这个lab可以说是烦而不难，基本上搞清楚flex c++的执行流，就完全可以干掉这个lab。 上面是一些无关痛痒的指令，算是第一步就可以很轻易写出来的东西。 一、处理嵌套注解 嵌套注解的处理其实从原理上就和括号匹配一样，至少要维护一个整形作为匹配的进度（/加一、/减一，最终为0时匹配完毕）。但是这样实现就要区分出在匹配完毕时（非注解情况下）和匹配进行时（注解情况下）的不同执行流，需要更改全部代码。其实，仔细查看lab2的提示和flex c++的文档就可以发现它提供了start condition来处理不同情况下的处理，这是一个很好用的工具。如上图所示，一旦匹配到了开始符号/*，那么自动push一个COMMENT状态即可；一旦匹配到了结束符号，那么就从栈中pop一个状态。 二、字符串的处理这部分是我debug了最久的地方，因为经常会出现匹配上了，但是开始的位置和标准答案差一个的情况、或者字符串后的各种符号平移了几位的情况，这是因为多种原因造成的： \\1. 比如A=”5”的情况，在匹配第一个”的时候，如果没有adjust，最终的开始位置就会在等号处。 \\2. 匹配转义字符时，不能根据最终转义完成的字符串的长度来计算adjust，这样会导致adjust数量过少。 首先是匹配普通字符串的情况，lex代码如下： 由于adjust函数会自动根据matched的字符串的长度来做调整位置，所以我们自己添加了一个adjustByLen，里面添加了对转义字符的offset。接下来是对转义字符的支持，主要可以分为四类： \\1. ^A, ^B, …., ^Z，具体其含义我们不细究，反正查ASCII表可知其值为1~26。 \\2. \\xxx，其中x为0~9的数字，也就是直接指定ASCII码。 \\3. \\n, \\t, \\”等，直接转义为对应字符即可。 \\4. \\跟了一个换行符的情况，我们需要支付字符串跨行输入，这个下一节再说。 总体逻辑就是trim我们的matched string，把转义使用的\\x都修剪掉，换成真的对应的转义字符。注意，我们需要累积计算我们的offset，这样我们才能知道最终我们需要移动多少位。 接下来是例子52中出现的这种情况： 我们自然也需要给予支持，也就是+一个换行符，自动进入ignore状态，等到读到下一个\\后，才退出到正常的字符串处理状态。注意ignore了多少个字符也需要添加进offset中。","link":"/blog/2021/10/17/complier-lab2/"},{"title":"complier-lab3","text":"​ This lab comes from SE3355 *Compilers* lab3, which requires me to use Bisonc++ to implement a parser for the Tiger language. ​ The blog is written after the completion of the lab, so some details may be forgotten. Our main target of this lab is to use a sequence of tokens which the lab2’s lexical scanner gives, to parse them and create a AST(Abstract Syntax Tree). The AST representation is necessary for us to do the semantic type checking in the next lab. What’s more, we need to convert AST to a intermediate language then, for our final processing when trying to get assembly language and binary executable file. ​ For convenience and succinctness, I will not again to explain the basic structure and class of the AST in the code. The code will be provided in my github repo when all labs are finished. ​ If you are not familiar with the workflow of the Bison C++, refer to the documentation for more details. Part 1 Some Simple ProductionsAccording to Appendix A of tiger book, we can easily write down the following productions. 1.1 basic productions of expression​ The basic primitive expression constructor is necessary, which forms the lower-level nodes(mainly leaf nodes) of the AST. 123456exp : INT {$$ = new absyn::IntExp(scanner_.GetTokPos(), $1);} | MINUS exp {$$ = new absyn::OpExp(scanner_.GetTokPos(), absyn::MINUS_OP, new absyn::IntExp(scanner_.GetTokPos(), 0), $2);} | STRING {$$ = new absyn::StringExp(scanner_.GetTokPos(), $1);} | lvalue {$$ = new absyn::VarExp(scanner_.GetTokPos(), $1);} | NIL {$$ = new absyn::NilExp(scanner_.GetTokPos());}; 1.2 Operation productions of expression1234567891011121314exp : MINUS INT {$$ = new absyn::IntExp(scanner_.GetTokPos(), -1 * $2);} | exp PLUS exp {$$ = new absyn::OpExp(scanner_.GetTokPos(), absyn::PLUS_OP, $1, $3);} | exp MINUS exp {$$ = new absyn::OpExp(scanner_.GetTokPos(), absyn::MINUS_OP, $1, $3);} | exp TIMES exp {$$ = new absyn::OpExp(scanner_.GetTokPos(), absyn::TIMES_OP, $1, $3);} | exp DIVIDE exp {$$ = new absyn::OpExp(scanner_.GetTokPos(), absyn::DIVIDE_OP, $1, $3);} | exp AND exp {$$ = new absyn::IfExp(scanner_.GetTokPos(), $1, $3, new absyn::IntExp(scanner_.GetTokPos(), 0));} | exp OR exp {$$ = new absyn::IfExp(scanner_.GetTokPos(), $1, new absyn::IntExp(scanner_.GetTokPos(), 1), $3);} | exp EQ exp {$$ = new absyn::OpExp(scanner_.GetTokPos(), absyn::EQ_OP, $1, $3);} | exp NEQ exp {$$ = new absyn::OpExp(scanner_.GetTokPos(), absyn::NEQ_OP, $1, $3);} | exp LT exp {$$ = new absyn::OpExp(scanner_.GetTokPos(), absyn::LT_OP, $1, $3);} | exp LE exp {$$ = new absyn::OpExp(scanner_.GetTokPos(), absyn::LE_OP, $1, $3);} | exp GT exp {$$ = new absyn::OpExp(scanner_.GetTokPos(), absyn::GT_OP, $1, $3);} | exp GE exp {$$ = new absyn::OpExp(scanner_.GetTokPos(), absyn::GE_OP, $1, $3);}; ​ We need to highlight the implementation of the AND and OR operation. Since we don’t have a AND_OP and OR_OP in the predefined declarations of the AST. We can use the following conversion: 123exp1 AND exp2&lt;=&gt;if exp1 then exp2 else 0 123exp1 OR exp2&lt;=&gt;if exp1 then 1 else exp2 ​ Since in tiger language, the if-expression will return either the then-body or the else-body according to the condition, we can implement the conversion without any worry. 1.3 General Declarations123456789101112exp : LET decs IN expseq END {$$ = new absyn::LetExp(scanner_.GetTokPos(), $2, $4);};decs : decs_nonempty {$$ = $1;} | {$$ = new absyn::DecList();} //may be empty here;decs_nonempty : decs_nonempty_s decs {$$ = $2; $$-&gt;Prepend($1);};decs_nonempty_s : vardec {$$ = $1;} | tydec {$$ = new absyn::TypeDec(scanner_.GetTokPos(), $1);} | fundec {$$ = new absyn::FunctionDec(scanner_.GetTokPos(), $1);}; ​ In let-in-end format, tiger language allow empty declarations in production. So decs can be devided into empty one and non-empty one. For those are not empty, it can be forwardly inferred to a list of decs_nonempty_s. We use a DecList class to realize such behavior. For example, 1234some declaration;some declaration;some declaration;some declaration; will finally form a class of absyn::DecList with four declarations in its member variable dec_list_ whose type is std::List&lt;dec *&gt;. 1.4 Variable declaration12vardec : VAR ID ASSIGN exp {$$ = new absyn::VarDec(scanner_.GetTokPos(), $2, nullptr, $4);} | VAR ID COLON ID ASSIGN exp {$$ = new absyn::VarDec(scanner_.GetTokPos(), $2, $4, $6);}; ​ Since now, we can handle codes like 12var a := 3;var b : string := &quot;foo&quot;; 1.5 Type declaration1234567891011121314151617tydec : tydec_one tydec {$$ = $2; $$-&gt;Prepend($1);} | tydec_one {$$ = new absyn::NameAndTyList($1);};tydec_one : TYPE ID EQ ty {$$ = new absyn::NameAndTy($2, $4); };ty : ID {$$ = new absyn::NameTy(scanner_.GetTokPos(), $1);} | LBRACE tyfields RBRACE {$$ = new absyn::RecordTy(scanner_.GetTokPos(), $2);} | ARRAY OF ID {$$ = new absyn::ArrayTy(scanner_.GetTokPos(), $3);};tyfields : tyfields_nonempty {$$ = $1;} | {$$ = new absyn::FieldList();} //may be empty here;tyfields_nonempty : tyfield {$$ = new absyn::FieldList($1);} | tyfield COMMA tyfields_nonempty {$$ = $3; $$-&gt;Prepend($1);};tyfield : ID COLON ID {$$ = new absyn::Field(scanner_.GetTokPos(), $1, $3);}; ​ As we can see, the tydec also contains a list of tydec_one, so we can apply the same strategy when we handle decs. There are three type declaration in tiger language. 123type a = int;type b = array of a;type c = {name:string, score:b}; ​ So we can easily recognize the first and third line of ty production, but the nested one seems more difficult. Since the typefields is also a non-fixed size list, so we also need to split it into the empty one and non-empty one. Each production will add one element to the typefields. 1.7 Function declaration​ The function declaration consists of two types. 12function id (tyfields) = expfunction id (tyfields) : type-id = exp ​ So we can easily get the following code. 12345fundec : fundec_one {$$ = new absyn::FunDecList($1);} | fundec_one fundec {$$ = $2; $$-&gt;Prepend($1);};fundec_one : FUNCTION ID LPAREN tyfields RPAREN EQ exp {$$ = new absyn::FunDec(scanner_.GetTokPos(), $2, $4, nullptr, $7);} | FUNCTION ID LPAREN tyfields RPAREN COLON ID EQ exp {$$ = new absyn::FunDec(scanner_.GetTokPos(), $2, $4, $7, $9);}; 1.8 Nested type assignment​ To initialize those type are declared in nest as follows, 1list{first=i, rest=readlist()} ​ we can also have 123456789exp : ID LBRACE rec RBRACE {$$ = new absyn::RecordExp(scanner_.GetTokPos(), $1, $3);}rec : rec_nonempty {$$ = $1;} | {$$ = new absyn::EFieldList();};rec_nonempty : rec_one {$$ = new absyn::EFieldList($1);} | rec_one COMMA rec_nonempty {$$ = $3; $$-&gt;Prepend($1);};rec_one : ID EQ exp {$$ = new absyn::EField($1, $3);}; 1.9 Other productions​ According to Appendix A, it’s easy to understand the following code. 1234567891011121314151617181920212223exp : LPAREN sequencing RPAREN {$$ = new absyn::SeqExp(scanner_.GetTokPos(), $2);} | LPAREN exp RPAREN {$$ = $2;} | LPAREN RPAREN {$$ = new absyn::VoidExp(scanner_.GetTokPos());} | LET IN END {$$ = new absyn::VoidExp(scanner_.GetTokPos());} | ID LPAREN actuals RPAREN {$$ = new absyn::CallExp(scanner_.GetTokPos(), $1, $3);} | lvalue ASSIGN exp {$$ = new absyn::AssignExp(scanner_.GetTokPos(), $1, $3);} | WHILE exp DO exp {$$ = new absyn::WhileExp(scanner_.GetTokPos(), $2, $4);} | FOR ID ASSIGN exp TO exp DO exp {$$ = new absyn::ForExp(scanner_.GetTokPos(), $2, $4, $6, $8);} | BREAK {$$ = new absyn::BreakExp(scanner_.GetTokPos());}; actuals : nonemptyactuals {$$ = $1;} | {$$ = new absyn::ExpList();};nonemptyactuals : exp {$$ = new absyn::ExpList($1);} | exp COMMA nonemptyactuals {$$ = $3; $$-&gt;Prepend($1);};sequencing : exp sequencing_exps {$$ = $2; $$-&gt;Prepend($1); };sequencing_exps: SEMICOLON exp sequencing_exps {$$ = $3; $$-&gt;Prepend($2);} | {$$ = new absyn::ExpList();};expseq : {$$ = new absyn::VoidExp(scanner_.GetTokPos());} | sequencing {$$ = new absyn::SeqExp(scanner_.GetTokPos(), $1);}; Part 2 Handle shift-reduce conflict2.1 If-then-else conflict​ I solve the conflict according to blog. 12345%nonassoc THEN%nonassoc ELSE...exp : IF exp THEN exp ELSE exp {$$ = new absyn::IfExp(scanner_.GetTokPos(), $2, $4, $6);} | IF exp THEN exp {$$ = new absyn::IfExp(scanner_.GetTokPos(), $2, $4, nullptr);} 2.2 ID[exp] conflict​ I have spent a lot time on debugging this conflict. To be specific, the problem can be illustrated in the following sheet. 12345ID [exp] ·, OF shiftID [exp] ·, $ reduceID [exp] ·, DOT shiftID [exp] ·, [ shiftID [exp] ·, ASSIGN shift ​ Let’s take an example, 12345var b := intArray [N] of 0a = c[10]c[10].first = 5d[10][5] = 2row[7] := 1 ​ I have tried a lot of method from blog1 and blog2, but it seems neither of both are useful. To enforce every production do reduce first, I configure “special_one” to achieve my goal. 123456789101112131415exp : special_one OF exp {$$ = new absyn::ArrayExp(scanner_.GetTokPos(), ((absyn::SimpleVar *)(((absyn::SubscriptVar *)($1))-&gt;var_))-&gt;sym_, ((absyn::SubscriptVar *)($1))-&gt;subscript_, $3);}lvalue : oneormore DOT ID {$$ = new absyn::FieldVar(scanner_.GetTokPos(), $1, $3);} | oneormore LBRACK exp RBRACK {$$ = new absyn::SubscriptVar(scanner_.GetTokPos(), $1, $3);} | oneormore {$$ = $1;} | special_one DOT ID {$$ = new absyn::FieldVar(scanner_.GetTokPos(), $1, $3);} | special_one LBRACK exp RBRACK {$$ = new absyn::SubscriptVar(scanner_.GetTokPos(), $1, $3);} | special_one {$$ = $1;};special_one : one LBRACK exp RBRACK{$$ = new absyn::SubscriptVar(scanner_.GetTokPos(), $1, $3);};oneormore : one DOT ID {$$ = new absyn::FieldVar(scanner_.GetTokPos(), $1, $3);} | one {$$ = $1;};one : ID {$$ = new absyn::SimpleVar(scanner_.GetTokPos(), $1);}; ​ I configured special_one equals “id[exp]”, enforcing every “id[exp]” to be reduced to special_one.","link":"/blog/2021/11/04/complier-lab3/"},{"title":"complier-lab4","text":"This lab comes from SE3355 *Compilers* lab4, which requires me to implement a type-checking module when scanning the AST. The AST is created by bisonc++ script implemented in lab3. To make the following statement more clear, we can take a see of the structure of the AST node. The venv contains the symbols of variables and functions and the tenv contains the symbols of all types. 123456789101112131415161718192021222324252627282930313233343536373839404142434445class Var {public: int pos_; virtual ~Var() = default; virtual void Print(FILE *out, int d) const = 0; virtual type::Ty *SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount, err::ErrorMsg *errormsg) const = 0;protected: explicit Var(int pos) : pos_(pos) {}};class Exp {public: int pos_; virtual ~Exp() = default; virtual void Print(FILE *out, int d) const = 0; virtual type::Ty *SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount, err::ErrorMsg *errormsg) const = 0;protected: explicit Exp(int pos) : pos_(pos) {}};class Dec {public: int pos_; virtual ~Dec() = default; virtual void Print(FILE *out, int d) const = 0; virtual void SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount, err::ErrorMsg *errormsg) const = 0;protected: explicit Dec(int pos) : pos_(pos) {}};class Ty {public: int pos_; virtual ~Ty() = default; virtual void Print(FILE *out, int d) const = 0; virtual type::Ty *SemAnalyze(env::TEnvPtr tenv, err::ErrorMsg *errormsg) const = 0;protected: explicit Ty(int pos) : pos_(pos) {}}; Part 1 Some Basic Type Checking1.1 some root and leavesWe want to get the type of every variable, expression and declaration, so we can easily do it from the root and leaves. 1234567891011121314151617181920212223void AbsynTree::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, err::ErrorMsg *errormsg) const { root_-&gt;SemAnalyze(venv, tenv, 0, errormsg);}type::Ty *VarExp::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount, err::ErrorMsg *errormsg) const { return var_-&gt;SemAnalyze(venv, tenv, labelcount, errormsg);}type::Ty *NilExp::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount, err::ErrorMsg *errormsg) const { return type::NilTy::Instance();}type::Ty *IntExp::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount, err::ErrorMsg *errormsg) const { return type::IntTy::Instance();}type::Ty *StringExp::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount, err::ErrorMsg *errormsg) const { return type::StringTy::Instance();}type::Ty *VoidExp::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount, err::ErrorMsg *errormsg) const { return type::VoidTy::Instance();} 1.2 VariablesThe type-checking code of three variables is as follows. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566type::Ty *SimpleVar::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv,int labelcount, err::ErrorMsg *errormsg) const { env::EnvEntry *entry = venv-&gt;Look(sym_); if (entry &amp;&amp; (typeid(*entry) == typeid(env::VarEntry))) { env::VarEntry* varEntry = static_cast&lt;env::VarEntry *&gt;(entry); type::Ty *type = varEntry-&gt;ty_; bool readonly = varEntry-&gt;readonly_; return type; } else { errormsg-&gt;Error(pos_, &quot;undefined variable %s&quot;, sym_-&gt;Name().data()); } return type::IntTy::Instance();}type::Ty *FieldVar::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv,int labelcount, err::ErrorMsg *errormsg) const { // for field var first check var need to be a RecordTy, then check the sym_ is in the recordTY type type::Ty * variable_type = var_-&gt;SemAnalyze(venv, tenv, labelcount, errormsg); if (variable_type == nullptr) { errormsg-&gt;Error(pos_, &quot;variable not defined.&quot;); } else if (typeid(*(variable_type-&gt;ActualTy())) != typeid(type::RecordTy)) { errormsg-&gt;Error(pos_, &quot;not a record type&quot;); } else { type::RecordTy * real_type = ((type::RecordTy *) (variable_type)); int matched = 0; for (type::Field* field:real_type-&gt;fields_-&gt;GetList()) { if (field-&gt;name_-&gt;Name() == sym_-&gt;Name()) { matched = 1; } } if (matched == 1) { return real_type; } else { errormsg-&gt;Error(pos_, &quot;field nam doesn't exist&quot;); return real_type; } }}type::Ty *SubscriptVar::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv,int labelcount, err::ErrorMsg *errormsg) const { //check the type of var_ is an array or not //if so, check subscript_ is an int or not, and check the range. type::Ty *var_type = var_-&gt;SemAnalyze(venv, tenv, labelcount, errormsg); //var_type is array of the type that we want to return if (var_type == nullptr) { errormsg-&gt;Error(pos_, &quot;variable not defined.&quot;); } else if (typeid(*(var_type-&gt;ActualTy())) != typeid(type::ArrayTy)) { errormsg-&gt;Error(pos_, &quot;array type required&quot;); } else { type::Ty *subscript_type = subscript_-&gt;SemAnalyze(venv, tenv, labelcount, errormsg); if (typeid(*(subscript_type-&gt;ActualTy())) != typeid(type::IntTy)) { errormsg-&gt;Error(pos_, &quot;subscribe is not a int type.&quot;); } else { return ((type::ArrayTy *) var_type)-&gt;ty_; } } return nullptr;} 1.3 Call expressionThe type-checking code of call expression is as follows. 12345678910111213141516171819202122232425262728293031323334353637383940414243type::Ty *CallExp::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount, err::ErrorMsg *errormsg) const { env::FunEntry *funEntry = static_cast&lt;env::FunEntry *&gt;(venv-&gt;Look(func_)); if (funEntry == nullptr) { errormsg-&gt;Error(pos_, &quot;undefined function &quot; + func_-&gt;Name()); return type::NilTy::Instance(); } std::list&lt;Exp *&gt; args_list = args_-&gt;GetList(); if (funEntry-&gt;formals_ == nullptr) { auto args_it = args_-&gt;GetList().begin(); for (; args_it != args_-&gt;GetList().end(); args_it++) { Exp *current_exp = *args_it; current_exp-&gt;SemAnalyze(venv, tenv, labelcount, errormsg); } } else { auto formal_it = funEntry-&gt;formals_-&gt;GetList().begin(); auto args_it = args_-&gt;GetList().begin(); for (; formal_it != funEntry-&gt;formals_-&gt;GetList().end() &amp;&amp; args_it != args_-&gt;GetList().end(); formal_it++, args_it++) { Exp *current_exp = *args_it; type::Ty *formal_type = *formal_it; if (typeid(*(current_exp-&gt;SemAnalyze(venv, tenv, labelcount, errormsg)-&gt;ActualTy())) != typeid(*formal_type)) { // type not match errormsg-&gt;Error(current_exp-&gt;pos_, &quot;para type mismatch&quot;); } } if (args_it != args_-&gt;GetList().end()) { // number does not match auto last_it = args_-&gt;GetList().end(); last_it--; errormsg-&gt;Error((*last_it)-&gt;pos_, &quot;too many params in function g&quot;); } else if (formal_it != funEntry-&gt;formals_-&gt;GetList().end()) { } } if (funEntry-&gt;result_ != nullptr) { return funEntry-&gt;result_; } else { return type::NilTy::Instance(); }} Part 2 Some Tricky Part2.1 the loop variable shouldn’t be assignedSince the loop variable in a for-loop can’t not be assigned because of the rule of the tiger language, when a for-loop declares a variable, the variable should be marked with “readonly = true”. In such way, when encountering a variable in assign exp, the module can check the variable is read-only or not. 1234567891011121314151617181920212223242526272829303132333435363738type::Ty *ForExp::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount, err::ErrorMsg *errormsg) const { venv-&gt;BeginScope(); type::Ty *lo_type = lo_-&gt;SemAnalyze(venv, tenv, labelcount, errormsg); venv-&gt;Enter(var_, new env::VarEntry(lo_type, true)); type::Ty *hi_type = hi_-&gt;SemAnalyze(venv, tenv, labelcount, errormsg); if (typeid(*(hi_type-&gt;ActualTy())) != typeid(type::IntTy)) { errormsg-&gt;Error(hi_-&gt;pos_, &quot;for exp's range type is not integer&quot;); } body_-&gt;SemAnalyze(venv, tenv, labelcount + 1, errormsg); venv-&gt;EndScope(); return type::NilTy::Instance();}type::Ty *AssignExp::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount, err::ErrorMsg *errormsg) const { type::Ty *var_type = var_-&gt;SemAnalyze(venv, tenv, labelcount, errormsg); type::Ty *exp_type = exp_-&gt;SemAnalyze(venv, tenv, labelcount, errormsg); if (typeid(*(exp_type-&gt;ActualTy())) == typeid(type::NilTy) &amp;&amp; typeid(*(var_type-&gt;ActualTy())) == typeid(type::RecordTy)) { return type::NilTy::Instance(); } else if (var_type &amp;&amp; typeid(*(var_type)-&gt;ActualTy()) == typeid(type::IntTy)) { absyn::SimpleVar *simpleVar = dynamic_cast&lt;SimpleVar *&gt;(var_); if (simpleVar != nullptr) { env::EnvEntry *envEntry = venv-&gt;Look(simpleVar-&gt;sym_); if (envEntry != nullptr) { bool readonly = envEntry-&gt;readonly_; if (readonly) { errormsg-&gt;Error(pos_, &quot;loop variable can't be assigned&quot;); } } } } else if (var_type &amp;&amp; exp_type &amp;&amp; typeid(*(var_type-&gt;ActualTy())) != typeid(*(exp_type-&gt;ActualTy()))) { errormsg-&gt;Error(pos_, &quot; unmatched assign exp&quot;); } return type::NilTy::Instance();} 2.2 The in-loop checking of break expression123456789101112131415161718192021type::Ty *ForExp::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount, err::ErrorMsg *errormsg) const { ... body_-&gt;SemAnalyze(venv, tenv, labelcount + 1, errormsg); ...}type::Ty *WhileExp::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv,int labelcount, err::ErrorMsg *errormsg) const { ... type::Ty *body_type = body_-&gt;SemAnalyze(venv, tenv, labelcount + 1, errormsg); ...}type::Ty *BreakExp::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount, err::ErrorMsg *errormsg) const { if (labelcount == 0) { errormsg-&gt;Error(pos_, &quot;break is not inside any loop&quot;); } return type::NilTy::Instance();} 2.3 handle the nested function declarationTiger supports the adjacent nested function, so we need to define all the function name first in order for all the function to find the reference function entry in the venv. 123456789101112131415161718192021222324252627282930void FunctionDec::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv,int labelcount, err::ErrorMsg *errormsg) const { absyn::FunDec *last_function = nullptr; for (absyn::FunDec *function:functions_-&gt;GetList()) { if (last_function &amp;&amp; function-&gt;name_-&gt;Name() == last_function-&gt;name_-&gt;Name()) { errormsg-&gt;Error(function-&gt;pos_, &quot;two functions have the same name&quot;); } venv-&gt;Enter(function-&gt;name_, new env::FunEntry(nullptr, nullptr)); last_function = function; } for (absyn::FunDec *function:functions_-&gt;GetList()) { type::Ty *result_ty = tenv-&gt;Look(function-&gt;result_); type::TyList *formals = function-&gt;params_-&gt;MakeFormalTyList(tenv, errormsg); venv-&gt;Set(function-&gt;name_, new env::FunEntry(formals, result_ty)); venv-&gt;BeginScope(); auto formal_it = formals-&gt;GetList().begin(); auto param_it = function-&gt;params_-&gt;GetList().begin(); for (; param_it != function-&gt;params_-&gt;GetList().end(); formal_it++, param_it++) { venv-&gt;Enter((*param_it)-&gt;name_, new env::VarEntry(*formal_it)); } type::Ty *ty = function-&gt;body_-&gt;SemAnalyze(venv, tenv, labelcount, errormsg); errormsg-&gt;Error(pos_, &quot;function body over&quot;); if (function-&gt;result_ == nullptr &amp;&amp; ty &amp;&amp; typeid(*(ty-&gt;ActualTy())) != typeid(type::NilTy)) { errormsg-&gt;Error(pos_, &quot;procedure returns value&quot;); } venv-&gt;EndScope(); }} 2.4 illegal cycle in nested type declaration12345678910111213// test.16.tig/* error: mutually recursive types thet do not pass through record or array */let type a=ctype b=atype c=dtype d=ain &quot;&quot;end The upper code is not allowed in tiger language, so we need to check if there is a cycle in type nested declaration. Each time we define a new type, we need to scan the defined type in the venv to make sure there is no cycle. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253void TypeDec::SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount,err::ErrorMsg *errormsg) const { for (NameAndTy* current: types_-&gt;GetList()) { // put all the name into the env first sym::Symbol *symbol = current-&gt;name_; for (NameAndTy* current_2: types_-&gt;GetList()) { if (current_2 == current) break; if (current_2-&gt;name_-&gt;Name() == current-&gt;name_-&gt;Name()) { errormsg-&gt;Error(pos_, &quot;two types have the same name&quot;); } } tenv-&gt;Enter(symbol, new type::NameTy(symbol, nullptr)); } for (NameAndTy* current: types_-&gt;GetList()) { sym::Symbol *symbol = current-&gt;name_; errormsg-&gt;Error(pos_, &quot;define symbol &quot; + symbol-&gt;Name()); absyn::Ty* type = current-&gt;ty_; absyn::NameTy *changed_type1 = dynamic_cast&lt;absyn::NameTy *&gt;(type); absyn::ArrayTy *changed_type2 = dynamic_cast&lt;absyn::ArrayTy *&gt;(type); absyn::RecordTy *changed_type3 = dynamic_cast&lt;absyn::RecordTy *&gt;(type); if (changed_type1 != nullptr) { type::Ty *type1 = changed_type1-&gt;SemAnalyze(tenv, errormsg); tenv-&gt;Set(symbol, type1); try { type::Ty * test_type = tenv-&gt;Look(symbol); type::NameTy * change_one = dynamic_cast&lt;type::NameTy *&gt;(test_type); type::NameTy * origin_one = dynamic_cast&lt;type::NameTy *&gt;(test_type); while (change_one != nullptr) { errormsg-&gt;Error(pos_, &quot;change_one= &quot; + change_one-&gt;sym_-&gt;Name()); change_one = dynamic_cast&lt;type::NameTy *&gt;(change_one-&gt;ty_); if (change_one == origin_one) { errormsg-&gt;Error(pos_, &quot;eeee= &quot; + change_one-&gt;sym_-&gt;Name()); throw(0); } } } catch (...) { errormsg-&gt;Error(pos_, &quot;illegal type cycle&quot;); } } else if (changed_type2 != nullptr) { type::Ty *type2 = changed_type2-&gt;SemAnalyze(tenv, errormsg); tenv-&gt;Set(symbol, type2); } else if (changed_type3 != nullptr) { type::Ty *type3 = changed_type3-&gt;SemAnalyze(tenv, errormsg); tenv-&gt;Set(symbol, type3); } else { exit(0); } } return;}","link":"/blog/2021/11/22/complier-lab4/"},{"title":"ikfast-configuration","text":"​ IKFAST is a very powerful tool when a robot requires inverse kinematic solutions, so it’s necessary for us to config a customized robot ikfast plugin from scratch, which is a basic skill for a robotic engineer, I deem. 1. Configure the ikfast for flexiv robot​ As first search shows, the ikfast is provided by OpenRAVE which is a very complex planning framework. OpenRAVE requires a tricky installing process, and using not supported environment to install it will cause a lot of frustration. Luckily, there is a docker image to help us get rid of such tedious and troublesome work. I fork the repo to better save the docker image. The docker image provides Ubuntu 14.04 with OpenRAVE 0.9.0 and ROS Indigo installed, which can be used to generate the solver code once. You can browse the docker image by the following command. 1docker pull kamicode/personalrobotics:ros-openrave ​ The script in moveit_kinematics which is called auto_create_ikfast_moveit_plugin.sh will automatically build the docker environment and prepare the OpenRAVE. ​ Since we have the robot’s urdf, we can use the following command to create the ikfast plugin. 12export MYROBOT_NAME=&quot;panda&quot; &amp;rosrun moveit_kinematics auto_create_ikfast_moveit_plugin.sh --iktype Transform6D $MYROBOT_NAME.urdf &lt;planning_group_name&gt; &lt;base_link&gt; &lt;eef_link&gt; ​ In my case, the command is 1rosrun moveit_kinematics auto_create_ikfast_moveit_plugin.sh --iktype Transform6D --name flexiv &lt;somewhere&gt;/ws_moveit/src/moveit_resources/flexiv_description/robot.urdf arm base link7 ​ I specify the –name attribute, because if I use the absolute path, the auto-extracted name may not be correct. ​ The result after several minute is as follows. 12345678910111213141516171819202122Running &lt;somewhere&gt;/ws_moveit/src/moveit/moveit_kinematics/ikfast_kinematics_plugin/scripts/create_ikfast_moveit_plugin.py &quot;flexiv&quot; &quot;arm&quot; &quot;flexiv_arm_ikfast_plugin&quot; &quot;base&quot; &quot;link7&quot; &quot;/tmp/ikfast.rn0uJk/.openrave/kinematics.f82de3cf02e1545e44ec052f9f4778ca/ikfast0x10000049.Transform6D.1_2_3_4_5_6_f0.cpp&quot;Creating IKFastKinematicsPlugin with parameters: robot_name: flexiv base_link_name: base eef_link_name: link7 planning_group_name: arm ikfast_plugin_pkg: flexiv_arm_ikfast_plugin ikfast_output_path: /tmp/ikfast.rn0uJk/.openrave/kinematics.f82de3cf02e1545e44ec052f9f4778ca/ikfast0x10000049.Transform6D.1_2_3_4_5_6_f0.cpp search_mode: OPTIMIZE_MAX_JOINT srdf_filename: flexiv.srdf robot_name_in_srdf: flexiv moveit_config_pkg: flexiv_moveit_configFound source code generated by IKFast version 73Failed to find package: flexiv_arm_ikfast_plugin. Will create it in &lt;somewhere&gt;/ws_moveit/src/moveit/moveit_kinematics/flexiv_arm_ikfast_plugin.Created ikfast header file at '&lt;somewhere&gt;/ws_moveit/src/moveit/moveit_kinematics/flexiv_arm_ikfast_plugin/include/ikfast.h'Created ikfast plugin file at '&lt;somewhere&gt;/ws_moveit/src/moveit/moveit_kinematics/flexiv_arm_ikfast_plugin/src/flexiv_arm_ikfast_moveit_plugin.cpp'Created plugin definition at '&lt;somewhere&gt;/ws_moveit/src/moveit/moveit_kinematics/flexiv_arm_ikfast_plugin/flexiv_arm_moveit_ikfast_plugin_description.xml'Created cmake file at '&lt;somewhere&gt;/ws_moveit/src/moveit/moveit_kinematics/flexiv_arm_ikfast_plugin/CMakeLists.txt'Wrote package.xml at '&lt;somewhere&gt;/ws_moveit/src/moveit/moveit_kinematics/flexiv_arm_ikfast_plugin/package.xml'Created update plugin script at '&lt;somewhere&gt;/ws_moveit/src/moveit/moveit_kinematics/flexiv_arm_ikfast_plugin/update_ikfast_plugin.sh'Modified kinematics.yaml at '&lt;somewhere&gt;/ws_moveit/src/flexiv_moveit_config/config/kinematics.yaml' ​ Notice the upper script has modified the kinematics.yaml in flexiv_moveit_config, which means the robot will use the ikfast to calculate ik. 1234arm: kinematics_solver: flexiv_arm/IKFastKinematicsPlugin kinematics_solver_search_resolution: 0.005 kinematics_solver_timeout: 0.005 ​ But if we using the following command to visualize the flexiv robot in RViz, we can find an error which means that the moveit can not load the flexiv-ikfast plugin. 1roslaunch flexiv_moveit_config demo.launch ​ The error is 12[ERROR] [1636036724.761299732]: The kinematics plugin (arm) failed to load. Error: According to the loaded plugin descriptions the class flexiv_arm/IKFastKinematicsPlugin with base class type kinematics::KinematicsBase does not exist. Declared types are cached_ik_kinematics_plugin/CachedKDLKinematicsPlugin cached_ik_kinematics_plugin/CachedSrvKinematicsPlugin kdl_kinematics_plugin/KDLKinematicsPlugin lma_kinematics_plugin/LMAKinematicsPlugin prbt_manipulator/IKFastKinematicsPlugin srv_kinematics_plugin/SrvKinematicsPlugin[ERROR] [1636036724.761371349]: Kinematics solver could not be instantiated for joint group arm. ​ It tells us that MoveIt can not find the flexiv-ikfast plugin. So we goto the /ws_moveit/src/moveit/moveit_kinematics to find out why. ​ There is flexiv_arm_ikfast_plugin indeed in the directory, but it seems not be complied. I try to add some lines to the CMakeList.txt in moveit_kinematics directory but failed in compiling. But I find if I copy the flexiv_arm…._plugin out as a single package to compile. Rviz works without informing “plugin not found”. 2. Validate the output ikfast file works​ Because RViz will pick available ik solver if it can not find configured ikfast plugin, even if the error message has been eliminated, we can not make sure that the RViz planing do use the ikfast. We need to find an another way. ​ An article proposes another way for us. That is, use the ikfastdemo.cpp to do the validation. We can simply put the ikfastdemo.cpp in the same directory with our ikfast cpp, and make ikfastdemo.cpp inlcude it. ​ So we add two lines to ikfastdemo.cpp. 12#define IK_VERSION 73#include &quot;flexiv_arm_ikfast_solver.cpp&quot; ​ And compile it. 1g++ ikfastdemo.cpp -lstdc++ -llapack -o compute -lrt ​ The usage is as follows. 123456789101112131415161718Usage: ./compute fk j0 j1 ... j6 Returns the forward kinematic solution given the joint angles (in radians). ./compute ik t0 t1 t2 qw qi qj qk free0 ... Returns the ik solutions given the transformation of the end effector specified by a 3x1 translation (tX), and a 1x4 quaternion (w + i + j + k). There are 1 free parameters that have to be specified. ./compute ik r00 r01 r02 t0 r10 r11 r12 t1 r20 r21 r22 t2 free0 ... Returns the ik solutions given the transformation of the end effector specified by a 3x3 rotation R (rXX), and a 3x1 translation (tX). There are 1 free parameters that have to be specified. ./compute iktiming For fixed number of iterations, generates random joint angles, then calculates fk, calculates ik, measures average time taken. ./compute iktiming2 For fixed number of iterations, with one set of joint variables, this finds the ik solutions and measures the average time taken. 2.1 Painful debug timeI use the following command to calculate a forward kinematic solution. 12345678910111213141516./compute fk 1.57 1.57 1.57 1.57 1.57 1.57 1.57Found fk solution for end frame: Translation: x: -0.484689 y: 0.475605 z: 0.400080 Rotation 0.001591 0.001596 -0.999997 Matrix: 0.999996 0.002387 0.001595 0.002390 -0.999996 -0.001592 Euler angles: Yaw: -1.572388 (1st: rotation around vertical blue Z-axis in ROS Rviz) Pitch: 1.568409 Roll: 0.001595 Quaternion: -0.500596 0.500199 0.500597 -0.498606 -0.500596 + 0.500199i + 0.500597j - 0.498606k (alternate convention) ​ To visualize the solution in the RViz, I set the flexiv robot at the same configuration, and add a box indicating the calculation of the ikfast forward kinematics. ​ It seems the base joint need to rotate 180 degree to fit the result of ikfast fk, which is very weird. I spend a lot of time debugging on the problem. What remains unknown is that the RViz motion planning plugin still works well though the ikfast is not giving the correct answer. So it may not use ikfast or experience some downgrade mode because the failure of finding a solution by ikfast plugin. ​ By chance, I find that the yaw angle of the first fixed joint is strange. 123456&lt;!-- ============ joint 0 =============== --&gt;&lt;joint name=&quot;joint0&quot; type=&quot;fixed&quot;&gt; &lt;origin rpy=&quot;0 0 -3.1415927&quot; xyz=&quot;0 0 0.0&quot;/&gt; &lt;parent link=&quot;world&quot;/&gt; &lt;child link=&quot;base&quot;/&gt;&lt;/joint&gt; ​ It is exactly 180 degree, which fits our guess. So I change the value of that yaw angle to zero. And everything works. But what cause the difference? Remember we do NOT set the virtual joint when configuring the flexiv in MoveIt Setup Assistant because we find the fixed joint in URDF ? If we did that as the tutorial, it will add following lines into our robot.srdf. 12&lt;!--VIRTUAL JOINT: Purpose: this element defines a virtual joint between a robot link and an external frame of reference (considered fixed with respect to the robot)--&gt;&lt;virtual_joint child_link=&quot;panda_link0&quot; name=&quot;virtual_joint&quot; parent_frame=&quot;world&quot; type=&quot;floating&quot;/&gt; ​ So BEAR IN MIND, IT IS A CONVENTION THAT THE VIRTUAL JOINT SHOULD BE WITH RPY=”0 0 0”!!! 2.2 Result ​ Finally, I find both the joint and the box overlap however I change the joint position, which means the forward kinematics of ikfast calculate the correct result ! 3. Add ikfast into pybullet-planning module​ Let’s first have a glance on the directory structure of the pybullet-planning/pybullet_tools/ikfast ​ We need to add flexiv directory which includes The ik.py defines some basic information of our new robot. 1234567from ..utils import IKFastInfofrom ..ikfast import * # For legacy purposesFLEXIV_URDF = &quot;models/flexiv_description/flexiv.urdf&quot;FLEXIV_INFO = IKFastInfo(module_name='flexiv.ikfast_flexiv', base_link='base', ee_link='link7', free_joints=['joint1']) The setup.py helps us to compile a pybind11 module, which a shared library .so file. The code is mainly modified from the existed ikfast robot directory. 1234567891011121314151617181920#!/usr/bin/env pythonfrom __future__ import print_functionimport sysimport ossys.path.append(os.path.join(os.pardir, os.pardir, os.pardir))from pybullet_tools.ikfast.compile import compile_ikfast# Build C++ extension by running: 'python setup.py'# see: https://docs.python.org/3/extending/building.htmldef main(): # lib name template: 'ikfast_&lt;robot name&gt;' sys.argv[:] = sys.argv[:1] + ['build'] robot_name = 'flexiv' compile_ikfast(module_name='ikfast_{}'.format(robot_name), cpp_filename='ikfast_{}.cpp'.format(robot_name))if __name__ == '__main__': main() ​ In the original ikfast_flexiv.cpp, to enable the pybind11, we have to add a lot of functions and include “Python.h”. Luckily the author of pybullet-planning has written it for us, so we only to add these lines at the end. ​ Then we simply use the command 1234$python setup.py build......ikfast module ikfast_flexiv imported successful ​ We can modify the test codes in pybullet-planning examples to check whether our flexiv robot can use ikfast to calculate now. ​ The result shows we did it ! 4. Drawbacks​ When I used ikfast in real application, I found a serious problem that ikfast demanded a high accuracy for the pos and orn of the end-effector. But in practice, due to the observation error and others, many times we just want ikfast to produce a solution with some tolerance rather than output “NO SOLUTION”. But according to my experience and this question, such problem can only solved by randomly sample some 6D pose within our predefined tolerant field manually. But such way will make a high-speed theoretically solvable inverse kinematics solve to be a sample-based ik-solver again and require some wrapper rather than just use the ikfast.cpp file, which becomes cumbersome again. So after consideration, I deem that a theoretically-solvable ik-solve is of more fancy than more use.","link":"/blog/2021/11/06/ikfast-configuration/"},{"title":"双系统的配置和安装","text":"​ 最近被疫情关在学校里，难以跑到校外的实验室中用台式机，远程桌面的问题就是网络实在是太卡，一些图形化界面操作起来非常不方便。而虚拟机的问题就是，因为虚拟机的显卡也是虚拟化的，用不了Nvidia的显卡驱动。最终权衡再三还是打算把双系统配一下一劳永逸。 ​ 本文章包含以下章节： Ubuntu 18.04 在拯救者y7000p上的双系统的配置 Nvidia Driver的配置 显示器的配置 文件夹共享方案 截屏和录制方案 OMPL机器人库的安装和配置 Ubuntu 18.04 在拯救者y7000p上的双系统的配置 首先从中科大镜像源中下载对应镜像。 准备一个至少7G的U盘，插入电脑上并右键选择“格式化”，等待完成。 在Rufus官网上下载Rufus软件并安装。Rufus 是一个可以帮助格式化和创建可引导USB闪存盘的工具。 如上图所示，我们需要在“引导类型选择”中选择我们刚才下载的ubuntu-18.04.6-desktop-amd64.iso镜像，然后点击开始。 接下来，我们右键电脑，点击管理，打开磁盘管理界面，如下图所示： ​ 我们把E盘剩余空间清理到200G以上，然后右键这里的新加卷(E:)，选择压缩卷，并且设置大小为204800（即，200G），然后等待界面响应。由于我E盘中原本就有很多文件，这个过程基本上耗费了20分钟左右，最终如下图所示： 我们在保持U盘插在电脑上的情况下，重启电脑，并且在重启过程中连续不停地按F12，直到进入BIOS。 我们选择Linpus lite进入U盘安装。 我们选择install Ubuntu，如下图所示： 此时，我们已经可以进入Ubuntu的图形化安装界面了。步骤如下图所示： 此时我们重启，并且拔掉U盘。我们就可以看到启动时，第一行出现了Ubuntu，第三行是Windows Manager，我们可以按需进入我们的系统了。 Nvidia 驱动的配置​ 一开始我选用的方案是根据我显卡的型号去官网下载对应的驱动并安装，但是在使用命令nvidia-smi的时候持续遇到如下报错。 12NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running. [网上的方法](https://blog.csdn.net/hangzuxi8764/article/details/86572093)一点没有用。最终，我还是从`软件和更新`中的`附加驱动`中，一键配置好了我的显卡驱动。 此时nvidia-smi可以正常检测到显卡。 ​ 我们安装显卡驱动以后，在终端输入nvcc --version还是显示如下： 123Command 'nvcc' not found, but can be installed with:sudo apt install nvidia-cuda-toolkit ​ 这是因为我们虽然安装了显卡驱动，但是还没有安装CUDA Toolkit的缘故，我们根据nvidia-smi中显示的显卡驱动版本，在官网上搜到了对应的CUDA Toolkit版本为11.4，我们下载并且开始安装。在安装过程中，由于我们已经安装了显卡驱动，它会提示如下： ​ 我们点击continue继续安装，但是选择“不要安装NVIDIA显卡驱动”即可。这样，我们就在/usr/local/cuda-11.4路径下安装好了CUDA工具包，最后设置一下环境变量即可。 显示器的配置​ 主要参考了这个问答。 ​ 最终我选用的解决方案为： 12sudo service gdm3 restartsudo reboot 文件夹共享方案​ 至少以我目前的需求来说（仅仅同步一些文档），坚果云的共享文件夹可以完全满足我的需求。 截屏和录制方案​ 写Typora文档的时候，最需要的就是截屏和录制GIF的功能，我选用了shutter来截屏以及peek来录制GIF。 OMPL机器人库的安装​ 这部分和双系统就没什么关系了，但是因为踩了第二次坑（约3小时），所以还是姑且写在这里，可能后续会把这部分搬到更加相关的文章里去。 ​ OMPL的代码仓库在这里。官网安装手册在这里。基本上我们按照官网的Ubuntu的步骤做就行了。 ​ 但是，需要注意的是，这个sh文件中默认的cmake的 DPYTHON的路径在-DPYTHON_EXEC=/usr/bin/python3下，这会导致cmake检测不到我们在conda的虚拟环境中安装的一些编译需要的python库，从而报如下错误： 12fails withmake: *** No rule to make target 'update_bindings'. Stop. ​ 具体信息可以参考issue。 ​ 所以，我们要将install-ompl-ubuntu.sh中修改为如下的这一行： 1cmake ../.. -DPYTHON_EXEC=/home/baochen/anaconda3/envs/py38/bin/python${PYTHONV} ​","link":"/blog/2022/03/22/dual_boot/"},{"title":"VoteNet presentation","text":"A class pre of SJTU SE-125 Machine-Learning. This is a improved version. You can retrieve the origin version from bilibili.","link":"/blog/2022/01/17/machine-learning-pre/"},{"title":"mmdection_hammer_segmentation","text":"The first time for me to construct a custom dataset and apply it in the mmdection. The whole pipeline is as follows: I. Collect and prepare the raw dataset. II. Convert the raw dataset into COCO style. III. Add and movify the mask-rcnn configuration in mmdection API to fit our requirement. IV. Train the model by mmdection. V. Evaluate and visualize the test dataset. VI. Modify the result into mask image so that we can insert it smoothly in our pipeline. Because of the fact that the project is still ongoing, the part I and part VI will NOT be described detailedly. 1.The dataset As shown above, it’s a sample from my created dataset which contains a rgb image and corresponding mask. The foreground is a tool(the hammer), and the background contains other unrelated things. At the very beginning when there was no relationship to how to training, it is of top priority to create enough data and split them into train, validate and test dataset. We collected 4535 imgs, 464 imgs and 454 imgs for training, validating and evaluating. 2. The format of COCOAs said by a famous blog, we use COCO to reconstruct our data is not because it is the best format, but it is the most widely used and accepted format. So we need to arrange our dataset in that format. To our relief, there are a lot of tools to help us automatically conduct such procedure, for example, pycococreator. What we need to do is only to set the related INFO, LICENSES, CATEGORIES and corresponding directory. 1234567891011121314151617181920212223242526272829303132333435363738INFO = { &quot;description&quot;: &quot;Hammer Segmentation Dataset&quot;, &quot;url&quot;: &quot;https://github.com/Kami-code&quot;, &quot;version&quot;: &quot;0.1.0&quot;, &quot;year&quot;: 2022, &quot;contributor&quot;: &quot;Kami-code&quot;, &quot;date_created&quot;: datetime.datetime.utcnow().isoformat(' ')}LICENSES = [ { &quot;id&quot;: 1, &quot;name&quot;: &quot;Attribution-NonCommercial-ShareAlike License&quot;, &quot;url&quot;: &quot;http://creativecommons.org/licenses/by-nc-sa/2.0/&quot; }]CATEGORIES = [ { 'id': 1, 'name': 'hammer', 'supercategory': 'none', },]...def main(): ROOT_DIR = 'hammer_' + MODE IMAGE_DIR = os.path.join(ROOT_DIR, &quot;rgb&quot;) ANNOTATION_DIR = os.path.join(ROOT_DIR, &quot;mask&quot;) coco_output = { &quot;info&quot;: INFO, &quot;licenses&quot;: LICENSES, &quot;categories&quot;: CATEGORIES, &quot;images&quot;: [], &quot;annotations&quot;: [] }... The structure is from the blog. You can refer to it to create your custom dataset. What I want to mention is if we have a lot of mask of different objects on a single image(though not in our current settings), the annotation_id should start from 0 and increase for each mask, while the meaning of image_id is easily understood. By the way, it seems no need to make annotation_id the same as “id” in CATEGORIES, so making it start from zero is necessary. After running the script of pycococreator, we get a single json file named “instance_train2022.json”. We can use the visualizer script provided in that repo to visualize to check whether we successfully get a COCO json file. COCO will contain the information which provided in the mask.png file using contour algorithm.(Finally, they will be stored in polygons format.) Since now, we have successfully got our first COCO dataset! 3. MMDectionMMDetection is an open source object detection toolbox based on PyTorch. It is a part of the OpenMMLab project. It provides a abstraction on the PyTorch which enables me train my network without writing any code. Frankly speaking, I will not use it if I have enough time to dig deep in mask-rcnn, cause it makes me tightly rely on the API, which do no good for my career. For now, given limited time, I have to use it. To use it, I mainly refer to the proceduce in this zhihu blog. But it also contains some frustrating bug, which I will explain below. This is the files need to add and modify if we want to add a new dataset to train by mask_rcnn. (marked in green means the newly-added file and marked in brown means the file needed to be modified) Let’s find how exactly these files works. mmdection/configs/mask_rcnn/my_mask_rcnn.py 123456_base_ = [ '../_base_/models/my_mask_rcnn_r50_fpn.py', '../common/my_coco_instance.py','../_base_/schedules/schedule_2x.py' ,'../_base_/default_runtime.py',] The file just links all the files we added. Next, we should define our own dataset structure, since we’ve got the dataset in COCO format. We can replicate the coco.py and change a little to get our “keto_coco.py”. mmdection/mmdet/datasets/coco.py 123456789101112131415161718@DATASETS.register_module()class CocoDataset(CustomDataset): CLASSES = ('person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush') ... mmdection/mmdet/datasets/keto_coco.py 12345@DATASETS.register_module()class KpCocoDataset(CustomDataset): CLASSES = ['hammer',] ... As we can see, what we do is just set the CLASSES variable into a list only containing one class hammer. The reason that we change the tuple to list seems to be a version-related problem that the low-level function will misleadingly parse the wrong type. mmdection/mmdet/datasets/init.py 12345678910__all__ = [ 'CustomDataset', 'XMLDataset', 'CocoDataset', 'DeepFashionDataset', 'VOCDataset', 'CityscapesDataset', 'LVISDataset', 'LVISV05Dataset', 'LVISV1Dataset', 'GroupSampler', 'DistributedGroupSampler', 'DistributedSampler', 'build_dataloader', 'ConcatDataset', 'RepeatDataset', 'ClassBalancedDataset', 'WIDERFaceDataset', 'DATASETS', 'PIPELINES', 'build_dataset', 'replace_ImageToTensor', 'get_loading_pipeline', 'NumClassCheckHook', 'CocoPanopticDataset', 'MultiImageMixDataset', 'KpCocoDataset'] Since we defined our KpCocoDataset, to make it register into the dataset collection, we need to add the structure name into its init.py and recomplie the module. In class_names.py, we should also add a simple function to get the classes of our defined dataset. mmdection/mmdet/core/evaluation/class_name.py 1234...def kp_coco_classes(): return ['hammer']... Also, a change in init.py is necessary. But there is no explicit call of this function, I GUESS the function may not be called or is called by its name in some format. The my_mask_rcnn_r50_fpn.py is replicated from the mask_rcnn_r50_fpn.py. We just change the class_num parameter 80 to 1. In my_coco_instance.py, we defined the path of the three COCO json files, and train and test pipeline. mmdection/configs/common/my_coco_instance.py 123456789101112131415161718192021222324...data = dict( samples_per_gpu=2, workers_per_gpu=2, train=dict( type='RepeatDataset', times=4, dataset=dict( type=dataset_type, ann_file=data_root + 'annotations/instances_train2022.json', img_prefix=data_root + 'train2022/', pipeline=train_pipeline)), val=dict( type=dataset_type, ann_file=data_root + 'annotations/instances_validate2022.json', img_prefix=data_root + 'validate2022/', pipeline=test_pipeline), test=dict( type=dataset_type, ann_file=data_root + 'annotations/instances_test2022.json', img_prefix=data_root + 'test2022/', pipeline=test_pipeline),)... And we will train the data on a single GPU with 2 samples_per_gpu, so we should downsize the learning rate defined in schedule_2x.py 8 times, because it’s default value is assuming the training is on 8 GPUs with 2 samples_per_gpu. 4. Train our modelWe can use the following command line to start training process. Though I have spent much time in debugging in the process. If we correctly config the mentioned files, it will finally works. 1python tools/train.py configs/mask_rcnn/my_mask_rcnn.py When training, we can read the logs to make sure we are on the right track. 12021-10-27 15:16:22,077 - mmdet - INFO - Epoch [1][2400/9070] lr: 2.500e-03, eta: 17:30:32, time: 0.288, data_time: 0.010, memory: 7875, loss_rpn_cls: 0.0216, loss_rpn_bbox: 0.0081, loss_cls: 0.0925, acc: 97.2070, loss_bbox: 0.1120, loss_mask: 0.1732, loss: 0.4074 How are the batches each epoch 9070 calculated? Recall that we set the training set to be repeat dataset 4, which means the total training dataset is 4535 * 4 = 18140. And we configure the training process on a single with samples_per_gpu = 2. So we get batch_size = 18140 / 2 = 9070. 1234567891011121314151617181920212223242021-10-27 22:03:28,363 - mmdet - INFO - Evaluating segm...Loading and preparing results...DONE (t=0.04s)creating index...index created!Running per image evaluation...Evaluate annotation type *segm*DONE (t=0.46s).Accumulating evaluation results...DONE (t=0.05s). Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.367 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=1000 ] = 0.416 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=1000 ] = 0.415 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = -1.000 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.365 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.369 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.373 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=300 ] = 0.373 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=1000 ] = 0.373 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = -1.000 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.369 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.3752021-10-27 22:03:28,939 - mmdet - INFO - Exp name: my_mask_rcnn.py2021-10-27 22:03:28,939 - mmdet - INFO - Epoch(val) [9][464] bbox_mAP: 0.3800, bbox_mAP_50: 0.4210, bbox_mAP_75: 0.4090, bbox_mAP_s: -1.0000, bbox_mAP_m: 0.3880, bbox_mAP_l: 0.3760, bbox_mAP_copypaste: 0.380 0.421 0.409 -1.000 0.388 0.376, segm_mAP: 0.3670, segm_mAP_50: 0.4160, segm_mAP_75: 0.4150, segm_mAP_s: -1.0000, segm_mAP_m: 0.3650, segm_mAP_l: 0.3690, segm_mAP_copypaste: 0.367 0.416 0.415 -1.000 0.365 0.369 Some results are as shown in the sheet. In this article, we don’t detailedly explain what each statistic means. 5. Evaluate and visualize our modelWe can run the following script to visualize our model. 12345678910111213from mmdet.apis import init_detector, inference_detector, show_result_pyplotif __name__ == '__main__': config_file = 'mmdetection/configs/mask_rcnn/my_mask_rcnn.py' # url: https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth checkpoint_file = 'mmdetection/work_dirs/my_mask_rcnn/latest.pth' device = 'cpu' # init a detector model = init_detector(config_file, checkpoint_file, device=device) # inference the demo image img = '../tool_seg_data/neee/rgb/1906.jpeg' result = inference_detector(model, img) show_result_pyplot(model, img, result, score_thr=0.3) Thank God we finally successfully train the segmentation network.","link":"/blog/2021/10/29/mmdection-hammer-segmentation/"},{"title":"MoveIt Configuration","text":"在配置ROS-noetic MoveIt的时候，不可避免地遇到了一些问题。在此记录一下。 官网配置教程地址： https://ros-planning.github.io/moveit_tutorials/doc/getting_started/getting_started.html Catkin是ROS官方的构建编译系统，是原先的ROS编译构建系统rosbuild的继承者，它组合了cmake宏和python脚本来在基本的cmake工作流之上提供一些额外的功能。Catkin设计得比rosbuild更加便捷，允许更好地分配package，支持更好的交叉编译以及更好的便捷性。Catkin的工作流和cmake的工作流很像，但是还添加了自动find package的基础结构，并且在同时构建多个相互依赖的项目。 catkin编译的工作流程如下： 首先在工作空间catkin_ws/src/下递归的查找其中每一个ROS的package。package中会有package.xml和CMakeLists.txt文件，Catkin(CMake)编译系统依据CMakeLists.txt文件,从而生成makefiles(放在catkin_ws/build/)。 然后make刚刚生成的makefiles等文件，编译链接生成可执行文件(放在catkin_ws/devel)也就是说，Catkin就是将cmake与make指令做了一个封装从而完成整个编译过程的工具。catkin有比较突出的优点，主要是： 1.操作更加简单 2.一次配置，多次使用 3.跨依赖项目编译 使用catkin_make进行编译 123$ cd ~/catkin_ws #回到工作空间,catkin_make必须在工作空间下执行$ catkin_make #开始编译$ source ~/catkin_ws/devel/setup.bash #刷新坏境 编译完成后，如果有新的目标文件产生（原来没有），那么一般紧跟着要source刷新环境，使得系统能够找到刚才编译生成的ROS可执行文件。这个细节比较容易遗漏，致使后面出现可执行文件无法打开等错误。 第一次catkin build的时候，我们遇到如下错误： 根据回答，我们修改原先的catkin_build指令，修改为catkin build -DPYTHON_EXECUTABLE=/usr/bin/python3 -DPYTHON_INCLUDE_DIR=/usr/include/python3.7m。换句话说，ROS的catkin默认的是用python2.7去编译的，哪怕我们安装了python3-empy它依然报错。告诉它使用python3后这个报错就解决了。 第二个问题是tinyxml.h找不到tinystr.h文件。我们找到对应位置后在源码中添加TIXML_USE_STL即可解决这个问题，使用标准的STL库来代替tinystr.h。 第三处build的问题是 它提示我们找不到pyconfig.h，根据这篇博客的提示，我先使用sudo find / -name pyconfig.h，发现在路径/usr/include/python3.8下存在pyconfig.h，原来是解决问题1的时候复制的路径/usr/include/python3.7m根本不存在，把命令改为 12catkin build -DPYTHON_EXECUTABLE=/usr/bin/python3 -DPYTHON_INCLUDE_DIR=/usr/include/python3.8source devel/setup.bash 即可正常编译成功。 接下来我们使用命令 1roslaunch panda_moveit_config demo.launch rviz_tutorial:=true 这里会再次遇到一个问题，根据此回答即可解决。 自此我们可以正常运行并且看到Rviz界面，添加MotionPlanning后就可以在RViz界面中见到我们的panda机器人。 我们首先需要知道4种重叠的可视化： robot在planning environment中的configuration（各个关节的当前角度）。 robot的plan出来的路径 绿色：robot motion planning的开始状态 橙色：robot motion planning的结束状态 这些展示的状态可以在上面的motionplanning的下拉框中勾选。Planning scene在show robot visual-&gt;Scene Robot中；planning path在show robot visual-&gt;Planned Path中；开始状态在Query Start State-&gt;Planning Request中；结束状态的设置在Query Start State-&gt;Planning Request。 我们可以拖动各个关节的orientation来设置开始状态和结束状态。然后点击plan可以查看整个的运动轨迹。我们勾选use collision-aware IK就可以使得求解器尝试找到一个collision-free的轨迹；如果没有勾选的话，求解器会允许在过程中碰到collision。不过在可视化的时候，存在碰撞的link无论我们有没有勾选use collision-aware IK，都会被标识成红色。 在MotionPlanning的joint窗口下，我们可以拖动nullspace exploration，可以看到在末端执行器的位姿不变的情况下，其他机械臂的configuration是可以变化的（满足连续关系）。 现在我们可以开始使用panda机器人在moveit rviz插件中进行运动规划。步骤如下： 把start state设置到期望的位置 把goal state设置到期望的位置 确保这两个state都没有自己和自己碰撞 确保planned path是可见的 按下plan键。 我们在Planning窗口中可以选择不同的start和goal states，eg：当前的状态，之前的状态（之前的planning attempt的start state），一个随机采样的configuration，一个有名字的在srdf中定义的state。 我们也可以查看轨迹路径点，只需要在导航栏中打开panels-&gt;motionplanning-slider，即可看到对应的路径点并拖动。 如果我们勾选use cartesian path的话，robot会尝试直线地移动end effector。 [1]Catkin工作原理 https://blog.csdn.net/qq_33876441/article/details/102958248 [2]Catkin conceptual http://wiki.ros.org/catkin/conceptual_overview [3]https://blog.csdn.net/num8owl/article/details/108689843","link":"/blog/2021/10/20/moveit-configuration/"},{"title":"moveit_planning_scene_tutorial","text":"moveit_planning_scene_tutorial Part1 Add primitive shape into planning scene​ The planning scene is a very useful tool when we want to create a collision-free trajectory for our robot. The first thing is that whatever our target is, the custom mesh object need to be added into the scene for realistic planning. We first introduce the tutorial on the moveit website. 12345678910111213141516171819202122232425262728293031323334353637383940414243......// Now let's define a collision object ROS message for the robot to avoid.moveit_msgs::CollisionObject collision_object;collision_object.header.frame_id = move_group_interface.getPlanningFrame();// The id of the object is used to identify it.collision_object.id = &quot;box1&quot;;// Define a box to add to the world.shape_msgs::SolidPrimitive primitive;primitive.type = primitive.BOX;primitive.dimensions.resize(3);primitive.dimensions[primitive.BOX_X] = 0.1;primitive.dimensions[primitive.BOX_Y] = 1.5;primitive.dimensions[primitive.BOX_Z] = 0.5;// Define a pose for the box (specified relative to frame_id)geometry_msgs::Pose box_pose;box_pose.orientation.w = 1.0;box_pose.position.x = 0.5;box_pose.position.y = 0.0;box_pose.position.z = 0.25;collision_object.primitives.push_back(primitive);collision_object.primitive_poses.push_back(box_pose);collision_object.operation = collision_object.ADD;std::vector&lt;moveit_msgs::CollisionObject&gt; collision_objects;collision_objects.push_back(collision_object);// Now, let's add the collision object into the world// (using a vector that could contain additional objects)ROS_INFO_NAMED(&quot;tutorial&quot;, &quot;Add an object into the world&quot;);planning_scene_interface.addCollisionObjects(collision_objects);// Show text in RViz of status and wait for MoveGroup to receive and process the collision object messagevisual_tools.publishText(text_pose, &quot;Add object&quot;, rvt::WHITE, rvt::XLARGE);visual_tools.trigger();visual_tools.prompt(&quot;Press 'next' in the RvizVisualToolsGui window to once the collision object appears in RViz&quot;);// Now when we plan a trajectory it will avoid the obstaclesuccess = (move_group_interface.plan(my_plan) == moveit::planning_interface::MoveItErrorCode::SUCCESS);...... ​ We also can attach objects to the robot, so that it moves with the robot geometry. This simulates picking up the object for the purpose of manipulating it. The motion planning should avoid collisions between the two objects as well. 123456789101112131415161718192021222324252627282930313233343536373839moveit_msgs::CollisionObject object_to_attach;object_to_attach.id = &quot;cylinder1&quot;;shape_msgs::SolidPrimitive cylinder_primitive;cylinder_primitive.type = primitive.CYLINDER;cylinder_primitive.dimensions.resize(2);cylinder_primitive.dimensions[primitive.CYLINDER_HEIGHT] = 0.20;cylinder_primitive.dimensions[primitive.CYLINDER_RADIUS] = 0.04;// We define the frame/pose for this cylinder so that it appears in the gripperobject_to_attach.header.frame_id = move_group_interface.getEndEffectorLink();geometry_msgs::Pose grab_pose;grab_pose.orientation.w = 1.0;grab_pose.position.z = 0.2;// First, we add the object to the world (without using a vector)object_to_attach.primitives.push_back(cylinder_primitive);object_to_attach.primitive_poses.push_back(grab_pose);object_to_attach.operation = object_to_attach.ADD;planning_scene_interface.applyCollisionObject(object_to_attach);// Then, we &quot;attach&quot; the object to the robot. It uses the frame_id to determine which robot link it is attached to.// You could also use applyAttachedCollisionObject to attach an object to the robot directly.ROS_INFO_NAMED(&quot;tutorial&quot;, &quot;Attach the object to the robot&quot;);move_group_interface.attachObject(object_to_attach.id, &quot;panda_hand&quot;);visual_tools.publishText(text_pose, &quot;Object attached to robot&quot;, rvt::WHITE, rvt::XLARGE);visual_tools.trigger();/* Wait for MoveGroup to receive and process the attached collision object message */visual_tools.prompt(&quot;Press 'next' in the RvizVisualToolsGui window once the new object is attached to the robot&quot;);// Replan, but now with the object in hand.move_group_interface.setStartStateToCurrentState();success = (move_group_interface.plan(my_plan) == moveit::planning_interface::MoveItErrorCode::SUCCESS);ROS_INFO_NAMED(&quot;tutorial&quot;, &quot;Visualizing plan 7 (move around cuboid with cylinder) %s&quot;, success ? &quot;&quot; : &quot;FAILED&quot;);visual_tools.publishTrajectoryLine(my_plan.trajectory_, joint_model_group);visual_tools.trigger();visual_tools.prompt(&quot;Press 'next' in the RvizVisualToolsGui window once the plan is complete&quot;); ​ Now we can create the collision-free trajectory with our attached object. Since, we can not replace everything with basic primitive shape in this tutorial, because such a simplification will cause some difference and mismatch in our digital twin and the real environment setting. Part2 Add custom object into planning scene​ This tutorial demonstrates the way to add the custom object into the planning scene. To be more specific, we need a .stl or .dae model file. This difference between .obj, .stl and .dae is described in blog1 and blog2, from which we can know the interconversion between .obj and .stl file can be easily conducted. 12345678910111213141516171819202122232425262728293031323334//Vector to scale 3D file units (to convert from mm to meters for example)Vector3d vectorScale(0.001, 0.001, 0.001);// Define a collision object ROS message.moveit_msgs::CollisionObject collision_object;// The id of the object is used to identify it.collision_object.id = &quot;custom_object&quot;;//Path where the .dae or .stl object is locatedshapes::Mesh* m = shapes::createMeshFromResource(&quot;&lt;YOUR_OBJECT_PATH&gt;&quot;, vectorScale); ROS_INFO(&quot;Your mesh was loaded&quot;);shape_msgs::Mesh mesh;shapes::ShapeMsg mesh_msg; shapes::constructMsgFromShape(m, mesh_msg);mesh = boost::get&lt;shape_msgs::Mesh&gt;(mesh_msg);//Define a pose for your mesh (specified relative to frame_id)geometry_msgs::Pose obj_pose;obj_pose.position.x = 0.1;obj_pose.position.y = 0.1;obj_pose.position.z = 1.0;// Add the mesh to the Collision object message collision_object.meshes.push_back(mesh);collision_object.mesh_poses.push_back(obj_pose);collision_object.operation = geometry_msgs::Pose::ADD;// Create vector of collision objects to add std::vector&lt;moveit_msgs::CollisionObject&gt; collision_objects;collision_objects.push_back(collision_object);// Add the collision object into the worldmoveit::planning_interface::MoveGroupInterface move_group(&lt;YOUR_PLANNING_GROUP&gt;);planning_scene_interface.addCollisionObjects(collision_objects); ​ Adding static meshes in the Rviz will be omitted because we want the pipeline to be automatic. ​ to be continued","link":"/blog/2021/12/08/moveit-planning-scene-tutorial/"},{"title":"openGL和openCV中的摄像机参数的转化","text":"本文章的主题为研究openGL和openCV中的摄像机参数的转化。 首先要搞清楚，坐标旋转和坐标系旋转的概念。 坐标旋转（点的运动） 如上图，二维坐标系中的绕原点逆时针旋转度，则得到的B点满足： 把上式写成矩阵乘法的形式： 这是点的运动，坐标系（参照系）并没有发生变化 坐标系旋转（基变换） 如上图，把原先的xy基逆时针旋转度到st基，p在xy基下的坐标为，p在st基下的坐标为，求变换关系。 我们有 即 2是坐标系的旋转，点是不动的，得到的是不动的点在变化了的坐标系下的表示 点旋转β相当于坐标系旋转了-β。所以可以直接在1的基础上，把角度反转，就成了坐标系的旋转。 基变换和坐标变换平面解析几何中的直角坐标系有时候需要坐旋转，这实际上是坐标向量绕原点坐旋转，设坐标轴逆时针旋转的角度为，那么不难有，新坐标向量和原坐标向量之间的关系为： OpenCV中的内参矩阵K（计算机视觉——算法和应用P41） 有几种方式来描述上三角矩阵K，一种是 上式使用相互独立的参数来描述x和y维度的焦距和，s项刻画任何可能的传感器轴间的倾斜，这由传感器的安装没有与光轴垂直所引起，而是以像素坐标表达的光心。 在实践中，通过设置和s=0，在很多应用中会使用如下更简单的形式： 通常情况下，通过将原点大致设置在图像的中心，即，其中W和H是图像的高和宽（用像素表示，如600*480），就可以得到仅含一个未知量焦距f的完全可用的摄像机模型。注意上式中的焦距都是以像素为单位表示的，要与现实相机中的毫米焦距等区分开来。 要转化到现实中的距离，我们首先需要知道图像的现实宽度，如，然后我们可以通过公式：，这里的就是视场角，也就是FOV，下图就展示了一个的情况。 为了更好地理解OpenGL中的透视投影，我们先来讨论函数glFrustum。根据OpenGL的官方文档，“glFrustum描述了一个透视矩阵，提供了一个透视投影。”这句话没错，但是只说出了一半实情。事实上，glFrustum做了两件事情，首先它进行透视投影，然后它把结果转换到归一化设备坐标系(NDC)上。前者是投影几何中的常规操作，但是后者是OpenGL中特有的实现细节。 为了讲明白这件事情，我们需要把投影矩阵（Projection Matrix）分成两部分，也就是透视矩阵(Prespective Matrix)和NDC矩阵。 我们的相机内参矩阵可以描述透视投影，所以它是求解出Perspective Matrix的关键。而对于NDC矩阵，我们会使用OpenGL的glOrtho。 第一步：投影变换 我们的3x3内参矩阵K为了能在OpenGL中使用需要两个小的变更，一个是为了正确的裁剪，位置3,3的元素必须为-1，因为OpenGL的摄像机是从原点向z的负半轴看的，所以如果位置3,3的元素为正，摄像机前方的顶点在投影后将具有负的w坐标。原则上，这是可以的，但是由于 OpenGL 执行裁剪的方式，所有这些点都会被裁剪。 所以我们现在有了 对于第二个更改，我们需要保护失去的Z轴的深度信息，所以我们会在内参矩阵的基础上添加一行和一列，即： ，其中 新的第三行保持了Z值的顺序的同时，把-near和-far映射到它们自己（在归一化了w后）。这个结果就是在裁剪平面之间的点依旧在乘上了Perspective Matrix后依旧保持在裁剪平面之间。 第二步：变换到NDC NDC矩阵可以通过glOrtho函数提供。Perspective Matrix把一个视锥空间转化为了一个长方体空间，而glOrtho把长方体空间转化为归一化设备空间。 调用glOrtho需要六个参数left,right,bottom,top,near,far ，其中 调用它的时候，far和near就和前述的一样。而top,bottom,left,right的裁剪平面的选取对应原图像的维度和标定时的坐标规范。 举个例子，如果你的摄像机用WxH的左上角为零点的图像标定了，那么就该使用left = 0, right = W, bottom = H, top = 0，注意到H作为了bottom参数而0作为了top参数，这意味着y轴正半轴是向下的规范。 如果标定时使用的是y轴向上的坐标系，并且原点在图像中心的话，那么就是left = -W/2, right = W/2, bottom = -H/2, top = H/2. 注意到其实glOrtho的参数和透视矩阵有很大的关系，比如说把视景体(viewing volume)向左平移X等价于把主轴向右平移X。而让翻倍就等于让left和right参数减半。很明显，用这两个矩阵来描述这个投影是冗余的，但是分别去考察这两个矩阵允许我们分离相机几何学和图像几何学。 根据文献 https://stackoverflow.com/questions/60430958/understanding-the-view-and-projection-matrix-from-pybullet 在pybullet中， 内参矩阵K为： 和是光心，通常是图像中心。不过有以下不同， \\1. 维度，pybullet保持了第三行和第四列来保持深度信息，这和之前提到的OpenGL相机相同。 \\2. 第四行第三列的元素不是1而是-1 \\3. Pybullet中s=0 \\4. Pybullet中 首先，pybullet使用OpenGL，所以它使用的是列优先的顺序，所以从pybullet中读到的真正的projection matrix应当转置，或者使用numpyarray的order=’F’。 其次，把FOV转化为f的方程如下： 和 因此，pybullet把焦距乘以了2/h，这是因为Pybullet使用归一化设备坐标系（也就是对x除以图像宽度，来归一化到01，再乘以2到02，所以如果我们的光心在图像中间x=1的位置时，那么裁剪平面就归一化到了-1~1）。因此，pybullet的焦距是使用NDC下的正确的焦距长度。 在内参矩阵K中，k和l是mm/px的比例，在使用pybullet时，我们可以认为k=l=1 mm/px，换句话说，在pybullet形式的内参矩阵中，和是以像素为单位的，而整个矩阵的每个元素都是以mm为单位的。 考虑到以上的所有条件，在pybullet中，内参矩阵为： ，其中 把h=1000和FOV=90代入， def computeProjectionMatrixFOV(*args, **kwargs): # real signature unknown “”” Compute a camera projection matrix from fov, aspect ratio, near, far values “”” pass def computeViewMatrix(*args, **kwargs): # real signature unknown “”” Compute a camera viewmatrix from camera eye, target position and up vector “”” pass http://ksimek.github.io/2013/06/03/calibrated_cameras_in_opengl/ https://amytabb.com/ts/2019_06_28/#conversion-corner-1 http://www.info.hiroshima-cu.ac.jp/~miyazaki/knowledge/teche0092.html https://zhuanlan.zhihu.com/p/339199471","link":"/blog/2021/10/15/opencv-opengl-camera-conversion/"},{"title":"pybind11初探","text":"最近项目过程中经常遇到底层为C++代码的情况，作为一个契机，详尽地研究一下pybind11。 虽然无关紧要，但是很在意的问题是11这个数字是哪来的，因为pybind11的介绍是 Seamless operability between C++11 and Python，据称其主要内核代码使用了C++11的语言特性，如匿名函数、元组等。 如上图所示，这是符合pybind11编程要求的c++代码。 图 1 https://zhuanlan.zhihu.com/p/92120645 根据pybind11官网教程，可以使用如下代码进行编译： c++ -O3 -Wall -shared -std=c++11 -fPIC $(python3 -m pybind11 –includes) example.cpp -o example$(python3-config –extension-suffix) -Wall 这个编译选项会强制输出所有警告，用于调试。 -o output_filename 确定输出文件的名称为output_filename。同时这个名称不能和源文件同名。如果不给出这个选项，gcc就给出默认的可执行文件a.out。 -fPIC 生成位置无关代码。 -I/home/baochen/anaconda3/include/python3.8 -I/home/baochen/anaconda3/lib/python3.8/site-packages/pybind11/include .cpython-38-x86_64-linux-gnu.so 案例分析：pybullet-planning中的ikfast编译件 我们考察如何将一个新的机械臂添加到pybullet-planning中。 首先考察已有的franka_panda机械臂，文件夹内容如下： setup.py内容如下： 调用的compile文件如下： 基本上就是传入模块名称和cpp文件的路径，然后构建出一个python可以调用的模块。 同目录的ik.py提供了一个向python暴露的接口，其中的IKFastInfo是一个namedtuple，其实就是指定了我们编译出来的模块名字(setup.py中的robot_name和compile_ikfast中的module_name)","link":"/blog/2021/10/15/pybind11/"},{"title":"pybullet-planning源码解析","text":"我们尝试来阅读一下pybullet-planning中和ik、planning相关的源码。 我们尝试从test_franka.py入手 首先是start_pose是起始的关于tool_link的6D位姿，也就是得到pos, orn 同理，end_pose是对于start_pose施加了一个相对位移z=-distance(1m)以后的位置 我们可以发现，每一次移动的位置确实是沿着z轴相对位置后退0.1m(因为我们分了十步去处理后退1m的最终目标) 分析multiply的源码，有比较陌生的poses参数，单个的意思其实就是按照数组形式处理传入的不定长个参数 术语表： solve joints: 需要用来求解ik的关节。 free joints: 需要在ik计算前指定的关节，这些值在运行时是已知的，但是在ik创建的时候是未知。(not known at IK generation time) 以panda_franka为例，传入的ikfast_info为IKFastInfo(module_name=’franla_panda.ikfast_panda_arm’, base_link=’panda_link0’, ee_link=’panda_link8’, free_joints=[‘panda_joint7’])。因为panda是7个自由度的机器人，所以需要指定一个joint为free joint。 以UR5为例，传入的infast_info为IKFastInfo(module_name=’ur5.ikfast_ur5_arm’, base_link=’base_link’, ee_link=’ee_link’, free_joints=[])。因为UR5是6个自由度的机器人，所以不需要指定free joint。 link_from_name：根据name得到link id 换句话说，对于一个link，施加get_ordered_ancesters以后可以得到包括这个link在内的其祖先link 以panda为例： get_ordered_ancestors(panda, ee_link)：[0, 1, 2, 3, 4, 5, 6, 7] get_ordered_ancesters(panda, tool_link)：[0, 1, 2, 3, 4, 5, 6, 7, 8] first_joint：连接了base_link的joint prune_fixed_joints：去除掉固定的joint以后，需要纳入计算的joint。 ik_joints：[0, 1, 2, 3, 4, 5, 6] free_joints：[6] assert set(free_joints) &lt;= set(ik_joints)：保证需要设置的free_joint包含在ik_joints内。 assert len(ik_joints) == 6 + len(free_joints)：保证剔除掉free_joints后为6个自由度，比如7自由度关节的机械臂需要1个free_joint，6自由度关节的机械臂需要0个free_joint。 difference_fn是一个求差值的函数，对于普通的joint来说，是直接两个参数相减，对于circular joint会有一些特殊的计算方式，但是circular joint在代码中的定义是upper limit &lt; lower limit的情况，并且google也并没有找到什么有用的信息，考虑到我们目前使用的机器人都没有circular joint，就直接当做两个值相减就可以了。 get_length函数就是单纯的求n-范数，范数通过norm参数传进去。 其实closet_inverse_kinematics的后半部分只是在对求出来的解进行排序，找到一个各关节位姿变化的距离最小的解输出。而前半部分是通过generator = ikfast_inverse_kinematics(…..)求解得到了所有的可行解。 在研究ikfast_inverse_kinematics之前，我们先来搞懂interval_generator Np.random.uniform从一个均匀分布[0,1)中随机采样，获得d个数据 Halton sequence:Halton序列是一种为数值方法（如蒙特卡洛模拟算法）产生顶点的系列生成算法。虽然这些序列是以确定的方法算出来的，但它们的偏差很小。也就是说，在大多数情况下这些序列可以看成是随机的。Halton系列于1960年提出，当时是作为quasi-random 数字序列的一个例子。 所以unit_generator是从均匀分布[0,1)中采样d个数据。而在interval_generator这个函数中，也就是随机采样d个0~1之间的权重，传入convex_conbination中。 所以其实就是在lower和upper之间随机取d组点，如果joint lower limit = joint upper limit，那么自然就直接取相等。 所以，调用interval_generator会返回一个满足条件的joints的各个角度。 接下来，我们再来尝试搞懂ikfast_inverse_kinematics \\1. 首先import_ikfast函数会根据我们传入的ikfast_info去import对应机器人的cpp编译出来的模组，每次需要使用新的机器人时，都要准备好这件事情。 \\2. ik_joints和free_joints相对比较容易理解，上文中有提到过这件事情。 \\3. lower_limits和upper_limits就是joint的上下界，free_deltas不详。 \\4. Islice(generator, max_attempts)，迭代器generator生成max_attempt个proposal后结束 所以generator迭代器将原先的joint pose和随机数采样得到的joint pose可行解传入compute_inverse_kinematics中，会继续被传入到每个机械臂特有的ikfast cpp中，具体作用不详，在ur5的ikfast代码中，似乎是在某些条件下会使用采样数据作为某个joint的值。 在ikfast cpp的源码中，我们可以看到这样的表述Computes all IK solutions given a end effector coordinates and the free joints. pfree is an array specifying the free joints of the chain. 换句话说，我们传入的采样得到的sampled参数实际上是指定了对应free joint的本次求解中的值，因为free joint是需要我们指定的，这无可厚非。 \\5. 在传入对应参数以后，满足条件的解会yield出来，所以其实ikfast_inverse_kinematics生成了一个产生对应configuration下的ik解的迭代器。solutions = list(generator)，也就是把这些解都放进list中，因为实现传入的时候，我们保证了要么max_attempts不为INF，要么max_times不为INF，所以不会出现generator会成为一个无限长的迭代器的情况，如果max_times设大了，可能会影响实时性。 接下来，我们来考察pybullet_inverse_kinematics函数。 具体流程是调用multiple_sub_inverse_kinematics，然后最后可以得到一堆解。 它实现逻辑是这样的： 每次需要计算ik的时候，sample一些在各个joint limit范围内的整个机械臂的姿态，创建对应的subrobot，然后subrobot会以ik target pose为目标，迭代式地求解这个ik，也就是求解一次ik，set到对应位置，再求解ik……，迭代约200次，然后最后check最终的位姿和ik的target位姿的差值是不是小于某个阈值，如果小于则成功。 其中sub_robot其实就是在Pybullet环境中创建一个不考虑碰撞的、不可见的、相同pos的robot，只用来计算ik，在计算完成后，就会把sub_robot删除。 Motion-planning 首先是refine_path这个函数，这个函数比较简单，也就是传入一组waypoint（每个waypoint是joint的一组值），refine_path会在每相邻两个waypoint中插入num_steps个中间点，作为新的path。 get_extend_fn(resolution)这个函数也是返回一个函数是根据resolutions确定在两个waypoint之间需要插值几个waypoint点，resolution默认值为3度，也就是0.05弧度。然后调用之前所提到的get_refine_fn函数。resolution对于不同的joint可以设置不同的joint。 remove_redundant其实就是把位姿path中太近（2范数小于1e-3）的点移除。 模拟一下这个情况，首先把path[0]加入waypoint中，然后考察下一个path[i]。对于对于相邻的两次，考察相邻path[i - 1]-&gt;path[i]和path[i]-&gt;path[i+1]这两个向量，我们规范化后考察其方向，如果方向一致的话就不再加到waypoints中。所以相对于path是path中一些平均相距为resolution的位姿。 interpolate_joint_waypoints，也就是对给定的waypoint，通过不同joint的resolution来插值得到一个均匀resolution的waypoints的路径。并且如果其中插值的路径中检测到了碰撞，直接输出空路径。 其实也是带了碰撞检测的interpolate_joint_waypoint。碰撞检测发生在枚举waypoint的需要插值的节点上。注意和interpolate_joint_waypoints中碰撞检测的对象的区分（一个是需要插值的节点，一个是插值后的新增节点）。 可以看到机器学习分为两个阶段，比如倒数第三段，check_link_pair循环中，就是判断自己的link之间的碰撞，也就是self-collision。在倒数第二段，其实就是check我们的body和obstacles之间的collision。 关于这个函数中提供的碰撞检测，有aabb碰撞检测和pair_wise_collision碰撞检测。Aabb比较容易理解，而pair_wise_collision主要是通过枚举需要判断的两个物体之间的笛卡尔积，分别通过p.getClosestPoints是否等于0来判断。","link":"/blog/2021/10/15/pybullet-planning/"},{"title":"(IROS2021)SOIL","text":"​ 因为灵巧手的操作的domain相对复杂，所以需要引入专家示教的方式要诱导分布。但是通常情况下，在现实世界中，要得到state-action pair是比较困难的，比如从视频中学习。所以这篇文章就训练了一个inverse dynamics model（反向动态模型）来从状态转移中预测对应的动作。 论文 网站 ​ 首先，传统的逆向动力学(Robotic Inverse Dynamics)的概念为：已知某一时刻机器人各关节的位置$q$，关节速度$\\dot{q}$以及关节加速度$\\ddot{q}$，求此时施加在机器人各杆件上的驱动力（力矩）$\\tau$。 ​ 而在模仿学习中，反向动态模型(Inverse Dynamics model)指的是从状态转移$\\{S_t,S_{t+1}\\}$到动作$A_t$的映射。 ​ 论文给了RL和模仿学习的定义，此处再回顾一下： 普通策略梯度算法$$g=\\sum_{(s,a)\\in\\pi}\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)A^{\\pi}(s,a)$$ ​ 其中的$A^{\\pi}(s,a)$是advantage function。 DAPG算法​ 我们可以使用示教的方式来加速收敛。示教以state-action pair的形式输入，可以使用DAPG（demo augmented policy gradient）算法。DAPG把原先的策略梯度上添加了一项辅助的模仿项（auxiliary imitation form），也就是：$$g_{dapg}=g+\\lambda_0\\lambda_1^k\\sum_{(s,a)\\in D}\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)$$​ 其中D是state-action示教，而$\\lambda_0$和$\\lambda_1$是尺度化的超参数来控制模仿学习目标的相对贡献。 Problem​ 我们假设模型可以直接访问到灵巧手的关节角度、对应的力、关节的速度。我们也会提供物体的位置。这允许我们单纯地研究state-only imitation learning，而不去纠结一些state estimation的内容。 注意到我们此时的示教内容只有state序列$\\{s_i\\}_{i=0}^T$，所以论文提出了反向动态模型来对$s_t$和$s_{t+1}$之间的action $a_t$做估计，如下图所示： ​ (a)图就是反向动态模型的结构，也就是从仿真器中收集到$(s_{t}, s_{t+1}, a_{t})$三元组作为GT，训练一个输入两个state，预测$a_t’$的模型。(b)图就是在说，我们有了反向动力学模型后，只需要在示教数据集上做一遍inference就可以得到state-action demonstration，放到DPAG中进行训练。 ​ 所以为了训练反向动态模型，我们需要在仿真器中收集足够多的$(s_{t}, s_{t+1}, a_{t})$来训练，但是如果我们在仿真器中使用初始策略随机采集轨迹的话，由于action空间是非常大的，所以效果会很差。我们需要有一个办法来探索action空间，使其往高目标价值函数方向优化。这就是为什么我们在SOIL算法中每一次都是用当前策略$\\pi_{\\theta}$下收集的轨迹来训练我们的反向动态模型。因为我们认为专家示教是相对于一系列高reward function的action。迭代可以保证策略逐步收敛，也就是我们的$\\pi_{\\theta}$正在逐渐接近我们的示教策略。这样的话，对示教动作的估计会越来越精准。 Algorithm: State-Only Imitation Learning(SOIL) $\\textbf{Input:}$ Inverse model $h$, Policy $\\pi$, Replay buffer $R$, State-Only Demonstration D.$\\textbf{Initialize:}$ Learnable parameters $\\phi$ for $h_{\\phi}$, $\\theta$ for $\\pi_{\\theta}$.$\\textbf{for}$ i = 1,2,…,$N_{iter}$ $\\textbf{do}$$\\ \\ \\ \\ $# 收集轨迹$\\ \\ \\ \\ $$\\tau_i\\equiv \\{s_t,a_t,s_{t+1}, r_t\\}_i\\sim\\pi_{\\theta} $$\\ \\ \\ \\ $# 添加轨迹到Replay Buffer中$\\ \\ \\ \\ $$\\textbf{for}$ j = 1,2,…,$N_{inv}$ $\\textbf{do}$$\\ \\ \\ \\ \\ \\ \\ \\ $# 从Replay Buffer中采样GT，因为这是在仿真器里收集的，所以是有state和action的 tuple。$\\ \\ \\ \\ \\ \\ \\ \\ $$B_j\\equiv \\{s_t,a_t,s_{t+1}\\}_j\\sim R$$\\ \\ \\ \\ \\ \\ \\ \\ $# 更新我们的inverse dynamics模型$\\ \\ \\ \\ \\ \\ \\ \\ $$\\phi\\leftarrow \\text{InvOpt}(B_j;\\phi)$$\\ \\ \\ \\ $$\\textbf{end for}$$\\ \\ \\ \\ $# 使用inverse model来预测纯状态示教中的actions$\\ \\ \\ \\ $$D’\\leftarrow 使用最新的模型h_{\\phi}来预测D中的动作$$\\ \\ \\ \\ $# 计算SOIL策略梯度，来更新我们的策略参数$\\theta$$\\ \\ \\ \\ $$\\theta\\leftarrow \\text{PolicyOpt}(\\tau_i, D’;\\theta)$$\\textbf{end for}$ Inverse Dynamics Model​ 我们使用一个小的MLP网络$h_{\\phi}$来估计$a_t’$，其参数为$\\phi$。我们以自监督的方式来训练这个网络。每一次策略迭代中，对于当前策略$\\pi_{\\theta}$，我们采样一些轨迹放到Replay Buffer中，然后我们在其中均匀采样B个样本来训练这个MLP，即：$$a_t’=h_{\\phi}(s_t,s_{t+1})$$​ 损失函数我们直接就用的是$L_2$范数，因为相对来说s和a都是一些低维度的变量（高维度RL也训不出来），即：$$L_{inv}=||a_t’-a_t||_2$$ SOIL策略梯度DAPG的策略梯度如下：$$g_{dapg}=g+\\lambda_0\\lambda_1^k\\sum_{(s,a)\\in D}\\nabla_{\\theta}\\log\\pi_{\\theta}(a|s)$$在SOIL中，因为我们只有s，a需要靠反向动态模型来估计。所以SOIL的策略梯度如下：$$g_{soil}=g+\\lambda_0\\lambda_1^k\\sum_{(s,a’)\\in D’}\\nabla_{\\theta}\\log\\pi_{\\theta}(a’|s)$$ 实验结果​ 自此，我们整个idea部分已经讲完了。State-Only允许我们从视频中做state estimation，然后直接得到state-action pair，这样我们就不需要对视频中的高维action做估计。 ​ 下图是对四个任务分别使用DAPG（蓝色）、SOIL（红色）和Pure RL（黄色）。注意到DAPG可以认为对于示教的每一步都有正确的$a_t$，所以可以认为是SOIL算法的上界，而Pure RL没有借助任何的示教，可以认为是SOIL算法的下界。 State-Only Baseline的比较​ 本文选用了两个State-Only Baseline和SOIL比较： State Matching with Chamfer Distance Data-driven Density Estimation Inverse Dynamics​ 我们比较了一个误差大的坏的反向动态模型和好的反向动态模型对引导模仿学习的作用，可见对于示教中的action $a_t$估计更准确的模型可以更快地让策略收敛。 示教不匹配的鲁棒性实验​ 我们考虑在object relocation任务下，相同示教、但是实验条件不同的全局。我们考虑不同的动力学、形态和物体的情况。在这种情况下，DAPG使用原先的标注的action可能会起反作用。而我们希望证明在实验条件变化的情况下，我们的State-Only Estimation可以起到更好的作用。控制的变量有如下三个： Dynamics：灵巧手的质量，质量增加以后，就需要施加更大的力，因为动量增加了。 Morphologies：原先的示教是使用灵巧手的所有手指，我们现在考虑只允许使用灵巧手部分手指的情况。 Objects：原先是放置木块，现在我们放置一个完全不同的物体（香蕉）。 实验结果如下图所示： ​ 在上面这三个例子中，我们认为是GT的示教state-action pairs并没有表现得很好，这是因为在不同的场景下，要从相同的$s_t$达到$s_{t+1}$所对应的action实际上是不同的。而SOIL因为反向动态模型是针对于每个场景训练的，所以就很好地解决了这个问题。","link":"/blog/2022/02/14/soil/"},{"title":"(ICRA2020)Unigrasp","text":"​ UniGrasp的出现确实让人耳目一新，感觉sequential地出n指灵巧手的抓点确实是一个很符合逻辑很直观的想法。并且这个工作比起6D-GraspNet和GraspNet-1Billion，有尝试对Gripper的URDF建模，也就是关注了夹爪的动力学模型，而之前的6D-GraspNet虽然在第二步的Grasp Pose Evaluation中，把夹爪的点云接在了原先待抓取点云之后，但是这其实更多的是对夹爪静态几何特征的提取。虽然对URDF建模的方法，我个人的直觉一定不是最优解，比较naive，但是也算提出了一个方向。 ​ 首先，这篇文章主打的就是把gripper的attributes（几何、动力学）和待抓取物体的feature联合作为Input输入，然后通过一个变长stage的方法来输入自定义自由度的机械爪的抓点。 ​ 正如文中所说，最终网络的输出的n个点，必须满足两个条件：force-closure和reachable（ik可解）。其实稍微检索一下就可以知道，在传统机器人领域，灵巧手的抓取判定条件，包括传统ik，都是基本上30年前研究完成的问题。感叹一下机器学习真的就是大一统了，可以把几十年前分析完毕的领域拿过来作为训练集正样本条件来做data-driven。 对Gripper的建模 ​ 基本上就是拿Gripper先训了一个AE，然后拿AE的Encoder部分来提夹爪的特征。注意到，上图是以2-fingered gripper为例的，它需要通过URDF构建出全开、全闭、半开三个点云喂入，得到3个特征向量，然后做max, min, mean-pooling最终得到3个特征向量输入。对于一个n-DOF gripper，它需要对每个自由度都枚举一遍最大和最小，然后再加上所有自由度的中值作为额外的一个点云，也就是$2^N+1$个点云输入，后面的3个pooling保证了所有输出的维度相同。 ​ 我认为这对于纯单个关节驱动的手指还好，如果是多节的那种灵巧手，这相当于尝试用一个神经网络去解IK问题，可能这种取最大值和最小值的输入是不够对整个动力学系统建模的。我看了看Intel做的求解IK的神经网络，其中的数据收集就是对游戏角色做大量的随机Pose的ik采样来创建数据，包括说用RL做的。考虑到这里不仅仅是IK的问题，更多意义上还有必须要和待抓取物体collision-free的因素，所以这个问题还是挺tricky的。 ​ 因为这篇论文的野心很大，希望通过这样的Gripper-Encoding的方式来泛化到所有的Gripper，根据上文中的分析显然是很难以做到的，所以其实我们在最终它的实验中也可以发现，它只做了一个unseen 2-fingered gripper的对照组，没有做多指的情况。包括说它虽然整个框架说支持n指，实际上也只做了2指和3指的，因为收集数据的难度和训练成功率都是会有很明显的影响的。 PSSN 第一阶段​ 原先的点云： $S_0={p_i}_{i=1}^L$, $F_0 : (L, 64)$ ​ 首先我们对$F_0$做1D CNN，得到$(L, 1)$，然后对这$L$维做一个softmax，得到每一个点的合法的概率（probability of each point being valid）。然后按照score从高到低选出$K_1$个点以及对应的特征。 第二阶段​ 此时我们拥有的数据如下 $S_0={p_i}_{i=1}^L$, $F_0 : (L, 64)$ $S_1={p_i}_{i=1}^{K_1}$, $F_1 : (K_1, 64)$ ​ 第二阶段和第一阶段类似，我们仍然需要从$S_0$中选出$K_2$个点，即我们得到了$S_2={p_i}_{i=1}^{K_2}$, $F_2 : (K_2, 64)$。接下来，我们要把$F_1$和$F_2$组合起来，得到最先2个抓点的预测。论文中提到的是做了一个reshape-1d-cnn得到了$F_3 : (K_2, K_1, 64)$，具体流程如下图所示： ​ 然后我们继续用1d-CNN从$F_3$得到$(K_2,K_1, 1)$，作为每个点对的分数，我们从中选出$K_3$个分数最高的点对，记为$S_3$。对于二指夹爪的情况，PSSN到此为止就结束了。 第三阶段​ 在这个阶段中，网络需要继续预测出第三个接触点。网络需要继续从$S_3={p_i,p_j}_{i=1,j=1}^{K_2,K_1}$中提取特征，$S_3$中的每个元素代表着从Stage1和Stage2中选择出的两个点。我们把对应的点对的特征连在一起，得到了$(K_3,128)$，继续做1D-CNN得到$F_4 : (K_3,64)$。我们继续把$F_1 : (K_1,64)$拿过来和$F_4$做reshape-1D-CNN，我们可以得到$F_5 : (K_3, K_1, 64)$，再1D-CNN + softmax得到大小为$(K_3,K_1)$的分数矩阵。其中在$(u, v)$处的值代表着$S_3$的第u个点对和$S_4$中的第v个点组合起来的分数。根据这一点，我们可以从中选出TOP-$K_4$的contact point三元组集合。这就是三指的情况。 ​ 在实际训练中，我们发现训练PSSN来reject大量不合法的点的集合是更容易的。如下的启发式算法把十亿的三元组抓点集合减少到了百万的数量级。首先，PSSN预测点云的法向量，我们把预测出来的法向量的集合记为${\\hat{pn_i}}_{i=1}^L$。然后，我们利用简单的启发式算法来根据点的位置和对应的法向量来reject不合法的点的集合。 对于二指机械手的情况，两个接触点的法向量需要大于120度来满足force-closure 对于三指机械手的情况，三个接触点需要构成一个三角形，并且我们约束三角形的每一边都要大于1cm，以免选出来的点太近了，并且为了让三角形更加常规一点，额外约束三角形的最大角不超过120度，对于每个接触点，它的法向量和两条边的角度小于90度（也就是让法向量朝三角形的外部指）。总而言之，三指的force-closure会满足上述的约束。 Loss​ 对于法向量的预测，Loss如下所示：$$L_{pn}=-\\sum_{i=1}^L(\\hat{pn_i}\\cdot pn_i)$$​ 对于一个物体的点云和夹爪，我们把满足force-closure和夹爪可达的点集标为正例，其余的点集标为负例。因此，每个点集都会有一个二分类标签$y$。对于第n个阶段，网络预测出一系列的点集$K_n$，以及对应的标签$\\hat{y}$标志着点集的valid概率。基于这些预测概率，我们可以得到这些点的集合的一个排好序的列表。我们使用ListNet损失来提升正例所对应的$\\hat{y}$$$L_n=-\\sum_{j=1}^{K_n}y_ilog\\left(\\frac{\\text{exp}(\\hat{y_j})}{\\sum_{j=1}^{K_n}\\text{exp}(\\hat{y_j})}\\right)$$​ 在训练过程中，我们先训练Stage 1，我们只计算Stage 1的Loss，但是梯度也会修改Stage2的参数。在Stage 1训练完之后，我们固定Stage 1的参数来继续训练Stage 2，此时只计算 Stage 2的Loss。这个步骤反复执行，直到训练到 Stage N。 数据收集​ 在仿真环境中，选出了1000个物体模型，并且按照5个大小来scale，最终得到3275个物体。使用12种不同的机械手，9个是二指的，3个是三指的。 ​ 把物体放在水平面上，通过八个角度的摄像头来创建深度图，并且合成一个点云并且降采样到2048个点。给定一个N指的机械手，我们需要创建出N个点所有的可能的contact point set，其需要满足force-closure和reachable。 ​ 在2048个点和三指的情况下，有$C_{2048}^3 \\approx 1.4\\times10^9$种可能性。我们假设二指的摩擦系数为0.5，三指的摩擦系数为0.65，并且使用16面的多边形对摩擦锥近似，并且使用之前提到的启发式算法来拒绝大量的point sets。我们使用FastGrasp来评估点的集合，并且计算Grasp质量的分数$Q_l^-$。 对于二指，$Q_l^-&gt;0$认为满足force-closure。 对于三指，$Q_l^-&gt;0.0001$认为满足force-closure。 ​ 对于满足force-closure的点的集合，我们使用N指机械手的ik来计算其可达性，需要满足以下两个条件： reachable，ik可解。 和物体的点云没有碰撞，collision-free。 ​ 只有force-closure、reachable、collision-free三个条件全部满足，我们才会标注为positive。","link":"/blog/2022/01/29/unigrasp/"},{"title":"(RSS2021)GIGA","text":"​ 第一次看别的组的GraspNet，最吸引我的是这篇文章有开源基于Pybullet的仿真器采集数据的部分，代码算是给了我很大的启发。不过随之而来的可能就是真的要重构我目前的仿真器部分……好在工程量并不算大，并且比起我现有的仿真器实现，性能会有巨大的提升。 仿真器部分​ 为了省去看完论文后又要重新看一遍代码的苦恼，我先从我最关注的仿真器实现部分说起。这篇论文主打的就是3D重建和基于点云几何的Grasp Proposal的联合训练。数据采集部分，其实就是从点云中选取点和对应的法向量，并且均匀从$[0,\\text{gripper_length}]$中采集一个接近距离，并且去实现抓取。不过，比起我类似操作的NaÏve实现，我觉得他们的实现有几个优点： 仅加载夹爪模型，省去了机械臂整体的IK和planning的复杂性。这一点其实6D-GraspNet也是这么做的，只是我限于当时的见解和关注的方向，一直忽略了这一点事实，导致有很长一段时间一直在关注IK和planning去做collision-free的整机避障。 代码轻量级，由于省去了IK和Planning，只需要一个循环即可完成数据的创建。并且可以很容易地做多进程。 提前剪枝，对于Grasp非法的collision情况，会提前通过仿真器的接触点API判断做continue，这样的话可以无需每个仿真都执行到最后。 ​ 这也引起了我的反思，确实应当广泛地从他人的代码中吸取方法论、代码结构、抽象上的经验，相对于把想法落到实处，想法对不对、高不高效可能更加重要。 ​ 我们先来看最主要的这个循环，其实做的事情也很简单，随机加载一个场景，拍很多张深度图恢复点云。然后随机采样Grasp Proposal执行，看看是成功还是失败。 1234567891011121314151617181920212223242526272829303132333435363738for _ in range(grasps_per_worker // GRASPS_PER_SCENE): # how many scene need to create # generate heap object_count = np.random.poisson(OBJECT_COUNT_LAMBDA) + 1 sim.reset(object_count) # generate the table and piled or packed objects sim.save_state() # render synthetic depth images n = MAX_VIEWPOINT_COUNT depth_imgs, extrinsics = render_images(sim, n) depth_imgs_side, extrinsics_side = render_side_images(sim, 1, args.random) # reconstrct point cloud using a subset of the images tsdf = create_tsdf(sim.size, 120, depth_imgs, sim.camera.intrinsic, extrinsics) pc = tsdf.get_cloud() # crop surface and borders from point cloud bounding_box = o3d.geometry.AxisAlignedBoundingBox(sim.lower, sim.upper) pc = pc.crop(bounding_box) # o3d.visualization.draw_geometries([pc]) if pc.is_empty(): print(&quot;Point cloud empty, skipping scene&quot;) continue # store the raw data scene_id = write_sensor_data(args.root, depth_imgs_side, extrinsics_side) if args.save_scene: mesh_pose_list = get_mesh_pose_list_from_world(sim.world, args.object_set) write_point_cloud(args.root, scene_id, mesh_pose_list, name=&quot;mesh_pose_list&quot;) for _ in range(GRASPS_PER_SCENE): # sample and evaluate a grasp point point, normal = sample_grasp_point(pc, finger_depth) grasp, label = evaluate_grasp_point(sim, point, normal) # store the sample write_grasp(args.root, scene_id, grasp, label) pbar.update() ​ 这个循环还是简单易懂的，我们接下来主要看看两个关键函数sample_grasp_point和evaluate_grasp_point。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950def sample_grasp_point(point_cloud, finger_depth, eps=0.1): # this function helps to sample grasp proposals from the raw observed point cloud points = np.asarray(point_cloud.points) normals = np.asarray(point_cloud.normals) ok = False while not ok: # TODO this could result in an infinite loop, though very unlikely idx = np.random.randint(len(points)) point, normal = points[idx], normals[idx] ok = normal[2] &gt; -0.1 # make sure the normal is poitning upwards grasp_depth = np.random.uniform(-eps * finger_depth, (1.0 + eps) * finger_depth) point = point + normal * grasp_depth return point, normaldef evaluate_grasp_point(sim, pos, normal, num_rotations=6): # define initial grasp frame on object surface z_axis = -normal x_axis = np.r_[1.0, 0.0, 0.0] if np.isclose(np.abs(np.dot(x_axis, z_axis)), 1.0, 1e-4): x_axis = np.r_[0.0, 1.0, 0.0] y_axis = np.cross(z_axis, x_axis) x_axis = np.cross(y_axis, z_axis) R = Rotation.from_matrix(np.vstack((x_axis, y_axis, z_axis)).T) # 在确定了pos和normal后，其实就只需要采样normal内的一个自由度的旋转角度即可，可以参考GraspNet-1Billion的叙述 # try to grasp with different yaw angles yaws = np.linspace(0.0, np.pi, num_rotations) outcomes, widths = [], [] for yaw in yaws: ori = R * Rotation.from_euler(&quot;z&quot;, yaw) sim.restore_state() candidate = Grasp(Transform(ori, pos), width=sim.gripper.max_opening_width) outcome, width = sim.execute_grasp(candidate, remove=False) outcomes.append(outcome) widths.append(width) # detect mid-point of widest peak of successful yaw angles # TODO currently this does not properly handle periodicity successes = (np.asarray(outcomes) == Label.SUCCESS).astype(float) if np.sum(successes): peaks, properties = signal.find_peaks( x=np.r_[0, successes, 0], height=1, width=1 ) idx_of_widest_peak = peaks[np.argmax(properties[&quot;widths&quot;])] - 1 ori = R * Rotation.from_euler(&quot;z&quot;, yaws[idx_of_widest_peak]) width = widths[idx_of_widest_peak] return Grasp(Transform(ori, pos), width), int(np.max(outcomes)) ​ 上面是一个对Grasp的评估函数，里面最主要的是sim.execute_grasp(candidate, remove=False)。 1234567891011121314151617181920212223242526272829303132333435363738394041424344def execute_grasp(self, grasp, remove=True, allow_contact=False): T_world_grasp = grasp.pose T_grasp_pregrasp = Transform(Rotation.identity(), [0.0, 0.0, -0.05]) T_world_pregrasp = T_world_grasp * T_grasp_pregrasp approach = T_world_grasp.rotation.as_matrix()[:, 2] angle = np.arccos(np.dot(approach, np.r_[0.0, 0.0, -1.0])) if angle &gt; np.pi / 3.0: # side grasp, lift the object after establishing a grasp # 对于Grasp的渐进轴过于倾斜的，抓住以后不再沿着轴运动，而是往上提 T_grasp_pregrasp_world = Transform(Rotation.identity(), [0.0, 0.0, 0.1]) T_world_retreat = T_grasp_pregrasp_world * T_world_grasp else: T_grasp_retreat = Transform(Rotation.identity(), [0.0, 0.0, -0.1]) T_world_retreat = T_world_grasp * T_grasp_retreat self.gripper.reset(T_world_pregrasp) # 如果把夹爪放到预接近位置都有碰撞了，那么就立马结束 if self.gripper.detect_contact(): result = Label.FAILURE, self.gripper.max_opening_width else: self.gripper.move_tcp_xyz(T_world_grasp, abort_on_contact=True) # gripper运动到指定位置 或者 碰到了东西 if self.gripper.detect_contact() and not allow_contact: result = Label.FAILURE, self.gripper.max_opening_width else: self.gripper.move(0.0) # 合上夹爪 self.gripper.move_tcp_xyz(T_world_retreat, abort_on_contact=False) # 夹爪抓着东西回退 if self.check_success(self.gripper): result = Label.SUCCESS, self.gripper.read() if remove: contacts = self.world.get_contacts(self.gripper.body) self.world.remove_body(contacts[0].bodyB) else: result = Label.FAILURE, self.gripper.max_opening_width self.world.remove_body(self.gripper.body) if remove: self.remove_and_wait() return result ​ 我们再来看gripper的封装。注意到命名中的TCP其实是tool center point的意思。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091class Gripper(object): &quot;&quot;&quot;Simulated Panda hand.&quot;&quot;&quot; def __init__(self, world): self.world = world self.urdf_path = Path(&quot;../data/urdfs/panda/hand.urdf&quot;) self.max_opening_width = 0.08 self.finger_depth = 0.05 self.T_body_tcp = Transform(Rotation.identity(), [0.0, 0.0, 0.022]) self.T_tcp_body = self.T_body_tcp.inverse() def reset(self, T_world_tcp): T_world_body = T_world_tcp * self.T_tcp_body self.body = self.world.load_urdf(self.urdf_path, T_world_body) self.body.set_pose(T_world_body) # sets the position of the COM, not URDF link self.constraint = self.world.add_constraint( self.body, None, None, None, pybullet.JOINT_FIXED, [0.0, 0.0, 0.0], Transform.identity(), T_world_body, ) self.update_tcp_constraint(T_world_tcp) # constraint to keep fingers centered self.world.add_constraint( self.body, self.body.links[&quot;panda_leftfinger&quot;], self.body, self.body.links[&quot;panda_rightfinger&quot;], pybullet.JOINT_GEAR, [1.0, 0.0, 0.0], Transform.identity(), Transform.identity(), ).change(gearRatio=-1, erp=0.1, maxForce=50) self.joint1 = self.body.joints[&quot;panda_finger_joint1&quot;] self.joint1.set_position(0.5 * self.max_opening_width, kinematics=True) self.joint2 = self.body.joints[&quot;panda_finger_joint2&quot;] self.joint2.set_position(0.5 * self.max_opening_width, kinematics=True) def update_tcp_constraint(self, T_world_tcp): # 其实就是以300N的力去施加在夹爪上，使它移动到对应的位置 T_world_body = T_world_tcp * self.T_tcp_body self.constraint.change( jointChildPivot=T_world_body.translation, jointChildFrameOrientation=T_world_body.rotation.as_quat(), maxForce=300, ) def set_tcp(self, T_world_tcp): T_word_body = T_world_tcp * self.T_tcp_body self.body.set_pose(T_word_body) self.update_tcp_constraint(T_world_tcp) def move_tcp_xyz(self, target, eef_step=0.002, vel=0.10, abort_on_contact=True): # 一个类似于ik的作用 T_world_body = self.body.get_pose() T_world_tcp = T_world_body * self.T_body_tcp diff = target.translation - T_world_tcp.translation n_steps = int(np.linalg.norm(diff) / eef_step) dist_step = diff / n_steps dur_step = np.linalg.norm(dist_step) / vel for _ in range(n_steps): T_world_tcp.translation += dist_step self.update_tcp_constraint(T_world_tcp) for _ in range(int(dur_step / self.world.dt)): self.world.step() if abort_on_contact and self.detect_contact(): # 检测到了碰撞的情况下，根据设置直接返回或继续 return def detect_contact(self, threshold=5): # 通过pybullet的API做碰撞检测 if self.world.get_contacts(self.body): return True else: return False def move(self, width): # 开合夹爪 self.joint1.set_position(0.5 * width) self.joint2.set_position(0.5 * width) for _ in range(int(0.5 / self.world.dt)): self.world.step() def read(self): width = self.joint1.get_position() + self.joint2.get_position() return width [CVPR2019] Occupancy Networks​ 为了理解GIGA整篇论文在做什么，我先尝试理解它主打的Occupancy Networks。这是3D重建领域的一篇工作。目标是使用3D的决策边界来描述任意精度的3D几何模型。 ​ 现有的表示方式如上图所示，前三者都是离散表示。但是如果使用决策边界这种表示方法的话，那就是连续表示了。我们可以通过后续介绍的采样算法采出任意精度来。 ​ 具体来说，我们的物体模型通过occupancy function $o:R^3\\rightarrow{0,1}$来描述，因为定义域是连续的$R^3$空间，首先不存在分辨率的问题。1就代表这个地方存在物体，而0就代表这个地方没有物体。 ​ 我们希望通过神经网络去拟合这样一个函数，这就是一个二分类的问题。而在3D重建的场景中，我们需要通过物体的观测$\\chi$（图片、点云等）去恢复这个占用函数$R^3\\rightarrow{0, 1}$，我们可以等价地转换为输入一个观测$\\chi$和查询的点$p$，查询这个点的占用概率(0~1)。 ​ 即我们的occupancy network就是用来拟合函数$f_{\\theta}:R^3\\times \\chi\\rightarrow{0,1}$的。而最终训练出来的概率分布，我们可以简单地通过一个阈值$\\tau$来约束它的边界在哪里。这样，最终的模型就是$f_{\\theta} = \\tau$就是物体模型的表面。 ​ 正如博客所说，大道至简，这篇工作的目的就是把2D图片的矢量图这个概念扩展到的3D模型上。用连续的占用函数去刻画一个模型，非常巧妙也非常直观。可以通过这个视频直观地感受一下。 ​ 这个视频最后的部分是训练的VAE在隐变量空间上连续采样得到的效果，非常惊艳。 Training​ 如果理解了这篇论文的独创性，训练其实就没什么好说的了，无非就是对于每个batch，采样然后对occupancy function的预测做交叉熵。$$L_B(\\theta)=\\frac{1}{|B|}\\sum^{|B|}_{i=1}\\sum^K_{j=1}L(f_{\\theta}(p_{ij},x_{i}),o_{ij})$$VAE的Loss类似，加上KL散度即可。$$L_B^{gen}(\\theta,\\psi)=\\frac{1}{|B|}\\sum^{|B|}_{i=1}\\left[\\sum^K_{j=1}L(f_{\\theta}(p_{ij},x_{i}),o_{ij})+KL(q_{\\psi}(z|(p_{ij},o_{ij})_{j=1:K})||p_0(z))\\right]$$ 通过占用函数还原网格 ​ 具体算法不再细究，我认为这是一个类似于碰撞检测中的KD-Tree的变分辨率采样。 [ECCV2020] Convolutional Occupancy Network​ 网站，先放作者讲解视频，有空再补。 ​ 暂略。 Return back to GIGA ​ 这张图就是GIGA的结构，输入的V其实可以理解为voxel representation of point cloud。经过3D卷积得到特征网格，投影其实是为了降维，因为3D CNN的计算存储开销都太大了。降维以后我们得到3个特征平面，在原文中使用的是canonical feature plane，我还以为和什么PCA相关，其实它就是往坐标轴对应的三个平面上投影了。然后在这个特征空间里，我们有待查询的抓点$t$和待查询的空间占用点$p$，我们通过对网格进行双线性插值得到对应的特征$\\psi_t$和$\\psi_p$，然后各自训练了一个网络去预测对应的数据。即$f_{a}(t,\\psi_t)=(q,r,w)$和$f_b(p,\\psi_p)=b\\in[0,1]$。 ​ 因为我们认为这个structured feature girds $C$是包含两个任务（重建模型和抓取创建）的综合信息（因为我们设计了两部分的loss都会回传到这里），所以后续的implicit function其实就是在做对应的回归和分类工作了。至于到底在哪里“implicit”了，最符合条件的其实也只有Occupancy probability的使用。因为说实话，我个人认为，对于Grasp(Affordance Implicit Functions)的分布是否连续，是否需要无限精度的函数去表征，并且这种方法的实用性存疑，只能说这样子做就是一个普通的二分类问题（预测Grasp Quality）、回归问题（预测Grasp的四元数、和夹爪的闭合宽度）。 Loss Functions​ 损失函数的定义还是比较容易的，需要注意的是，因为在structured feature girds $C$这一部分是对我们的待查询点做双线性插值得到的，所以其实这个梯度是可以回传到前面的3D Conv和2D U-Nets的。$$L_A(\\hat{g},g)=L_q(\\hat{q},q)+\\lambda(L_r(\\hat{r},r)+L_w(\\hat{w},w))$$​ 其中$\\hat{q}$就是预测的抓取的成功率，而q就是成功/失败的GT标签。后面的r和w就是对四元数和宽度做回归。注意到$L_q$是交叉熵、$L_w$是2-范数。而$L_r$定义如下：$$L_r(\\hat{r},r)=\\min(L_{quat}(\\hat{r},r),L_{quat}(\\hat{r},r_{\\pi})) \\\\r_{\\pi}=\\pi-r$$​ 因为我们定义夹爪是parallel-jaw gripper，所以可以通过如上的定义使其拥有对称性。$L_G$就是Occupancy Network中的损失，总的网络损失定义如下：$$L=L_A+L_G$$ 总结​ 这个工作把抓点预测和3D重建结合起来，确实比较有意思，而且最终创建出了Grasp的Implicit Function，就等于在空间任意点对应了一个夹爪的分数。是一个无限精度的heatmap，感觉还是不错的。但是ConvONET投影的方式确实让人的逻辑又绕了一层，并且最终训练完以后做inference居然是要在空间内均匀采样grid，然后对每个grid center做前向推导，最后还要做NMS取最高分。总的来说，感觉不太顺畅，不太符合大道至简的原则。","link":"/blog/2022/01/26/GIGA/"},{"title":"(ICCV2019)6D-GraspNet","text":"6D GraspNet是英伟达2019年提出的一篇抓取的论文。 ​ 总体结构如上图所示。 Grasp Sampler​ 传入一个部分观测的物体点云$X$，我们需要能够生成出对应的抓取proposal $G^*$。这是一个生成式模型的场景，也就是我们估计出后验分布$P(G^*|X)$，这样我们才能够根据任意输入$X$来得到$G^*$。论文使用的VAE来对底层隐变量进行建模。此处提供一个复习VAE的博客。 ​ 在VAE中，因为涉及到随机变量的采样，如果按照Original Form采样会导致梯度不能回传。所以，重参数化步骤会使得$z = \\mu + \\sigma\\odot\\varepsilon $，使得梯度可以回传，而随机节点$\\varepsilon \\sim N(0, 1)$不需要更新。 ​ 我们使用VAE来最大化$P(G|X)$的。传入一个点云$X$和隐变量$z$，Decoder部分就是一个确定性的函数来预测出一个grasp。中间的隐变量空间为$P(z)=N(0, I)$。所以，传入一个部分观测点云X以后，我们可以在这个高斯分布上多次采样来得到不同的$z$，那么我们的问题就转化成了最大化$$P(G|X)=\\int P(G|X,z;\\Theta)P(z)dz$$​ 直接积分是不可积的，所以我们需要通过encoder$Q(z|X,g)$把正样本X和g映射到隐变量空间的一个子空间中。Encoder和Decoder都是基于PointNet++的，Encoder使用的方法就是把GT抓取$g$接在点云$X$后面，而Decoder就是把采样出的隐变量$z$接在点云$X$后面。 Grasp Pose Evaluation​ 因为Decoder预测出来的抓取肯定有些能成功而有些会失败。我们需要对每个预测出来的$&lt;X,\\hat{g}&gt;$预测一个成功率$P(S|X,g)$，其实就是一个二分类问题。但是比起直接把16维的6D grasp接在点云后面，本文使用了能够更好利用好点云的特点的encoding方法，也就是把夹爪的点云$X_g$接在原先待抓取物体点云$X$之后，并且再使用一个指示向量来标志哪些是原物体的点云。 ​ 然后这个二分类网络的输入就是原点云和合成的夹爪点云，输出的就是抓取的成功率，使用交叉熵损失来优化此网络。在标签中1代表成功，0代表失败。 ​ 在数据增强部分，我们对于正样本$g\\in G^*$做随机扰动，使得夹爪和物体点云有碰撞或者远离物体点云，得到增强的负样本$G^-$。 Iterative Grasp Pose Refinement​ 现在我们的网络已经可以根据点云预测出一个抓点集合了，那么我们有没有办法去进一步让这些抓取更好呢？我们希望得到一个优化位移$\\Delta g$，使得$P(s=1|g+\\Delta g) &gt; P(s=1|g)$。这样的话我们就可以通过求成功率S对于抓取$g$的偏导数来做梯度上升来优化S，即$\\frac{\\partial S}{\\partial g}$。为了保证刚体变换的约束，我们让夹爪点云$X_g$通过平移向量和欧拉角$R_g=(\\alpha_g,\\beta_g,\\gamma_g)$，根据链式法则，我们有$$\\Delta g=\\frac{\\partial S}{\\partial g}=\\eta\\times\\frac{\\partial S}{\\partial T(g;p)}\\times\\frac{\\partial T(g;p)}{\\partial g}$$​ 这样我们就可以更新了。这一步并不需要什么新的网络，只需要Grasp Evaluator提供梯度就一切好办了。 创建训练集​ 在仿真环境中，我们随机在物体的mesh上采样点，并且把grasp的z轴对齐到点的法向量上。夹爪和物体表面的距离是从[0, gripper_length]上随机采样的，而z轴上的旋转角度也是随机采样得到的。我们对于那些closing volume和物体重叠的grasp来做simulation，抓上来以后会做一个抖动动作，如果物体在抖动以后依旧被抓着，那么我们就认为是一个正样本。 缺点​ From实验室学长：最主要的问题是这套proposing或者sampling后面接evaluation网络的两步法，方法论上落后。两步法慢，而且没法生成dense的grasp pose，一步能搞定的时候为什么要拆成两步呢？ 普通VAE的代码​ 为了仔细理解本论文的代码，我们先从普通的VAE代码开始阅读。 1234567891011121314151617class BaseVAE(nn.Module): def __init__(self) -&gt; None: super(BaseVAE, self).__init__() def encode(self, input: Tensor) -&gt; List[Tensor]: raise NotImplementedError def decode(self, input: Tensor) -&gt; Any: raise NotImplementedError def sample(self, batch_size:int, current_device: int, **kwargs) -&gt; Tensor: raise NotImplementedError def generate(self, x: Tensor, **kwargs) -&gt; Tensor: raise NotImplementedError @abstractmethod def forward(self, *inputs: Tensor) -&gt; Tensor: pass @abstractmethod def loss_function(self, *inputs: Any, **kwargs) -&gt; Tensor: pass ​ 相对于普通的模块，我们需要额外定义encode, decode和sample函数。 ​ 在init中，无非是对称地构造encoder和decoder。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546modules = []hidden_dims = [32, 64, 128, 256, 512]# Build Encoderfor h_dim in hidden_dims: modules.append( nn.Sequential( nn.Conv2d(in_channels, out_channels=h_dim, kernel_size=3, stride=2, padding=1), nn.BatchNorm2d(h_dim), nn.LeakyReLU()) )in_channels = h_dimself.encoder = nn.Sequential(*modules)self.fc_mu = nn.Linear(hidden_dims[-1]*4, latent_dim)self.fc_var = nn.Linear(hidden_dims[-1]*4, latent_dim)# Build Decodermodules = []self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * 4)hidden_dims.reverse()for i in range(len(hidden_dims) - 1): modules.append( nn.Sequential( nn.ConvTranspose2d(hidden_dims[i], hidden_dims[i + 1], kernel_size=3, stride=2, padding=1, output_padding=1), nn.BatchNorm2d(hidden_dims[i + 1]), nn.LeakyReLU()) )self.decoder = nn.Sequential(*modules)self.final_layer = nn.Sequential( nn.ConvTranspose2d(hidden_dims[-1], hidden_dims[-1], kernel_size=3, stride=2, padding=1, output_padding=1), nn.BatchNorm2d(hidden_dims[-1]), nn.LeakyReLU(), nn.Conv2d(hidden_dims[-1], out_channels=3, kernel_size=3, padding=1), nn.Tanh()) ​ Encoder和Decoder的代码如下，因为本仓库是按照 64 * 64 * 3的图像来计算的，所以在应用不同分辨率的时候，需要计算参数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445def encode(self, input: Tensor) -&gt; List[Tensor]: &quot;&quot;&quot; Encodes the input by passing through the encoder network and returns the latent codes. :param input: (Tensor) Input tensor to encoder [B x C x H x W] :return: (Tensor) List of latent codes &quot;&quot;&quot; result = self.encoder(input) # B * C * 64 * 64 # B * 32 * 32 * 32 # B * 64 * 16 * 16 # B * 128 * 8 * 8 # B * 256 * 4 * 4 # B * 512 * 2 * 2 result = torch.flatten(result, start_dim=1) # B * 2048 # Split the result into mu and var components # of the latent Gaussian distribution mu = self.fc_mu(result) # B * latent_dim log_var = self.fc_var(result) # B * latent_dim return [mu, log_var]def decode(self, z: Tensor) -&gt; Tensor: &quot;&quot;&quot; Maps the given latent codes onto the image space. :param z: (Tensor) [B x latent_dim] :return: (Tensor) [B x C x H x W] &quot;&quot;&quot; result = self.decoder_input(z) # B * latent_dim =&gt; B * 2048 result = result.view(-1, 512, 2, 2) # B * 512 * 2 * 2 result = self.decoder(result) # B * 256 * 4 * 4 # B * 128 * 8 * 8 # B * 64 * 16 * 16 # B * 32 * 32 * 32 result = self.final_layer(result) # B * 32 * 64 * 64 # B * 3 * 64 * 64 return result ​ 下面我们可以看到重参数化其实就是做了一个$z = \\mu + \\sigma\\odot\\varepsilon $，前向传播的时候，我们就需要采样一个z出来继续做decode操作。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758def reparameterize(self, mu: Tensor, logvar: Tensor) -&gt; Tensor: &quot;&quot;&quot; Reparameterization trick to sample from N(mu, var) from N(0,1). :param mu: (Tensor) Mean of the latent Gaussian [B x D] :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D] :return: (Tensor) [B x D] &quot;&quot;&quot; std = torch.exp(0.5 * logvar) eps = torch.randn_like(std) return eps * std + mudef forward(self, input: Tensor, **kwargs) -&gt; List[Tensor]: mu, log_var = self.encode(input) z = self.reparameterize(mu, log_var) return [self.decode(z), input, mu, log_var]def loss_function(self, *args, **kwargs) -&gt; dict: recons = args[0] input = args[1] mu = args[2] log_var = args[3] #重建误差 recons_loss =F.mse_loss(recons, input) #计算KL散度 kld_weight = kwargs['M_N'] # Account for the minibatch samples from the dataset kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0) loss = recons_loss + kld_weight * kld_loss return {'loss': loss, 'Reconstruction_Loss':recons_loss.detach(), 'KLD':-kld_loss.detach()}def sample(self, num_samples:int, current_device: int, **kwargs) -&gt; Tensor: &quot;&quot;&quot; Samples from the latent space and return the corresponding image space map. :param num_samples: (Int) Number of samples :param current_device: (Int) Device to run the model :return: (Tensor) &quot;&quot;&quot; z = torch.randn(num_samples, self.latent_dim) z = z.to(current_device) samples = self.decode(z) return samplesdef generate(self, x: Tensor, **kwargs) -&gt; Tensor: &quot;&quot;&quot; Given an input image x, returns the reconstructed image :param x: (Tensor) [B x C x H x W] :return: (Tensor) [B x C x H x W] &quot;&quot;&quot; return self.forward(x)[0] KL散度公式如下：$ KL(N(\\mu, \\sigma), N(0, 1)) = -\\log \\sigma + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}$ 6D-GraspNet代码Grasp Sampler在代码中GraspSamplerVAE和GraspSamplerGAN都继承于GraspSampler。GraspSampler提供了共用的decoder函数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960class GraspSampler(nn.Module): def __init__(self, latent_size, device): super(GraspSampler, self).__init__() self.latent_size = latent_size self.device = device def create_decoder(self, model_scale, pointnet_radius, pointnet_nclusters, num_input_features): # The number of input features for the decoder is 3+latent space where 3 # represents the x, y, z position of the point-cloud self.decoder = base_network(pointnet_radius, pointnet_nclusters, model_scale, num_input_features) self.q = nn.Linear(model_scale * 1024, 4) self.t = nn.Linear(model_scale * 1024, 3) self.confidence = nn.Linear(model_scale * 1024, 1) def decode(self, xyz, z): # 我们输入一个点云和一个采样得到的隐变量z，我们需要预测出抓取的pos(t(x))和orn(q(x))以及这个抓取的置信度。 # 把隐变量接在点云后面作为特征 xyz_features = self.concatenate_z_with_pc(xyz, z).transpose(-1, 1).contiguous() for module in self.decoder[0]: xyz, xyz_features = module(xyz, xyz_features) # 过一轮decoder，也就是先从PointNet++中提取出B * 1024(scale)的特征向量 x = self.decoder[1](xyz_features.squeeze(-1)) predicted_qt = torch.cat( (F.normalize(self.q(x), p=2, dim=-1), self.t(x)), -1) return predicted_qt, torch.sigmoid(self.confidence(x)).squeeze() def concatenate_z_with_pc(self, pc, z): z.unsqueeze_(1) z = z.expand(-1, pc.shape[1], -1) return torch.cat((pc, z), -1) def get_latent_size(self): return self.latent_size def base_network(pointnet_radius, pointnet_nclusters, scale, in_features): sa1_module = pointnet2.PointnetSAModule( npoint=pointnet_nclusters, radius=pointnet_radius, nsample=64, mlp=[in_features, 64 * scale, 64 * scale, 128 * scale]) sa2_module = pointnet2.PointnetSAModule( npoint=32, radius=0.04, nsample=128, mlp=[128 * scale, 128 * scale, 128 * scale, 256 * scale]) sa3_module = pointnet2.PointnetSAModule( mlp=[256 * scale, 256 * scale, 256 * scale, 512 * scale]) sa_modules = nn.ModuleList([sa1_module, sa2_module, sa3_module]) fc_layer = nn.Sequential(nn.Linear(512 * scale, 1024 * scale), nn.BatchNorm1d(1024 * scale), nn.ReLU(True), nn.Linear(1024 * scale, 1024 * scale), nn.BatchNorm1d(1024 * scale), nn.ReLU(True)) return nn.ModuleList([sa_modules, fc_layer]) ​ 如下就是类似地GraspSampleVAE的代码，相对比较容易理解。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768class GraspSamplerVAE(GraspSampler): &quot;&quot;&quot;Network for learning a generative VAE grasp-sampler &quot;&quot;&quot; # omit some functions def create_encoder(self, model_scale, pointnet_radius, pointnet_nclusters): # The number of input features for the encoder is 19: the x, y, z # position of the point-cloud and the flattened 4x4=16 grasp pose matrix # 其实就是创建了一个接收 N * 19, 输出1024的一个PointNet++ Encoder self.encoder = base_network(pointnet_radius, pointnet_nclusters, model_scale, 19) def create_bottleneck(self, input_size, latent_size): # 创建了均值向量和方差向量 mu = nn.Linear(input_size, latent_size) logvar = nn.Linear(input_size, latent_size) self.latent_space = nn.ModuleList([mu, logvar]) def encode(self, xyz, xyz_features): for module in self.encoder[0]: xyz, xyz_features = module(xyz, xyz_features) return self.encoder[1](xyz_features.squeeze(-1)) def forward(self, pc, grasp=None, train=True): if train: return self.forward_train(pc, grasp) else: return self.forward_test(pc, grasp) def forward_train(self, pc, grasp): # 在训练的时候，确实需要通过重参数化对z采样，这样梯度才能回传 input_features = torch.cat( (pc, grasp.unsqueeze(1).expand(-1, pc.shape[1], -1)), -1).transpose(-1, 1).contiguous() z = self.encode(pc, input_features) mu, logvar = self.bottleneck(z) z = self.reparameterize(mu, logvar) qt, confidence = self.decode(pc, z) return qt, confidence, mu, logvar def forward_test(self, pc, grasp): # 在测试的时候，可以直接用均值来代替隐变量z input_features = torch.cat( (pc, grasp.unsqueeze(1).expand(-1, pc.shape[1], -1)), -1).transpose(-1, 1).contiguous() z = self.encode(pc, input_features) mu, _ = self.bottleneck(z) qt, confidence = self.decode(pc, mu) return qt, confidence def sample_latent(self, batch_size): return torch.randn(batch_size, self.latent_size).to(self.device) def generate_grasps(self, pc, z=None): # 这个就是在inference阶段用的了，传入点云和采样到的z，可以直接把对应的grasp给预测出来 if z is None: z = self.sample_latent(pc.shape[0]) qt, confidence = self.decode(pc, z) return qt, confidence, z.squeeze() def generate_dense_latents(self, resolution): &quot;&quot;&quot; For the VAE sampler we consider dense latents to correspond to those between -2 and 2 &quot;&quot;&quot; latents = torch.meshgrid(*[ torch.linspace(-2, 2, resolution) for i in range(self.latent_size) ]) return torch.stack([latents[i].flatten() for i in range(len(latents))], dim=-1).to(self.device) ​ GraspSamplerGAN因为涉及到另一篇文章的优化，此处不再扩展。 GraspNet Evaluator​ 其实这部分就是一个PointNet++提特征的二分类。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061class GraspEvaluator(nn.Module): def __init__(self, model_scale=1, pointnet_radius=0.02, pointnet_nclusters=128, device=&quot;cpu&quot;): super(GraspEvaluator, self).__init__() self.create_evaluator(pointnet_radius, model_scale, pointnet_nclusters) self.device = device def create_evaluator(self, pointnet_radius, model_scale, pointnet_nclusters): # The number of input features for the evaluator is 4: the x, y, z # position of the concatenated gripper and object point-clouds and an # extra binary feature, which is 0 for the object and 1 for the gripper, # to tell these point-clouds apart self.evaluator = base_network(pointnet_radius, pointnet_nclusters, model_scale, 4) self.predictions_logits = nn.Linear(1024 * model_scale, 1) self.confidence = nn.Linear(1024 * model_scale, 1) def evaluate(self, xyz, xyz_features): for module in self.evaluator[0]: xyz, xyz_features = module(xyz, xyz_features) return self.evaluator[1](xyz_features.squeeze(-1)) def forward(self, pc, gripper_pc, train=True): # 把原始点云和夹爪点云融合在一起 pc, pc_features = self.merge_pc_and_gripper_pc(pc, gripper_pc) x = self.evaluate(pc, pc_features.contiguous()) # 过 return self.predictions_logits(x), torch.sigmoid(self.confidence(x)) def merge_pc_and_gripper_pc(self, pc, gripper_pc): &quot;&quot;&quot; Merges the object point cloud and gripper point cloud and adds a binary auxiliary feature that indicates whether each point belongs to the object or to the gripper. &quot;&quot;&quot; pc_shape = pc.shape gripper_shape = gripper_pc.shape assert (len(pc_shape) == 3) assert (len(gripper_shape) == 3) assert (pc_shape[0] == gripper_shape[0]) npoints = pc_shape[1] batch_size = pc_shape[0] l0_xyz = torch.cat((pc, gripper_pc), 1) # 先把两个点云接在一起 labels = [ torch.ones(pc.shape[1], 1, dtype=torch.float32), torch.zeros(gripper_pc.shape[1], 1, dtype=torch.float32) ] labels = torch.cat(labels, 0) labels.unsqueeze_(0) labels = labels.repeat(batch_size, 1, 1) # 把标志着是否是原点云的特征接在点云后 l0_points = torch.cat([l0_xyz, labels.to(self.device)], -1).transpose(-1, 1) return l0_xyz, l0_points Iterative Grasp Pose Refinement​ 这部分代码比较复杂，也非常重要。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192class GraspEstimator: &quot;&quot;&quot; Includes the code used for running the inference. &quot;&quot;&quot; def __init__(self, grasp_sampler_opt, grasp_evaluator_opt, opt): self.grasp_sampler_opt = grasp_sampler_opt self.grasp_evaluator_opt = grasp_evaluator_opt self.opt = opt self.target_pc_size = opt.target_pc_size self.num_refine_steps = opt.refine_steps self.refine_method = opt.refinement_method self.threshold = opt.threshold self.batch_size = opt.batch_size self.generate_dense_grasps = opt.generate_dense_grasps if self.generate_dense_grasps: self.num_grasps_per_dim = opt.num_grasp_samples self.num_grasp_samples = opt.num_grasp_samples * opt.num_grasp_samples else: self.num_grasp_samples = opt.num_grasp_samples self.choose_fn = opt.choose_fn self.choose_fns = { &quot;all&quot;: None, &quot;better_than_threshold&quot;: utils.choose_grasps_better_than_threshold, &quot;better_than_threshold_in_sequence&quot;: utils.choose_grasps_better_than_threshold_in_sequence, } self.device = torch.device(&quot;cuda:0&quot;) self.grasp_evaluator = create_model(grasp_evaluator_opt) self.grasp_sampler = create_model(grasp_sampler_opt) def keep_inliers(self, grasps, confidences, z, pc, inlier_indices_list): for i, inlier_indices in enumerate(inlier_indices_list): grasps[i] = grasps[i][inlier_indices] confidences[i] = confidences[i][inlier_indices] z[i] = z[i][inlier_indices] pc[i] = pc[i][inlier_indices] def generate_and_refine_grasps(self, pc): pc_list, pc_mean = self.prepare_pc(pc) grasps_list, confidence_list, z_list = self.generate_grasps(pc_list) inlier_indices = utils.get_inlier_grasp_indices(grasps_list, torch.zeros(1, 3).to( self.device), threshold=1.0, device=self.device) self.keep_inliers(grasps_list, confidence_list, z_list, pc_list, inlier_indices) improved_eulers, improved_ts, improved_success = [], [], [] for pc, grasps in zip(pc_list, grasps_list): out = self.refine_grasps(pc, grasps, self.refine_method, self.num_refine_steps) improved_eulers.append(out[0]) improved_ts.append(out[1]) improved_success.append(out[2]) improved_eulers = np.hstack(improved_eulers) improved_ts = np.hstack(improved_ts) improved_success = np.hstack(improved_success) if self.choose_fn is &quot;all&quot;: selection_mask = np.ones(improved_success.shape, dtype=np.float32) else: selection_mask = self.choose_fns[self.choose_fn](improved_eulers, improved_ts, improved_success, self.threshold) grasps = utils.rot_and_trans_to_grasps(improved_eulers, improved_ts, selection_mask) utils.denormalize_grasps(grasps, pc_mean) refine_indexes, sample_indexes = np.where(selection_mask) success_prob = improved_success[refine_indexes, sample_indexes].tolist() return grasps, success_prob def prepare_pc(self, pc): if pc.shape[0] &gt; self.target_pc_size: pc = utils.regularize_pc_point_count(pc, self.target_pc_size) pc_mean = np.mean(pc, 0) pc -= np.expand_dims(pc_mean, 0) pc = np.tile(pc, (self.num_grasp_samples, 1, 1)) pc = torch.from_numpy(pc).float().to(self.device) pcs = [] pcs = utils.partition_array_into_subarrays(pc, self.batch_size) return pcs, pc_mean def generate_grasps(self, pcs): all_grasps = [] all_confidence = [] all_z = [] if self.generate_dense_grasps: latent_samples = self.grasp_sampler.net.module.generate_dense_latents( self.num_grasps_per_dim) # 对标准高斯分布采样 latent_samples = utils.partition_array_into_subarrays( latent_samples, self.batch_size) for latent_sample, pc in zip(latent_samples, pcs): grasps, confidence, z = self.grasp_sampler.generate_grasps( pc, latent_sample) all_grasps.append(grasps) all_confidence.append(confidence) all_z.append(z) # 对每个点云获得对应的抓取 else: for pc in pcs: grasps, confidence, z = self.grasp_sampler.generate_grasps(pc) all_grasps.append(grasps) all_confidence.append(confidence) all_z.append(z) return all_grasps, all_confidence, all_z def refine_grasps(self, pc, grasps, refine_method, num_refine_steps=10): grasp_eulers, grasp_translations = utils.convert_qt_to_rt(grasps) if refine_method == &quot;gradient&quot;: improve_fun = self.improve_grasps_gradient_based grasp_eulers = torch.autograd.Variable(grasp_eulers.to( self.device),requires_grad=True) grasp_translations = torch.autograd.Variable(grasp_translations.to( self.device),requires_grad=True) else: improve_fun = self.improve_grasps_sampling_based improved_success = [] improved_eulers = [] improved_ts = [] improved_eulers.append(grasp_eulers.cpu().data.numpy()) improved_ts.append(grasp_translations.cpu().data.numpy()) last_success = None for i in range(num_refine_steps): # 对于每个grasp都通过improve_fun来提高成功率 success_prob, last_success = improve_fun(pc, grasp_eulers, grasp_translations, last_success) improved_success.append(success_prob.cpu().data.numpy()) improved_eulers.append(grasp_eulers.cpu().data.numpy()) improved_ts.append(grasp_translations.cpu().data.numpy()) # we need to run the success on the final improved grasps grasp_pcs = utils.control_points_from_rot_and_trans( grasp_eulers, grasp_translations, self.device) improved_success.append( self.grasp_evaluator.evaluate_grasps( pc, grasp_pcs).squeeze().cpu().data.numpy()) return np.asarray(improved_eulers), np.asarray( improved_ts), np.asarray(improved_success) def improve_grasps_gradient_based(self, pcs, grasp_eulers, grasp_trans, last_success): #euler_angles, translation, eval_and_improve, metadata): grasp_pcs = utils.control_points_from_rot_and_trans( grasp_eulers, grasp_trans, self.device) success = self.grasp_evaluator.evaluate_grasps(pcs, grasp_pcs) success.squeeze().backward( torch.ones(success.shape[0]).to(self.device)) delta_t = grasp_trans.grad norm_t = torch.norm(delta_t, p=2, dim=-1).to(self.device) # Adjust the alpha so that it won't update more than 1 cm. Gradient is only valid # in small neighborhood. alpha = torch.min(0.01 / norm_t, torch.tensor(1.0).to(self.device)) # 这里就直接对grasp的pos和orn做梯度上升 grasp_trans.data += grasp_trans.grad * alpha[:, None] temp = grasp_eulers.clone() grasp_eulers.data += grasp_eulers.grad * alpha[:, None] return success.squeeze(), None def improve_grasps_sampling_based(self, pcs, grasp_eulers, grasp_trans, last_success=None): with torch.no_grad(): if last_success is None: grasp_pcs = utils.control_points_from_rot_and_trans( grasp_eulers, grasp_trans, self.device) last_success = self.grasp_evaluator.evaluate_grasps( pcs, grasp_pcs) delta_t = 2 * (torch.rand(grasp_trans.shape).to(self.device) - 0.5) delta_t *= 0.02 delta_euler_angles = (torch.rand(grasp_eulers.shape).to(self.device) - 0.5) * 2 # 基于采样的算法就要如上进行采样，然后计算出优化以后的grasp perturbed_translation = grasp_trans + delta_t perturbed_euler_angles = grasp_eulers + delta_euler_angles grasp_pcs = utils.control_points_from_rot_and_trans( perturbed_euler_angles, perturbed_translation, self.device) perturbed_success = self.grasp_evaluator.evaluate_grasps(pcs, grasp_pcs) ratio = perturbed_success / torch.max(last_success, torch.tensor(0.0001).to(self.device)) # 丢到Estimator里看看有没有增加成功率 mask = torch.rand(ratio.shape).to(self.device) &lt;= ratio next_success = last_success ind = torch.where(mask)[0] next_success[ind] = perturbed_success[ind] grasp_trans[ind].data = perturbed_translation.data[ind] grasp_eulers[ind].data = perturbed_euler_angles.data[ind] return last_success.squeeze(), next_success","link":"/blog/2022/01/17/6D-GraspNet/"},{"title":"complier-lab5","text":"This lab comes from SE3355 *Compilers* lab5, which requires me to implement a complete runnable tiger compiler. ​ 致谢：Lab5的完成离不开Girafboy学长仓库所提供的参考和思路，为了避免有抄袭之嫌，我将重新以我的思路阐述清楚我的代码。 ​ 首先明确，我们这个Lab5的目标是把做完escape analysis的AST转化为tree language(middle IR)，再转化为assem(lower IR)。正如院长所说，前4个Lab其实是把代码文本转化成了AST，只生成了一个IR，但是我们需要在Lab5一下子完成两个IR的转换和生成，并且我们生成tree language后，其实非常不好Debug，因为有各种各样的嵌套语句，也没有解释器来解析tree language，我们必须等到生成了assem后才能够被汇编解释器所解释执行。 Part1 AST翻译成Tree Language​ 前半部分是根据AST翻译成tree language。这部分相对比较简单，但是一件很麻烦的事情就是translation和Lab4的semantic analysis是非常紧密地intertwine在一起的，如果我们希望在Translate中调用SemAnalyze，会导致大量的代码重构，基本上是一整个Lab4的工作量加上额外的Debug时间，并且还涉及到一个非常trivial，还是很麻烦的点： 12345type::Ty *SemAnalyze(env::VEnvPtr venv, env::TEnvPtr tenv, int labelcount, err::ErrorMsg *errormsg) override;tr::ExpAndTy *Translate(env::VEnvPtr venv, env::TEnvPtr tenv, tr::Level *level, temp::Label *label, err::ErrorMsg *errormsg) override; ​ 我们可以看到，在模板所提供的的接口定义中，Translate函数是没有labelcount参数的，所以它并不能判断某个break语句是否在某些循环语句的里面，我们必须使用SemAnalyze来判定这个错误，所以哪怕我们把语义分析的语句全部迁移到Translate中，还是会有这个问题。为了避免过于丑陋的实现，我在这里纠结了一段时间，不过后来我仔细地查看了Lab5的测试代码（而不是Lab5-part1，Lab5-part1测试脚本没有Semantic analysis阶段，误导了我很久），如下所示： 123456789101112131415161718192021222324...{ // Lab 4: semantic analysis TigerLog(&quot;-------====Semantic analysis=====-----\\n&quot;); sem::ProgSem prog_sem(std::move(absyn_tree), std::move(errormsg)); prog_sem.SemAnalyze(); absyn_tree = prog_sem.TransferAbsynTree(); errormsg = prog_sem.TransferErrormsg();}{ // Lab 5: escape analysis TigerLog(&quot;-------====Escape analysis=====-----\\n&quot;); esc::EscFinder esc_finder(std::move(absyn_tree)); esc_finder.FindEscape(); absyn_tree = esc_finder.TransferAbsynTree();}{ // Lab 5: translate IR tree TigerLog(&quot;-------====Translate=====-----\\n&quot;); tr::ProgTr prog_tr(std::move(absyn_tree), std::move(errormsg)); prog_tr.Translate(); errormsg = prog_tr.TransferErrormsg();}... ​ 所以虽然上课说应该在translation的时候做同时做类型检查的，在Lab中评测中还是没有要求的这么严格。为了避免在Translate阶段又做一遍类型检查，我对AST上的所有节点添加了一个成员type::Ty *type，这样我们做完SemAnalyze后就可以缓存住每个节点的真正类型，然后在Translate阶段使用。这样我们基本上就完成了做Translate的准备动作。 ​ Translate接口的定义如下： 123tr::ExpAndTy *Translate(env::VEnvPtr venv, env::TEnvPtr tenv, tr::Level *level, temp::Label *label, err::ErrorMsg *errormsg) override; ​ 解决掉Translate的关键就是理解level和label参数的含义，理解了自然后面就都好做了。 ​ 我们先来看label，涉及到label修改和使用的只有两处： 12345678910111213141516tr::ExpAndTy *WhileExp::Translate(env::VEnvPtr venv, env::TEnvPtr tenv, tr::Level *level, temp::Label *label, err::ErrorMsg *errormsg) { tr::Exp *exp = nullptr; temp::Label *done_label = temp::LabelFactory::NewLabel(); tr::ExpAndTy *check_test = test_-&gt;Translate(venv, tenv, level, label, errormsg); tr::ExpAndTy *check_body = body_-&gt;Translate(venv, tenv, level, done_label, errormsg); ......}tr::ExpAndTy *BreakExp::Translate(env::VEnvPtr venv, env::TEnvPtr tenv, tr::Level *level, temp::Label *label, err::ErrorMsg *errormsg) { tree::Stm *stm = new tree::JumpStm(new tree::NameExp(label), new std::vector&lt;temp::Label *&gt;({label})); return new tr::ExpAndTy(new tr::NxExp(stm), type);} ​ 注意到for循环最终是规约到while的，所以for循环没有使用到label。这样我们就很清楚了，label就是为了让break语句跳出循环使用的。换句话说，就是循环中done:的位置。这样看来，如果我们希望在translation中一遍同时做类型检查，没有labelcount也无伤大雅，我们只需要判断label是不是等于外面传入的默认值即可。 ​ 接下来是level函数，它主要的意义就是负责static link。换句话说，如果我们在$level=k_1$的函数体中引用到了$level=k_2&lt;k_1$的变量，那么我们需要通过$k_1-k_2$次static link跳转，找到这个变量所在的frame，并且通过其InReg还是InFrame来获取到具体的值。 ​ 所以，level的修改只会出现在FunctionDec的时候： 12345678910tr::Exp *FunctionDec::Translate(env::VEnvPtr venv, env::TEnvPtr tenv, tr::Level *level, temp::Label *label, err::ErrorMsg *errormsg) { for (FunDec *fundec : functions_-&gt;GetList()) { type::TyList *formaltys = make_formal_tylist(tenv, fundec-&gt;params_); tr::Level *new_level = tr::Level::NewLevel(level, fundec-&gt;name_, make_formal_esclist(fundec-&gt;params_)); ...... tr::ExpAndTy *entry = fundec-&gt;body_-&gt;Translate(venv, tenv, funentry-&gt;level_, funentry-&gt;label_, errormsg); }} StaticLink的处理​ static link的核心逻辑如下所示： 123456789tree::Exp *StaticLink(tr::Level *target, tr::Level *level) { tree::Exp *staticlink = new tree::TempExp(reg_manager-&gt;FramePointer()); while(level != target){ frame::Access *sl = level-&gt;frame_-&gt;formals.back(); staticlink = sl-&gt;ToExp(staticlink); level = level-&gt;parent_; } return staticlink;} ​ 当我们在某个函数中尝试使用一个变量的时候，我们需要在venv中找到此变量的定义层数，并且调用StaticLink函数找到对应的层数。 123456789101112131415161718192021222324tr::Exp *TranslateSimpleVar(tr::Access *access, tr::Level *level) { tree::Exp *real_fp = StaticLink(access-&gt;level_, level); return new tr::ExExp(access-&gt;access_-&gt;ToExp(real_fp));}tr::ExpAndTy *CallExp::Translate(env::VEnvPtr venv, env::TEnvPtr tenv, tr::Level *level, temp::Label *label, err::ErrorMsg *errormsg) { ...... auto *list = new tree::ExpList(); for (Exp* args_p:args_-&gt;GetList()) { tr::ExpAndTy *check_arg = args_p-&gt;Translate(venv, tenv, level, label, errormsg); list-&gt;Append(check_arg-&gt;exp_-&gt;UnEx()); } if(!fun_entry-&gt;level_-&gt;parent_) { exp = new tr::ExExp(frame::externalCall(func_-&gt;Name(), list)); } else { list-&gt;Append(StaticLink(fun_entry-&gt;level_-&gt;parent_, level)); exp = new tr::ExExp(new tree::CallExp(new tree::NameExp(func_), list)); } return new tr::ExpAndTy(exp, type);} ​ 注意在CallExp中，如果我们是外部调用的话，不需要添加static link参数，因为外部调用要求不传入static link。 外部调用的处理123tree::Exp *externalCall(std::string s, tree::ExpList *args) { return new tree::CallExp(new tree::NameExp(temp::LabelFactory::NamedLabel(s)), args);} 关系运算的翻译123456789101112131415161718192021222324case EQ_OP:case NEQ_OP:{ tree::CjumpStm *stm; switch (oper_) { case EQ_OP: if (dynamic_cast&lt;type::StringTy *&gt;(leftExpAndTy-&gt;ty_) != nullptr) { auto expList = new tree::ExpList({leftExp, rightExp}); stm = new tree::CjumpStm(tree::RelOp::EQ_OP, frame::externalCall(&quot;string_equal&quot;, expList), new tree::ConstExp(1), nullptr, nullptr); } else stm = new tree::CjumpStm(tree::RelOp::EQ_OP, leftExp, rightExp, nullptr, nullptr); break; case NEQ_OP: stm = new tree::CjumpStm(tree::RelOp::NE_OP, leftExp, rightExp, nullptr, nullptr); break; } auto trues = new temp::Label*[2]; auto falses = new temp::Label*[2]; trues[0] = stm-&gt;true_label_; trues[1] = nullptr; falses[0] = stm-&gt;false_label_; falses[1] = nullptr; exp = new tr::CxExp(trues, falses, stm); break;} ​ 其实就是一个CxExp的构造，我们需要把true_label和false_label的地址传入，以便之后回填。 Part2 Tree Language翻译成Assem基本数据结构12345678910class Frame {public: temp::Label *label; std::list&lt;Access*&gt; formals; int s_offset; Frame() {}; Frame(temp::Label *name, std::list&lt;bool&gt; *escapes) : label(name) {} virtual frame::Access *allocLocal(bool escape) = 0;}; ​ Frame主要是存放函数的名字、寄存器的escape信息、以及栈上的分配offset。 1234567891011121314151617181920212223class X64RegManager : public RegManager {public : X64RegManager() { …… caller_saved_regiters = new temp::TempList({rax, rdi, rsi, rdx, rcx, r8, r9, r10, r11}); callee_saved_registers = new temp::TempList({rbx, rbp, r12, r13, r14, r15}); registers = new temp::TempList({rax, rdi, rsi, rdx, rcx, r8, r9, r10, r11, rbx, rbp, r12, r13, r14, r15, fp, rsp}); args_registers = new temp::TempList({rdi, rsi, rdx, rcx, r8, r9}); ret_sink_registers = new temp::TempList({rsp, rax, rbx, rbp, r12, r13, r14, r15}); allregs_noRSP = new temp::TempList({rax, rdi, rsi, rdx, rcx, r8, r9, r10, r11, rbx, rbp, r12, r13, r14, r15, fp}); } //caller-saved registers temp::Temp *rax, *rdi, *rsi, *rdx, *rcx, *r8, *r9, *r10, *r11; //callee-saved registers temp::Temp *rbx, *rbp, *r12, *r13, *r14, *r15; temp::Temp *fp; temp::Temp *rsp; temp::TempList *caller_saved_regiters, *callee_saved_registers, *registers, *args_registers, *ret_sink_registers, *allregs_noRSP;}; 其他情况​ 其他情况不再赘述，详见之后公开的Github仓库。 ProcEntryExit1​ 主要是用来做view shift的，也就是把传入的寄存器参数存放到函数内来看它的位置。强调我的实现只是完成了功能，并没有性能上的考虑。所以，考虑到多个参数的情况，我先把所有传入的寄存器上的值和6个参数以外的栈上的值全部移到临时寄存器中，再重新根据我们的需要安排到栈上，此处的多余move应当是不必要的。 1234567891011121314151617181920212223tree::Stm *procEntryExit1(frame::Frame *frame, tree::Stm *stm) { int num = 1; tree::Stm *viewshift = new tree::ExpStm(new tree::ConstExp(0)); int total_size = frame-&gt;formals.size(); std::vector&lt;temp::Temp *&gt; tempList; for (frame::Access *formal : frame-&gt;formals) { temp::Temp *reg = temp::TempFactory::NewTemp(); if (num &lt;= 6) { viewshift = new tree::SeqStm(viewshift, new tree::MoveStm(new tree::TempExp(reg), new tree::TempExp(reg_manager-&gt;ARG_nth(num)))); } else { viewshift = new tree::SeqStm(viewshift, new tree::MoveStm(new tree::TempExp(reg), tree::NewMemPlus_Const(new tree::TempExp(reg_manager-&gt;FramePointer()), -1 * (num - 6) * reg_manager-&gt;WordSize()))); } tempList.push_back(reg); num++; } num = 1; for (frame::Access *formal : frame-&gt;formals) { viewshift = new tree::SeqStm(viewshift, new tree::MoveStm(formal-&gt;ToExp(new tree::TempExp(reg_manager-&gt;FramePointer())), new tree::TempExp(tempList[num - 1]))); num++; } return new tree::SeqStm(viewshift, stm);} ProcEntryExit2​ ProcEntryExit2是要在翻译完的body instruction后添加一条空指令，告诉我们函数结束的时候哪些寄存器是要被用到的。 12345assem::InstrList *procEntryExit2(assem::InstrList *body) { temp::TempList *returnSink = reg_manager-&gt;ReturnSink(); body-&gt;Append(new assem::OperInstr(&quot;&quot;, nullptr, returnSink, nullptr)); return body;} ProcEntryExit3​ ProcEntryExit3生成过程入口处理和出口处理的汇编语言代码，主要是对栈指针的更改，以及设置framesize变量。 1234567891011121314151617assem::Proc *procEntryExit3(frame::Frame *frame, assem::InstrList * body) { static char instr[256]; std::string prolog; int size = -frame-&gt;s_offset - 8; sprintf(instr, &quot;.set %s_framesize, %d\\n&quot;, frame-&gt;label-&gt;Name().c_str(), size); prolog = std::string(instr); sprintf(instr, &quot;%s:\\n&quot;, frame-&gt;label-&gt;Name().c_str()); prolog.append(std::string(instr)); sprintf(instr, &quot;subq $%d, %%rsp\\n&quot;, size); prolog.append(std::string(instr)); sprintf(instr, &quot;addq $%d, %%rsp\\n&quot;, size); std::string epilog = std::string(instr); epilog.append(std::string(&quot;retq\\n&quot;)); return new assem::Proc(prolog, body, epilog);} 处理传入大于6个参数的函数调用的情况首先，我们在tigermain中存在一个返回值-1，所以当我们主程序结束后，会根据这个-1跳转到run函数的循环。 123456# interpreter.pydef run(self): pc = self._state_table.get_pc() while pc &gt;= 0: self._instructions[pc].execute(self._state_table) pc = self._state_table.get_pc() ​ 如下图所示，我们目前创造了A的栈帧，并且把传入的参数都根据其是否逃逸放到了对应的寄存器和栈上。 ​ 我们假设此时A希望调用函数B，并且需要传入8个参数+1个static link，由于传参寄存器只有6个，所以后两个参数（记作23和45）和static link都需要放在栈上。 ​ 注意此时，在%rsp-8处我们需要填充一个空的八字节，然后再依次放入23,45和static link。上图为call(B)前的栈上状态。 ​ 在call(B)后，解释器会帮我们填入A的返回地址pc的位置，所以这八个字节是要空出来的，如上图所示。 12345678910111213# state_table.pydef call(self, label): # print(self._reg_table) label = label.replace('@PLT', '') if label in self._label_table: self._func_name_stack.append(self._current_func) self._current_func = label rsp = self.load_reg('%rsp') - 8 self.store_mem(rsp, self._pc) self.store_reg('%rsp', rsp) self._pc = self._label_table[label] self._func_temp_stack.append(self._temp_table.copy()) self._temp_table.clear() ​ 如下图所示，这是我们调用call(B)瞬间，还没有对传入参数做寄存器和栈分配时，栈上的情况。 ​ 接下来，我们继续对寄存器做escape allocation，最终得到一个完整的B运行时栈。 ​ 故代码如下： 12345678910111213141516171819202122232425temp::TempList *ExpList::MunchArgs(assem::InstrList &amp;instr_list, std::string_view fs) { auto tempList = new temp::TempList(); int num = 1; int out_formal = exp_list_.size() - 6; if (out_formal &gt; 0) instr_list.Append(new assem::OperInstr(&quot;subq $&quot; + std::to_string(reg_manager-&gt;WordSize()) + &quot;,%rsp&quot;, nullptr, nullptr, nullptr)); for (tree::Exp *exp : exp_list_) { temp::Temp *arg = exp-&gt;Munch(instr_list, fs); tempList-&gt;Append(arg); if (reg_manager-&gt;ARG_nth(num)) { instr_list.Append(new assem::MoveInstr(&quot;movq `s0, `d0&quot;, new temp::TempList(reg_manager-&gt;ARG_nth(num)), new temp::TempList(arg))); } else { instr_list.Append(new assem::OperInstr(&quot;subq $&quot; + std::to_string(reg_manager-&gt;WordSize()) + &quot;,%rsp&quot;,nullptr, nullptr, nullptr)); instr_list.Append(new assem::MoveInstr(&quot;movq `s0, (`d0)&quot;, new temp::TempList(reg_manager-&gt;StackPointer()), new temp::TempList(arg))); } num++; } if (out_formal &gt; 0) instr_list.Append(new assem::OperInstr(&quot;addq $&quot; + std::to_string(reg_manager-&gt;WordSize() * (out_formal + 1)) + &quot;,%rsp&quot;,nullptr, nullptr, nullptr)); return tempList;}","link":"/blog/2021/12/14/complier-lab5/"},{"title":"(ICRA2021)ScrewNet","text":"ScrewNet论文阅读和源码分析。 核心表述​ 其实ScrewNet的目的很简单，希望从深度图中直接估计出物体的关节模型及其位形信息，而之前的工作都需要引入额外的关节体的纹理信息、或者指定关节体的类型(rigid、revolute、prismatic，helical)。 ​ ScrewNet的核心结构如下： ​ 对于N帧深度图，每一帧都先使用ResNet-18作为backbone来提取2D图像特征，然后提取出来的N个特征向量传入到LSTM层中计算，然后输出N-1 X 8个相对的Screw Parameter。此处提供一个复习LSTM的博客。 Screw Theory​ 这篇文章能中ICRA的点就在于此。“空间中任何一个物体的位移都可以通过绕着一条直线的旋转以及绕着这条线的平行移动解决。”这条线叫做screw axis of displacement $S$。在普吕克坐标系下，直线可以表示成$(\\textbf{l}, \\textbf{m})$的形式，并且满足$||\\textbf{l}||=1$和$\\textbf{l}\\cdot\\textbf{m}=0$这两个约束条件。其实也很容易理解，在欧式坐标下，我们取直线的一段线段，有端点x和y。我们令$$\\textbf{l}=\\text{normalize} (y-x) \\\\textbf{m}=x\\times y$$​ 这就是普吕克坐标下表示直线的方法。 ​ 上图中的$(d,m)$就是我们的$(\\textbf{l}, \\textbf{m})$。 ​ 所以ScrewNet就使用了$(\\textbf{l}, \\textbf{m}, \\theta, d)$来表示在SE(3)中的刚体运动。其中$d$是沿着轴的线性平移，而$\\theta$是绕着轴的旋转，并且满足$d=h\\theta$。 Loss Function​ Screw位移包括了两部分：screw轴$S$，以及对应的位形$q_i$。所以ScrewNet希望同时优化如下的多个目标损失：$$L=\\lambda_1L_{S_{ori}}+\\lambda_2L_{S_{dist}}+\\lambda_3L_{S_{cons}}+\\lambda_4{L_q}$$​ 其中$L_{S_{ori}}$惩罚的是screw轴的偏差，所以通过GT轴和screw轴的角度偏差来计算，而$L_{S_{dist}}$是惩罚的是预测的screw轴和GT轴的空间距离，通过普吕克坐标系下的直线距离来表示。即： $$d((\\textbf{l}_1,\\textbf{m}_1),(\\textbf{l}_2,\\textbf{m}_2))=\\begin{cases}0, &amp; \\text{if $\\textbf{l}_1$ and $\\textbf{l}_2$ intersect} \\\\||\\textbf{l}_1\\times(\\textbf{m}_1-\\textbf{m}_2)||, &amp; \\text{else if $\\textbf{l}_1$ and $\\textbf{l}_2$ are parallel, i.e.$||\\textbf{l}_1\\times \\textbf{l}_2||=0$} \\\\\\frac{|\\textbf{l}_1\\cdot \\textbf{m}_2+\\textbf{l}_2 \\cdot \\textbf{m}_1|}{||\\textbf{l}_1\\times \\textbf{l}_2||}, &amp; else,\\textbf{l}_1\\ and\\ \\textbf{l}_2\\ are\\ skew\\ lines\\end{cases}\\\\L_{S_{dist}}=d((\\textbf{l}_{GT},\\textbf{m}_{GT}),(\\textbf{l}_{pred},\\textbf{m}_{pred}))$$​ 而$L_{S_{cons}}$强迫其满足预测出来的直线满足普吕克约束$(\\textbf{l}\\cdot\\textbf{m}=0)$和$||\\textbf{l}||=1$；$L_q$是位形损失。 ​ 其中$L_q$可以由两部分组成：旋转误差$L_{\\theta}$和平移误差$L_d$，如下计算：$$L_q=\\alpha_1L_{\\theta}+\\alpha_2L_d \\\\L_{\\theta}=I_{3\\times3}-R(\\theta_{GT},\\textbf{l}_{GT})R(\\theta_{pred},\\textbf{l}_{pred})^T \\\\L_d=||d_{GT}\\cdot\\textbf{l}_{GT}-d_{pred}\\cdot\\textbf{l}_{pred}||$$​ 其中的$R(\\theta,\\textbf{l})$就是沿着轴$\\textbf{l}$旋转$\\theta$角度的旋转矩阵$R$。之所以不直接对$q_{GT}$和$q_{pred}$施加$L_2$损失是因为这个损失函数的构成确保了其物理的含义，因为这个损失函数是基于旋转矩阵正交的性质而设计的，所以它可以确保学出来的$\\theta_{pred}$和$\\textbf{l}_{pred}$是满足$R(\\theta_{pred},\\textbf{l}_{pred})\\in SO(3)$。类似的，损失函数$L_d$也计算了沿着两根不同的轴$\\textbf{l}_{GT}$和$\\textbf{l}_{pred}$的平移误差，如果我们只是计算$d_{GT}$和$d_{pred}$的范数的话，就等于我们默认了它们是沿着同一个轴平移的，这就不合理。 ​ 综上所述，我们Loss函数的选取遵循了我们所提出的Screw理论。 打标签方法​ ScrewNet的训练集包括了一系列的深度图像，并且需要有对应的screw displacement。使用Mujoco来渲染仿真中的关节体并且记录深度图像。使用了数据集中的柜子、抽屉、微波炉、烤箱等。 ​ 为了创建screw displacement标签，我们考虑$o_i$作为基物体，然后我们计算后面的$o_j$相对于基物体的相对screw displacement。具体来说，就是给定一个N帧图片的视频流$I_{1:N}$，我们首先选定视频的第一帧是物体的基础位姿，然后计算出n-1帧的相对的screw displacement。 ​ 也就是我们有相对于坐标系$F_{O_j^1}$的n-1个位移了，我们可以通过在普吕克坐标下做变换把这n-1个相对位移全部转换到相对于基坐标轴$O_i$下。具体的普吕克坐标系下的变换形式可以参考原文。 代码实现1.models.py​ 在代码中，ScrewNet提供了三个模型：ScrewNet、ScrewNet_2imgs、ScrewNet_NoLSTM，在实际训练和测试中，和数据集的对应关系如下： 123456789101112if args.model_type == '2imgs': print(&quot;Testing Model: ScrewNet_2imgs&quot;) best_model = ScrewNet_2imgs(n_output=8) testset = RigidTransformDataset(args.ntest, args.test_dir)elif args.model_type == 'noLSTM': print(&quot;Testing Model: ScrewNet_noLSTM&quot;) best_model = ScrewNet_NoLSTM(seq_len=16, fc_replace_lstm_dim=1000, n_output=8) testset = ArticulationDataset(args.ntest, args.test_dir)else: print(&quot;Testing ScrewNet&quot;) best_model = ScrewNet(lstm_hidden_dim=1000, n_lstm_hidden_layers=1, n_output=8) testset = ArticulationDataset(args.ntest, args.test_dir) ​ 其中ScrewNet_2imgs似乎是一个降级版本，我们先从这个模型的源码开始读起： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class ScrewNet_2imgs(nn.Module): def __init__(self, n_output=8): super(ScrewNet_2imgs, self).__init__() self.fc_mlp_dim_1 = 2000 self.fc_mlp_dim_2 = 512 self.fc_mlp_dim_3 = 256 self.n_output = n_output self.resnet = models.resnet18() self.bn_res_1 = nn.BatchNorm1d(1000, momentum=0.01) #需要归一化的维度为1000 self.fc_mlp_1 = nn.Linear(self.fc_mlp_dim_1, self.fc_mlp_dim_1) self.bn_mlp_1 = nn.BatchNorm1d(self.fc_mlp_dim_1, momentum=0.01) self.fc_mlp_2 = nn.Linear(self.fc_mlp_dim_1, self.fc_mlp_dim_2) self.bn_mlp_2 = nn.BatchNorm1d(self.fc_mlp_dim_2, momentum=0.01) self.fc_mlp_3 = nn.Linear(self.fc_mlp_dim_2, self.fc_mlp_dim_3) self.bn_mlp_3 = nn.BatchNorm1d(self.fc_mlp_dim_3, momentum=0.01) self.fc_mlp_4 = nn.Linear(self.fc_mlp_dim_3, self.n_output) def forward(self, X_3d): # X shape: Batch x Sequence x 3 Channels x img_dims # Run resnet sequentially on the data to generate embedding sequence cnn_embed_seq = [] for t in range(X_3d.size(1)): #从第一帧开始枚举 x = self.resnet(X_3d[:, t, :, :, :]) #resnet的输入B * 1 * 3 * W * H x = x.view(x.size(0), -1) #拉伸为B * vector_size(每一个vector为CNN隐变量) x = self.bn_res_1(x) #归一化为B * 1000 cnn_embed_seq.append(x) # 此时我们得到了cnn_embed_seq是大小为N的一个list，其中每个元素为B * 1000的格式 # 首先我们把它变为torch.tensor，并且交换sample dim和time dim cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1) # 此时我们有cnn_embed_seq = (B * N * 1000) # 因为在transpose后，虽然tensor的shape改变了，但是它在内存中的存储位置并没有改变，如果我们直接调用view会出错，所以我们需要先做.contiguous()，然后再使用view x_rnn = cnn_embed_seq.contiguous().view(-1, self.fc_mlp_dim_1) # 注意到我们此时view成了(B * 2000)，所以我们可以反推原先输入的N=2，所以这个模型的dataset都是2帧的视频 # FC layers x_rnn = self.fc_mlp_1(x_rnn) # B * 2000 =&gt; B * 2000 x_rnn = F.relu(x_rnn) x_rnn = self.bn_mlp_1(x_rnn) x_rnn = self.fc_mlp_2(x_rnn) # B * 2000 =&gt; B * 512 x_rnn = F.relu(x_rnn) x_rnn = self.bn_mlp_2(x_rnn) x_rnn = self.fc_mlp_3(x_rnn) # B * 512 =&gt; B * 256 x_rnn = F.relu(x_rnn) x_rnn = self.bn_mlp_3(x_rnn) x_rnn = self.fc_mlp_4(x_rnn) # B * 256 =&gt; B * 8（其中8维就是screw parameter) return x_rnn.view(X_3d.size(0), -1) #返回 B * 8 ​ 其中的models.resnet18()其实是torchvision.models.resnet.py中帮我们实现好的resnet，我们可以单纯地认为输入B * 3 * W * H，输出一个B * 1000的ResNet特征。 ​ 接下来是No_LSTM版本的ScrewNet： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263class ScrewNet(nn.Module): def __init__(self, lstm_hidden_dim=1000, n_lstm_hidden_layers=1, drop_p=0.5, n_output=8): super(ScrewNet, self).__init__() self.fc_res_dim_1 = 512 self.lstm_input_dim = 1000 self.lstm_hidden_dim = lstm_hidden_dim self.n_lstm_hidden_layers = n_lstm_hidden_layers self.fc_lstm_dim_1 = 256 self.fc_lstm_dim_2 = 128 self.n_output = n_output self.drop_p = drop_p self.resnet = models.resnet18() self.fc_res_1 = nn.Linear(self.lstm_input_dim, self.fc_res_dim_1) self.bn_res_1 = nn.BatchNorm1d(self.fc_res_dim_1, momentum=0.01) self.fc_res_2 = nn.Linear(self.fc_res_dim_1, self.lstm_input_dim) self.LSTM = nn.LSTM( input_size=self.lstm_input_dim, hidden_size=self.lstm_hidden_dim, num_layers=self.n_lstm_hidden_layers, batch_first=True, ) self.fc_lstm_1 = nn.Linear(self.lstm_hidden_dim, self.fc_lstm_dim_1) self.bn_lstm_1 = nn.BatchNorm1d(self.fc_lstm_dim_1, momentum=0.01) self.fc_lstm_2 = nn.Linear(self.fc_lstm_dim_1, self.fc_lstm_dim_2) self.bn_lstm_2 = nn.BatchNorm1d(self.fc_lstm_dim_2, momentum=0.01) self.dropout_layer1 = nn.Dropout(p=self.drop_p) self.fc_lstm_3 = nn.Linear(self.fc_lstm_dim_2, self.n_output) def forward(self, X_3d): # 输入的大小 B * N * 3 * W * H cnn_embed_seq = [] for t in range(X_3d.size(1)): #枚举N帧 x = self.resnet(X_3d[:, t, :, :, :]) # B * 1 * 1000 x = x.view(x.size(0), -1) # B * 1000 x = self.bn_res_1(self.fc_res_1(x)) # B * 1000 =&gt; B * 512 x = F.relu(x) x = self.fc_res_2(x) # B * 512 =&gt; B * 1000 cnn_embed_seq.append(x) cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1) # 此时我们有cnn_embed_seq = (B * N * 1000) # 为了提高内存的利用率和效率，调用flatten_parameters让parameter的数据存放成contiguous chunk(连续的块)。类似我们调用tensor.contiguous self.LSTM.flatten_parameters() RNN_out, (h_n, h_c) = self.LSTM(cnn_embed_seq, None) # h_c shape (n_layers, B, hidden_size)，默认值为(1, B, 1000) # h_n shape (n_layers, B, hidden_size)，默认值为(1, B, 1000) # RNN_out = (B * N * 1000) # None represents zero initial hidden state # FC layers x_rnn = RNN_out.contiguous().view(-1, self.lstm_hidden_dim) # BN * 1000 x_rnn = self.bn_lstm_1(self.fc_lstm_1(x_rnn)) # BN * 1000 =&gt; BN * 256 x_rnn = F.relu(x_rnn) x_rnn = self.bn_lstm_2(self.fc_lstm_2(x_rnn)) # BN * 256 =&gt; BN * 128 x_rnn = F.relu(x_rnn) x_rnn = self.fc_lstm_3(x_rnn) # BN * 8 return x_rnn.view(X_3d.size(0), -1) # return B * 8N ​ 这里涉及到LSTM的输入和输出，可以参考官网上的参数介绍。注意到最后的全连接层的维度变化，最后之所以预测B * 8N，是因为每个样本都有N帧，我们需要预测出每一帧的关节体参数，至于为什么不是$B \\times N \\times8$，这倒不是很重要，反正在back propagation的时候我们只需要准确地实现loss function，都能回归出来。 ​ 代码中还提供了一个no_lstm版本， 12345678910111213141516class ScrewNet_NoLSTM(nn.Module): def __init__(self, seq_len=16, fc_replace_lstm_dim=1000, n_output=8): super(ScrewNet_NoLSTM, self).__init__() self.fc_replace_lstm_seq_dim = fc_replace_lstm_dim * seq_len ... self.fc_replace_lstm = nn.Linear(self.fc_replace_lstm_seq_dim, self.fc_replace_lstm_seq_dim) def forward(self, X_3d): ... # FC replacing LSTM layer # cnn_embed_seq = (B * N * 1000) cnn_embed_seq = cnn_embed_seq.contiguous().view(cnn_embed_seq.size(0), -1) # (B * N * 1000) =&gt; (B * 1000N) x_rnn = F.relu(self.fc_replace_lstm(cnn_embed_seq)) #(B * 1000N) =&gt; (B * 1000N) x_rnn = x_rnn.view(-1, self.fc_replace_lstm_dim) #(BN * 1000) #后面就继续连接FC层和上面一模一样了 ... loss.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980def articulation_lstm_loss_spatial_distance(pred, target, wt_on_ortho=1.): &quot;&quot;&quot; Based on Spatial distance. Please refer to the paper for more details. &quot;&quot;&quot; pred = pred.view(pred.size(0), -1, 8)[:, 1:, :] # We don't need the first row as it is for single image # (B, N - 1, 8) # Spatial Distance loss，计算的是轴角度的误差以及轴平移的误差 dist_err = orientation_difference_bw_plucker_lines(target, pred) ** 2 + \\ 2. * distance_bw_plucker_lines(target, pred) ** 2 # Configuration Loss，也就是theta和d的误差 conf_err = theta_config_error(target, pred) ** 2 + d_config_error(target, pred) ** 2 err = dist_err + conf_err loss = torch.mean(err) # Ensure l_hat has norm 1. # 单位向量约束 loss += torch.mean((torch.norm(pred[:, :, :3], dim=-1) - 1.) ** 2) # Ensure orthogonality between l_hat and m # l和m的正交约束 loss += wt_on_ortho * torch.mean(torch.abs(torch.sum(torch.mul(pred[:, :, :3], pred[:, :, 3:6]), dim=-1))) if torch.isnan(loss): print(&quot;target: Min: {}, Max{}&quot;.format(target.min(), target.max())) print(&quot;Prediction: Min: {}, Max{}&quot;.format(pred.min(), pred.max())) print(&quot;L2 error: {}&quot;.format(torch.mean((target - pred) ** 2))) print(&quot;Distance loss:{}&quot;.format(torch.mean(orientation_difference_bw_plucker_lines(target, pred) ** 2))) print(&quot;Orientation loss:{}&quot;.format(torch.mean(distance_bw_plucker_lines(target, pred) ** 2))) print(&quot;Configuration loss:{}&quot;.format(torch.mean(conf_err))) return lossdef distance_bw_plucker_lines(target, prediction, eps=1e-10): &quot;&quot;&quot; Input shapes Tensors: Batch X #Images X 8 # Based on formula from Plücker Coordinates for Lines in the Space by Prof. Yan-bin Jia # Verified by https://keisan.casio.com/exec/system/1223531414 &quot;&quot;&quot; norm_cross_prod = torch.norm(torch.cross(target[:, :, :3], prediction[:, :, :3], dim=-1), dim=-1) dist = torch.zeros_like(norm_cross_prod) # Checking for Parallel Lines if torch.any(norm_cross_prod &lt;= eps): zero_idxs = (norm_cross_prod &lt;= eps).nonzero(as_tuple=True) scales = torch.norm(prediction[zero_idxs][:, :3], dim=-1) / torch.norm(target[zero_idxs][:, :3], dim=-1) + eps dist[zero_idxs] = torch.norm(torch.cross(target[zero_idxs][:, :3], ( target[zero_idxs][:, 3:6] - prediction[zero_idxs][:, 3:6] / scales.unsqueeze(-1))), dim=-1) / ( torch.mul(target[zero_idxs][:, :3], target[zero_idxs][:, :3]).sum(dim=-1) + eps) # Skew Lines: Non zero cross product nonzero_idxs = (norm_cross_prod &gt; eps).nonzero(as_tuple=True) dist[nonzero_idxs] = torch.abs( torch.mul(target[nonzero_idxs][:, :3], prediction[nonzero_idxs][:, 3:6]).sum(dim=-1) + torch.mul( target[nonzero_idxs][:, 3:6], prediction[nonzero_idxs][:, :3]).sum(dim=-1)) / ( norm_cross_prod[nonzero_idxs] + eps) return distdef orientation_difference_bw_plucker_lines(target, prediction, eps=1e-6): &quot;&quot;&quot; Input shapes Tensors: (B, N, 8) range of arccos ins [0, pi)&quot;&quot;&quot; return torch.acos(torch.clamp(torch.mul(target[:, :, :3], prediction[:, :, :3]).sum(dim=-1) / (torch.norm(target[:, :, :3], dim=-1) * torch.norm(prediction[:, :, :3], dim=-1) + eps), min=-1, max=1))def theta_config_error(target, prediction): # theta的loss rot_tar = angle_axis_to_rotation_matrix(target[:, :, :3], target[:, :, 6]).view(-1, 3, 3) rot_pred = angle_axis_to_rotation_matrix(prediction[:, :, :3], prediction[:, :, 6]).view(-1, 3, 3) I_ = torch.eye(3).reshape((1, 3, 3)) I_ = I_.repeat(rot_tar.size(0), 1, 1).to(target.device) return torch.norm(I_ - torch.bmm(rot_pred, rot_tar.transpose(1, 2)), dim=(1, 2), p=2).view(target.shape[:2])def d_config_error(target, prediction): tar_d = target[:, :, 7].unsqueeze(-1) pred_d = prediction[:, :, 7].unsqueeze(-1) tar_d = target[:, :, :3] * tar_d pred_d = prediction[:, :, :3] * pred_d return (tar_d - pred_d).norm(dim=-1) 2.dataset.py​ 我们先从简单的两张图片的数据集RigidTransformDataset入手，注意到数据集只需要override __len__和 __getitem__。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&quot;&quot;&quot;Data loader class for the 2-imgs ablated version &quot;&quot;&quot;class RigidTransformDataset(Dataset): def __init__(self, ntrain, root_dir, n_dof=1, norm_factor=1., transform=None): super(RigidTransformDataset, self).__init__() self.root_dir = root_dir self.labels_data = None self.length = ntrain self.n_dof = n_dof self.normalization_factor = norm_factor self.transform = transform self.augmentation_factor = 15 def __len__(self): return self.length def __getitem__(self, idx, imgs_per_object=16): if self.labels_data is None: self.labels_data = h5py.File(os.path.join(self.root_dir, 'complete_data.hdf5'), 'r') obj_idx = int(idx / self.augmentation_factor) obj_data_idx = idx % self.augmentation_factor + 1 obj_data = self.labels_data['obj_' + str(obj_idx).zfill(6)] #编号补齐0到6位 # Load depth image depth_imgs = torch.tensor([obj_data['depth_imgs'][0], obj_data['depth_imgs'][obj_data_idx]]) #只取开始和最后的两张图片 #此时为 N * W * H depth_imgs.unsqueeze_(1).float() #使用unsqueeze_添加一维，变成 N * 1 * W * H depth_imgs = torch.cat((depth_imgs, depth_imgs, depth_imgs), dim=1) # 深度通道复制三份，变成N * 3 * W * H（这真的会有用吗???） # # Load labels pt1 = obj_data['moving_frame_in_world'][0, :] # 世界系下的四元数1 pt2 = obj_data['moving_frame_in_world'][obj_data_idx, :] # 世界西夏的四元数2 pt1_T_pt2 = change_frames(pt1, pt2) #计算出相对pose # Object pose in world obj_pose_in_world = np.array(obj_data['embedding_and_params'])[-7:] # obj_pose, obj_quat_wxyz obj_T_pt1 = change_frames(obj_pose_in_world, pt1) #也就是论文中提到的向着base object frame转换 # 用screw参数创建标签，label := &lt;l_hat, m, theta, d&gt; = &lt;3, 3, 1, 1&gt; l_hat, m, theta, d = transform_to_screw(translation=pt1_T_pt2[:3], quat_in_wxyz=pt1_T_pt2[3:]) # Convert line in object_local_coordinates new_l = transform_plucker_line(np.concatenate((l_hat, m)), trans=obj_T_pt1[:3], quat=obj_T_pt1[3:]) label = np.concatenate((new_l, [theta], [d])) # This defines frames wrt pt 1 # Normalize labels label[3:6] /= self.normalization_factor # Scaling m appropriately label = torch.from_numpy(label).float() sample = {'depth': depth_imgs, 'label': label} #最终一个GT以dict的形式打包传出 return sample ​ 多张图片的其实就大差不差了，不过里面有一个细节我们需要深究一下。 123456789101112131415161718192021222324252627282930313233343536class ArticulationDataset(Dataset): def __getitem__(self, idx): ... pt1 = moving_body_poses[0, :] # Fixed common reference frame for i in range(len(moving_body_poses) - 1): pt2 = moving_body_poses[i + 1, :] pt1_T_pt2 = change_frames(pt1, pt2) # Generating labels in screw notation: label := &lt;l_hat, m, theta, d&gt; = &lt;3, 3, 1, 1&gt; l_hat, m, theta, d = transform_to_screw(translation=pt1_T_pt2[:3], quat_in_wxyz=pt1_T_pt2[3:]) # Convert line in object_local_coordinates new_l = transform_plucker_line(np.concatenate((l_hat, m)), trans=obj_T_pt1[:3], quat=obj_T_pt1[3:]) label[i, :] = np.concatenate((new_l, [theta], [d])) # This defines frames wrt pt 1 def transform_to_screw(translation, quat_in_wxyz, tol=1e-6): dq = dq3d.dualquat(dq3d.quat(quat_as_xyzw(quat_in_wxyz)), translation) screw = dual_quaternion_to_screw(dq, tol) return screwdef dual_quaternion_to_screw(dq, tol=1e-6): l_hat, theta = tf3d.quaternions.quat2axangle(np.array([dq.real.w, dq.real.x, dq.real.y, dq.real.z])) if theta &lt; tol or abs(theta - np.pi) &lt; tol: t_vec = dq.translation() l_hat = t_vec / (np.linalg.norm(t_vec) + 1e-10) theta = tol # This makes sure that tan(theta) is defined else: t_vec = (2 * tf3d.quaternions.qmult(dq.dual.data, tf3d.quaternions.qconjugate(dq.real.data)))[ 1:] # taking xyz from wxyz d = t_vec.dot(l_hat) m = (1 / 2) * (np.cross(t_vec, l_hat) + ((t_vec - d * l_hat) / np.tan(theta / 2))) return l_hat, m, theta, d ​ 只有理解了transform_to_screw这个函数在做什么，我们才真正摸索到了Screw Theory的实质。主要可以参考这篇文献。总体逻辑就是四元数可以表示三维旋转，而对偶四元数可以同时表示三维旋转和平移，所以使用对偶四元数来表示Screw Parameter就是很合理的事情，满足以下推导： ​ 空间任意刚体运动，可分解为刚体上某一点的平移，以及绕经过此点的旋转轴的转动，我们令这个点为连体基坐标原点，我们记作$R$和$t$，旋转矩阵$R$对应的四元数为$p$，由$R$和$t$可以计算出对偶四元数$q$。根据Chasles theorem(Screw theory，沙勒定理)我们又知道：空间任意刚体运动，均可看作有限螺旋运动，即均可表示为绕一轴的旋转和沿该轴的平移，参数可以记为$(\\textbf{l},\\textbf{m}, \\theta, d)$。 ​ 首先，四元数$p$转化为轴角表达$p=(cos(\\frac{\\theta}{2}),\\textbf{l}sin(\\frac{\\theta}{2}))$就可以直接得到$\\textbf{l}$和$\\theta$，参数物理意义完全相同。其余参数满足下式：$$d=\\textbf{t}\\cdot\\textbf{l}=(2qp^*)\\cdot\\textbf{l} \\\\m=\\frac{1}{2}(\\textbf{t}\\times\\textbf{l}+(\\textbf{t}-d\\textbf{l})\\cot\\frac{\\theta}{2})$$ 3.train_model.py​ 里面涉及到三种不同的模型的定义，封装地也很好，总体逻辑还是非常简单易懂的。 12345678910111213141516171819202122232425262728293031323334trainset = ...testset = ...loss_fn = ...network = ...testloader = torch.utils.data.DataLoader(testset, batch_size=args.batch, shuffle=True, num_workers=args.nwork, pin_memory=True)trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch, shuffle=True, num_workers=args.nwork, pin_memory=True)# Load Saved weightsif args.load_wts: network.load_state_dict(torch.load(args.wts_dir + args.prior_wts + '.net'))# setup trainerif torch.cuda.is_available(): device = torch.device(args.device)else: device = torch.device('cpu')optimizer = torch.optim.Adam(network.parameters(), lr=args.learning_rate, weight_decay=1e-2)scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_schedule, gamma=lr_gamma)trainer = ModelTrainer(model=network, train_loader=trainloader, test_loader=testloader, optimizer=optimizer, scheduler=scheduler, criterion=loss_fn, epochs=args.epochs, name=args.name, test_freq=args.val_freq, device=args.device)# trainbest_model = trainer.train() 4.model_trainer.py​ 其实这就没啥好说的了，无非就是训练（算loss，反向传播，画图，训练日志，保存模型）和测试（计算均值和方差）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142class ModelTrainer(object): def __init__(self, *kwargs): pass def train(self): best_tloss = 1e8 for epoch in range(self.epochs + 1): sys.stdout.flush() loss = self.train_epoch(epoch) self.losses.append(loss) self.writer.add_scalar('Loss/train', loss, epoch) if epoch % self.test_freq == 0: tloss = self.test_epoch(epoch) self.tlosses.append(tloss) self.plot_losses() self.writer.add_scalar('Loss/validation', tloss, epoch) if tloss &lt; best_tloss: print('saving model.') net_fname = os.path.join(self.wts_dir, str(self.name) + '.net') torch.save(self.model.state_dict(), net_fname) # 把表现更好的模型存到本地 best_tloss = tloss self.scheduler.step() # Visualize gradients total_norm = 0. nan_count = 0 for tag, parm in self.model.named_parameters(): if torch.isnan(parm.grad).any(): print(&quot;Encountered NaNs in gradients at {} layer&quot;.format(tag)) nan_count += 1 else: self.writer.add_histogram(tag, parm.grad.data.cpu().numpy(), epoch) param_norm = parm.grad.data.norm(2) total_norm += param_norm.item() ** 2 total_norm = total_norm ** (1. / 2) self.writer.add_scalar('Gradient/2-norm', total_norm, epoch) if nan_count &gt; 0: raise ValueError(&quot;Encountered NaNs in gradients&quot;) # plot losses one more time self.plot_losses() # re-load the best state dictionary that was saved earlier. self.model.load_state_dict(torch.load(net_fname, map_location='cpu')) # export scalar data to JSON for external processing self.writer.export_scalars_to_json(&quot;./all_scalars.json&quot;) self.writer.close() return self.model def train_epoch(self, epoch): start = time.time() running_loss = 0 batches_per_dataset = len(self.trainloader.dataset) / self.trainloader.batch_size self.model.train() # Put model in training mode for i, X in enumerate(self.trainloader): self.optimizer.zero_grad() depth, labels = X['depth'].to(self.device), \\ X['label'].to(self.device) y_pred = self.model(depth) loss = self.criterion(y_pred, labels) if loss.data == -float('inf'): print('inf loss caught, not backpropping') running_loss += -1000 else: loss.backward() # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs. torch.nn.utils.clip_grad_norm_(self.model.parameters(), 10.) self.optimizer.step() running_loss += loss.item() stop = time.time() print('Epoch %s - Train Loss: %.5f Time: %.5f' % (str(epoch).zfill(3), running_loss / batches_per_dataset, stop - start)) return running_loss / batches_per_dataset def test_epoch(self, epoch): start = time.time() running_loss = 0 batches_per_dataset = len(self.testloader.dataset) / self.testloader.batch_size self.model.eval() # Put batch norm layers in eval mode with torch.no_grad(): for i, X in enumerate(self.testloader): depth, labels = X['depth'].to(self.device), \\ X['label'].to(self.device) y_pred = self.model(depth) loss = self.criterion(y_pred, labels) running_loss += loss.item() stop = time.time() print('Epoch %s - Test Loss: %.5f Euc. Time: %.5f' % (str(epoch).zfill(3), running_loss / batches_per_dataset, stop - start)) return running_loss / batches_per_dataset def test_best_model(self, best_model, fname_suffix='', dual_quat_mode=False): best_model.eval() # Put model in evaluation mode ... with torch.no_grad(): for X in self.testloader: depth, all_labels, labels = X['depth'].to(self.device), \\ X['all_labels'].to(self.device), \\ X['label'].to(self.device) y_pred = best_model(depth, all_labels) y_pred = y_pred.view(y_pred.size(0), -1, 8) if dual_quat_mode: y_pred = dual_quaternion_to_screw_batch_mode(y_pred) labels = dual_quaternion_to_screw_batch_mode(labels) err = labels - y_pred all_l_hat_err = torch.cat( (all_l_hat_err, torch.mean(torch.norm(err[:, :, :3], dim=-1), dim=-1).cpu())) all_m_err = torch.cat((all_m_err, torch.mean(torch.norm(err[:, :, 3:6], dim=-1), dim=-1).cpu())) all_q_err = torch.cat((all_q_err, torch.mean(err[:, :, 6], dim=-1).cpu())) all_d_err = torch.cat((all_d_err, torch.mean(err[:, :, 7], dim=-1).cpu())) all_l_hat_std = torch.cat( (all_l_hat_std, torch.std(torch.norm(err[:, :, :3], dim=-1), dim=-1).cpu())) all_m_std = torch.cat((all_m_std, torch.std(torch.norm(err[:, :, 3:6], dim=-1), dim=-1).cpu())) all_q_std = torch.cat((all_q_std, torch.std(err[:, :, 6], dim=-1).cpu())) all_d_std = torch.cat((all_d_std, torch.std(err[:, :, 7], dim=-1).cpu())) # Plot variation of screw axis pass def plot_grad_flow(self, named_parameters): pass def plot_losses(self): pass 5.Other Modules​ 其他辅助模组就暂时不继续占据篇幅了。注意到还存在一个noisy_models.py，引用了这个仓库，可能是对应的paper的数据增强手段，此处不表。","link":"/blog/2021/12/20/screwnet/"},{"title":"PointNet and related works","text":"​ 最近一门机器学习的课有一个阅读论文的作业，我选了老本行votenet，不过发现自己一些实现细节还是不过关。借着这个机会再从pointnet开始梳理一下思路并且解读一下源码。讲解视频 PointNet​ PointNet一定是3D pointcloud perception的里程碑，保留了大道至简的美感的同时，给出了严格的证明。是从每个角度都应该称赞的作品。 ​ 正如原文所说，又是空间中的n个点的集合有如下特性： 无序性（输出应当与输入的n个点的排序无关） 点和其邻居之间是有一些依赖关系的，我们设计的网络需要能够从邻居点中提取出局部特征 旋转和位移不变性（不过pointnet和pointnet++对旋转的处理都不是很好） 所以pointnet使用了max-pooling层来作为对称函数解决点云无序性的问题。 PointNet的证明​ 我们令$\\chi={S:S\\subseteq[0,1]^m\\text{and}|S|=n}$，此处的S就是我们的input，即一个欧式空间中的点集实例。此处做了归一化处理，所以它是模长恒定的m维向量。而$\\chi$自然就是欧式空间中的点集的集合。我们希望拟合一个定义在集合上的连续函数$f:\\chi\\rightarrow\\mathbb{R}$，注意此处的$\\mathbb{R}$是指实数空间，而不是实数。 ​ 因为这个函数是连续函数，所以在点集$S,S’\\in\\chi$上有如下性质：$$\\forall\\varepsilon&gt;0,\\exist\\delta&gt;0 \\\\\\textbf{if } d_h(S,S’)&lt;\\delta \\\\\\textbf{then} |f(S)-f(S’)|&lt;\\varepsilon$$​ 其中的$d_h$其实就是集合距离（豪斯多夫距离），我们只需要把上述认为是普通函数连续性的推广即可。 ​ 对于PointNet来说，它得到的拟合出来的函数的形式是：$\\gamma(\\mathop{MAX}_{x_i\\in S}{h(x_i)})$。其中，我们令$S={x_1,x_2,…,x_n}$，其中$x_i\\in R^N$，比如说我们单纯的空间位置的话，那就是$N=3$，而$h(x_i)$其实就是对这n个点做shared-MLP，得到的结果做一个maxpooling出一个$1\\times1024$维的global feature。然后$\\gamma$其实就是最后处理global feature的MLP。 ​ 我们需要证明的就是$$\\forall\\varepsilon&gt;0,\\exist \\text{ such a function} \\\\\\textbf{then} |f(S)-\\gamma(\\mathop{MAX}_{x_i\\in S}{h(x_i)})|&lt;\\varepsilon$$ 相对直观的分析​ 我们可以简单地认为$h(·)$就是把$x_i$映射到空间网格中的一个格子里，我们记这个空间网格的大小为$M\\times M\\times M$。那么因为点只会映射到一个网格中，我们可以得到$h(x_i)$就是一个大小为$1\\times M^3$的向量，并且只有一个值为1，其余值为0。 而MAX函数我们可以认为是使用空间网格来重建我们的输入点云，我们可以令这个网格足够密集（M足够大），使得每个网格至多包含一个原先的点，这样就等于我们做完max-pooling后，得到了一个$1\\times M^3$的向量，其中有n个点为1，其余为0。 ​ 因为网格的密度可以足够大，所以我们可以使用网格模型以任意精度去近似我们原先的点云集合$S$。接下来的$\\gamma$其实就是对这个新的表示形式的$1\\times M^3$向量(global feature)去做一个MLP。因为MLP可以近似任意函数，那它自然也能近似$f$函数，得证。本证明过程参考了深蓝学院的点云课程。 论文中的纯数学分析参考CSDN。知乎。 PointNet++​ PointNet只使用一个max-pooling层来整合全局信息，而PointNet++使用层级结构来逐层提取特征，并且不断地从层级中抽象出更大的局部区域。在PointNet++中，主要是通过set abstraction layer来实现的，它包含了sampling layer（最远点采样）, grouping layer和PointNet layer。 Sampling Layer​ 其实这一部分就是一个最远点采样的工作。也就是输入的维度是$B\\times N \\times(C+D)$，通常情况下$C=3$，而$D$就是特征维度，并且我们已知当前层需要采样到$\\text{npoints}$个点。那么其实就是$B\\times N \\times (C+D)\\rightarrow B\\times\\text{npoints}\\times(C+D)$。 Grouping Layer​ 这个层的目标是找到每个点的邻居，其实就是从半径为$r$的球中找到至多$\\text{nsample}$个元素。 PointNet Layer​ 这个其实就是拿原先的PointNet来提取特征。 Set Abstraction Layer的MSG优化 ​ 因为原先的set abstraction layer是在固定的一个半径上去做的，感受野是固定大小。而MSG就是在多个不同的半径上去提取特征，最后组合在一起。 Point Feature Propagation​ 在set abstraction layer中，原先的点集被降采样了。然而在点分割任务中，我们需要对每个点获取到一个点的种类标签，所以我们希望得到原先所有点的一个特征。一个方法就是在set abstraction layer中，我们永远采样所有的点作为中心点，但是这会导致计算消耗非常大，另一个方法就是使用point feature propagation。 ​ 在feature propagation layer中，它的输入是$N_i\\times(d+C)$，而它的输出的$N_{i-1}\\times(d+C)$，注意到其中的$N_i$就是在每层的set abstraction layer中的大小。所以其实这就是一个Decoder，并且在每一层Decode得到了全局特征后，再拼上了原先set abstraction layer的局部特征。 代码​ 接下来我们尝试来理解PointNet++的源码，PointNet++提供了三种任务的代码：classification、part segmentation和semantic segmentation，而set abstraction layer分为了SSG(single-scale grouping)和MSG(multi-scale grouping)。通常有纯Python版本的Pytorch实现和带有Cuda实现功能函数的Pytorch实现，因为最原先开源的版本是Tensorflow版本，这两个Pytorch版本在变量命名上都借鉴了原先版本的命名，但是这和论文中的参数命名是一个都对不上。并且个别出出现了破坏软件抽象规则的地方，如在PointNetSetAbstractionMsg中调用了类似于sample_and_group的结构但并没有复用代码，亦或者sample_and_group_all其实可以规约到sample_and_group，但是又重新写了一个函数等一系列问题。所以总体代码看起来比较痛苦，不过当我们彻底搞清楚代码以后，我们就可以把PoineNet++当做开箱即用的东西，再也不管它的底层实现了。 sample_and_group的实现12345678910111213141516171819202122232425262728293031323334def sample_and_group(npoint, radius, nsample, xyz, features, returnfps=False): &quot;&quot;&quot; Input: npoint: N_{i+1} radius: 查询半径 nsample: 考察至多几个邻域中的点 xyz: input points position data, [B, N, 3] features: input points feature data, [B, N, C] Return: new_xyz: sampled points position data, [B, npoint, nsample, 3] new_FEATURES: sampled points feature data, [B, npoint, nsample, 3+D] &quot;&quot;&quot; B, N, d = xyz.shape # d = 3 fps_idx = farthest_point_sample(xyz, npoint) # 最远点采样，从N_{i}个点中选出N_{i+1}个作为中心点 new_xyz = index_points(xyz, fps_idx) # [B, N_i, 3] -&gt; [B, N_{i+1}, 3] idx = query_ball_point(radius, nsample, xyz, new_xyz) grouped_xyz = index_points(xyz, idx) # [B, N_{i+1}, nsample, 3] grouped_xyz_norm = grouped_xyz - new_xyz.view(B, npoint, 1, d) # 此处的new_xyz先变为[B, N_{i+1}, 1, d]，然后加减法自动repeat，最终得到 [B, N_{i+1}, nsample, 3] # 此处就是把所有得到的点都转化为了相对于中心点的位置。 if features is not None: grouped_features = index_points(features, idx) # [B, N_{i+1}, nsample, C_i] new_features = torch.cat([grouped_xyz_norm, grouped_features], dim=-1) # [B, N_{i+1}, nsample, 3+C_i] else: new_features = grouped_xyz_norm if returnfps: return new_xyz, new_features, grouped_xyz, fps_idx else: return new_xyz, new_features # new_xyz = [B, N_{i+1}, K_{i+1}, 3] # new_features = [B, N_{i+1}, K_{i+1}, 3+C_i] query_ball_point的实现12345678910111213141516171819202122232425def query_ball_point(radius, K, xyz, new_xyz): &quot;&quot;&quot; Input: radius: local region radius nsample: max sample number in local region，注意这个邻居是要在原先的N_i个点中找的。 xyz: all points, [B, N_i, 3] new_xyz: query points, [B, N_{i+1}, 3] Return: group_idx: grouped points index, [B, N_{i+1}, K] &quot;&quot;&quot; device = xyz.device B, N, C = xyz.shape _, npoint, _ = new_xyz.shape group_idx = torch.arange(N, dtype=torch.long).to(device).view(1, 1, N).repeat([B, npoint, 1]) # 每个S（中心点）都有一个向量[1, 2, 3, ..., N]，如果这个向量里的值为N，那么就是不在中心点的邻域里；如果向量值等于自己，就是在中心点的邻域里 # 此时我们得到group_idex = [B * N_{i+1} * N_{i}] sqrdists = square_distance(new_xyz, xyz) # 输出的大小为 [B, N_{i+1}, N_{i}]，每个[N_{i+1}, N_{i}]里记录了两个点之间的距离 group_idx[sqrdists &gt; radius ** 2] = N # 那些半径不满足条件的索引值都设置为N group_idx = group_idx.sort(dim=-1)[0][:, :, :K] # 按照序号从小到大排，eg:[1, 3, 4, 5, ..., N, N, N] # 排完序后取前K个，那也就是[B, N_{i+1}, K]，意思就是每个中心点都对应了K个邻居的序号。 group_first = group_idx[:, :, 0].view(B, S, 1).repeat([1, 1, K]) mask = group_idx == N group_idx[mask] = group_first[mask] # 对于每个中心点的满足半径的邻居可能少于K个的情况，全部用第一个邻居的序号来替代。 return group_idx Classificaction Task​ 这一部分其实就是论文中的这个结构： ​ 我们注意到在实现中有$N_1=512,N_2=128$，$d=3$是点云的欧式空间坐标，如果点云输入有法向量数据，那么$C=3$，否则$C=0$。并且中间层有$C_1=128,C_2=256，C_4=1024,k=\\text{num_class}$，在数据集modelnet40下，$k=\\text{num_class}=40$。 1234567891011121314151617181920212223242526272829303132333435class get_model(nn.Module): def __init__(self,num_class,normal_channel=True): super(get_model, self).__init__() in_channel = 6 if normal_channel else 3 self.normal_channel = normal_channel self.sa1 = PointNetSetAbstraction(npoint=512, radius=0.2, K=32, in_channel=in_channel, mlp=[64, 64, 128], group_all=False) self.sa2 = PointNetSetAbstraction(npoint=128, radius=0.4, K=64, in_channel=128 + 3, mlp=[128, 128, 256], group_all=False) self.sa3 = PointNetSetAbstraction(npoint=None, radius=None, K=None, in_channel=256 + 3, mlp=[256, 512, 1024], group_all=True) self.fc1 = nn.Linear(1024, 512) self.bn1 = nn.BatchNorm1d(512) self.drop1 = nn.Dropout(0.4) self.fc2 = nn.Linear(512, 256) self.bn2 = nn.BatchNorm1d(256) self.drop2 = nn.Dropout(0.4) self.fc3 = nn.Linear(256, num_class) def forward(self, xyz): #xyz = [B, 3+ C(0 or 3), N_0] B, _, _ = xyz.shape if self.normal_channel: norm = xyz[:, 3:, :] # norm = [B, 0 or 3, N_0] (如果用到了法向量) xyz = xyz[:, :3, :] # xyz = [B, 3, N_0] else: norm = None l1_xyz, l1_features = self.sa1(xyz, norm) # l1_xyz = [B, 3, 512], l1_features = [B, 128, 512] l2_xyz, l2_features = self.sa2(l1_xyz, l1_features) # l2_xyz = [B, 3, 128], l1_features = [B, 256, 128] l3_xyz, l3_features = self.sa3(l2_xyz, l2_features) # l3_xyz = [B, 1, 3], l3_features = [B, 1, 1024] # 如果只是利用PointNet++提取特征的话，这样就可以了 x = l3_features.view(B, 1024) x = self.drop1(F.relu(self.bn1(self.fc1(x)))) # [B, 1024] -&gt; [B, 512] x = self.drop2(F.relu(self.bn2(self.fc2(x)))) # [B, 512] -&gt; [B, 256] x = self.fc3(x) # [B, 256] -&gt; [B, 40] x = F.log_softmax(x, -1) return x, l3_features ​ 接下来其实就是看set abstraction模块了，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class PointNetSetAbstraction(nn.Module): def __init__(self, npoint, radius, K, in_channel, mlp, group_all): super(PointNetSetAbstraction, self).__init__() self.npoint = npoint self.radius = radius self.K = K self.mlp_convs = nn.ModuleList() self.mlp_bns = nn.ModuleList() last_channel = in_channel for out_channel in mlp: self.mlp_convs.append(nn.Conv2d(last_channel, out_channel, 1)) # 卷积核大小为1 self.mlp_bns.append(nn.BatchNorm2d(out_channel)) last_channel = out_channel # 对于sa1来说，就是有 3个MLP层，每层的卷积核大小都是1*1 # 第一层 [B, 3+C_i, K_{i+1}, N_{i+1}] -&gt; [B, 64, K_{i+1}, N_{i+1}] # 第二层 [B, 64, K_{i+1}, N_{i+1}] -&gt; [B, 64, K_{i+1}, N_{i+1}] # 第三层 [B, 64, K_{i+1}, N_{i+1}] -&gt; [B, 128, K_{i+1}, N_{i+1}] self.group_all = group_all def forward(self, xyz, points): &quot;&quot;&quot; Input: xyz: input points position data, [B, 3, N_i] features: input points feature data, [B, C_i, N_i] Return: new_xyz: sampled points position data, [B, 3, N_{i+1}] new_features: sample points feature data, [B, C_{i+1}, N_{i+1}] &quot;&quot;&quot; xyz = xyz.permute(0, 2, 1) if features is not None: features = features.permute(0, 2, 1) if self.group_all: new_xyz, new_features = sample_and_group_all(xyz, features) # new_xyz = [B, 1, 3] # new_features = [B, 1, K_{i+1}, 3+C_i] else: new_xyz, new_features = sample_and_group(self.npoint, self.radius, self.K, xyz, features) # new_xyz: sampled points position data, [B, N_{i+1}, 3] # new_features: sampled points position and feature data, [B, N_{i+1}, K_{i+1}, 3+C_i] new_features_concat = new_points.permute(0, 3, 2, 1) # [B, 3+C_i, K_{i+1}, N_{i+1}] for i, conv in enumerate(self.mlp_convs): bn = self.mlp_bns[i] new_features_concat = F.relu(bn(conv(new_features_concat))) # [B, C_{i+1}, K_{i+1}, N_{i+1}] new_features_concat = torch.max(new_features_concat, 2)[0] # [B, C_{i+1}, K_{i+1}, N_{i+1}] -&gt; [B, C_{i+1}, N_{i+1}] # 其实就是说从邻域特征中找到一个最大响应的值，因为每次处理的邻域的半径不同，所以每次提取特征响应的感受野也不同，实现了不同尺度下的特征提取 new_xyz = new_xyz.permute(0, 2, 1) # [B, N_{i+1}, 3] -&gt; [B, 3, N_{i+1}] return new_xyz, new_features_concat ​ 对于set abstraction layer来说，它的输入是$B\\times N_{i} \\times (3+C_i)$，而它的输出是$B\\times N_{i+1}\\times(3+C_{i+1})$。 ​ 理论上这样我们的分类任务已经可以完成了，这也是SSG(single-scale grouping)的情况。我们需要再看一下论文所提出的MSG的set abstraction module是如何实现的。 ​ 我们可以看到models里总体架构没有变，唯一有区别的就是两个sa层变为了SA_MSG。 123self.sa1 = PointNetSetAbstractionMsg(npoint=512, radius_list=[0.1, 0.2, 0.4], K_list=[16, 32, 128], in_channel=in_channel, mlp_list=[[32, 32, 64], [64, 64, 128], [64, 96, 128]])self.sa2 = PointNetSetAbstractionMsg(npoint=128, radius_list=[0.2, 0.4, 0.8], K_list=[32, 64, 128], in_channel=320, mlp_list=[[64, 64, 128], [128, 128, 256], [128, 128, 256]])self.sa3 = PointNetSetAbstraction(npoint=None, radius=None, K=None, in_channel=640 + 3, mlp=[256, 512, 1024], group_all=True) ​ 其实我们只要搞明白上述的in_channel各自是怎么来的。以sa2的in_channel为例：$$\\text{in_channel}=320=\\Sigma_j(\\text{out_channel}_j)=64+128+128$$​ 这样我们就明白了，其实就是把不同的radius所提取的不同尺度的特征拼接在一起得到320维的特征向量。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667class PointNetSetAbstractionMsg(nn.Module): def __init__(self, npoint, radius_list, K_list, in_channel, mlp_list): super(PointNetSetAbstractionMsg, self).__init__() self.npoint = npoint self.radius_list = radius_list self.nsample_list = nsample_list self.conv_blocks = nn.ModuleList() self.bn_blocks = nn.ModuleList() for i in range(len(mlp_list)): convs = nn.ModuleList() bns = nn.ModuleList() last_channel = in_channel + 3 for out_channel in mlp_list[i]: convs.append(nn.Conv2d(last_channel, out_channel, 1)) bns.append(nn.BatchNorm2d(out_channel)) last_channel = out_channel self.conv_blocks.append(convs) self.bn_blocks.append(bns) def forward(self, xyz, points): &quot;&quot;&quot; Input: xyz: input points position data, [B, 3, N_i] points: input points data, [B, C_i, N_i] Return: new_xyz: sampled points position data, [B, 3, N_{i+1}] new_points_concat: sample points feature data, [B, C_{i+1}, N_{i+1}] &quot;&quot;&quot; xyz = xyz.permute(0, 2, 1) # [B, N_i, 3] if points is not None: points = points.permute(0, 2, 1) # [B, N, C_i] ,C_i就是每个点额外的特征向量 B, N, C = xyz.shape new_xyz = index_points(xyz, farthest_point_sample(xyz, self.npoint)) # [B, N_{i+1}, C_i] new_points_list = [] for j, radius in enumerate(self.radius_list): # 在不同的尺度下找ball query，枚举radius_list K = self.nsample_list[j] # 当前半径下，中心节点的邻居的数量 # ======================以下开始其实是sample_and_group的逻辑==================== group_idx = query_ball_point(radius, K, xyz, new_xyz) grouped_xyz = index_points(xyz, group_idx) # [B, 3, npoints, nsample] grouped_xyz -= new_xyz.view(B, S, 1, C) # ??? 转换成相对坐标 if points is not None: grouped_points = index_points(points, group_idx) grouped_points = torch.cat([grouped_points, grouped_xyz], dim=-1) else: grouped_points = grouped_xyz # =========================================================================== grouped_points = grouped_points.permute(0, 3, 2, 1) # [B, N_{i+1}, K_{j}, (3+D)] -&gt; [B, (3+D), K_{j}, N_{i+1}] for k in range(len(self.conv_blocks[j])): # 所以mlp_list是一个二维数组，每一行都代表在对应半径下使用的input_channels conv = self.conv_blocks[j][k] bn = self.bn_blocks[j][k] grouped_points = F.relu(bn(conv(grouped_points))) # 得到了 [B, out_channel_j, K_{j}, N_{i+1}] # 最终我们得到一个group_points, [B, out_channel_j, K_{j}, N_{i+1}] new_points = torch.max(grouped_points, 2)[0] # [B, out_channel_j, N_{i+1}] new_points_list.append(new_points) # 最终new_points_list为 len_radius_list 个 [B, out_channel_j, N_{i+1}] new_xyz = new_xyz.permute(0, 2, 1) new_points_concat = torch.cat(new_points_list, dim=1) # 最终得到 [B, sigma{out_channel_j}, N_{i+1}] print(&quot;new_points_concat.size = &quot;, new_points_concat) return new_xyz, new_points_concat Q：PointNet++梯度是如何回传的？？？ A：PointNet++ fps实际上并没有参与梯度计算和反向传播。 可以理解成是PointNet++将点云进行不同规模的fps降采样，事先将这些数据准备好，再送到网络中去训练。 VoteNet​ VoteNet是基于end-to-end的3D目标检测网络，它基于3D的深度点云网络和霍夫投票。 ​ 可以从讲解视频和PPT获得相关内容。 votenet.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970class VoteNet(nn.Module): r&quot;&quot;&quot; A deep neural network for 3D object detection with end-to-end optimizable hough voting. Parameters ---------- num_class: int Number of semantics classes to predict over -- size of softmax classifier num_heading_bin: int num_size_cluster: int input_feature_dim: (default: 0) Input dim in the feature descriptor for each point. If the point cloud is Nx9, this value should be 6 as in an Nx9 point cloud, 3 of the channels are xyz, and 6 are feature descriptors num_proposal: int (default: 128) Number of proposals/detections generated from the network. Each proposal is a 3D OBB with a semantic class. vote_factor: (default: 1) Number of votes generated from each seed point. &quot;&quot;&quot; def __init__(self, num_class, num_heading_bin, num_size_cluster, mean_size_arr, input_feature_dim=0, num_proposal=128, vote_factor=1, sampling='vote_fps'): super().__init__() ... # omit variable init ... # Backbone point feature learning self.backbone_net = Pointnet2Backbone(input_feature_dim=self.input_feature_dim) # Hough voting self.vgen = VotingModule(self.vote_factor, 256) # Vote aggregation and detection self.pnet = ProposalModule(num_class, num_heading_bin, num_size_cluster, mean_size_arr, num_proposal, sampling) def forward(self, inputs): &quot;&quot;&quot; Forward pass of the network Args: inputs: dict {point_clouds} point_clouds: Variable(torch.cuda.FloatTensor) (B, N, 3 + input_channels) tensor Point cloud to run predicts on Each point in the point-cloud MUST be formated as (x, y, z, features...) Returns: end_points: dict &quot;&quot;&quot; end_points = {} batch_size = inputs['point_clouds'].shape[0] end_points = self.backbone_net(inputs['point_clouds'], end_points) # --------- HOUGH VOTING --------- xyz = end_points['fp2_xyz'] # (B, M, 3) features = end_points['fp2_features'] # (B, M, 256) end_points['seed_inds'] = end_points['fp2_inds'] end_points['seed_xyz'] = xyz # (batch_size, num_seed, 3) end_points['seed_features'] = features # (batch_size, num_seed, 256) xyz, features = self.vgen(xyz, features) # xyz : (batch_size, num_vote, 3) # features : (batch_size, out_dim, num_vote) features_norm = torch.norm(features, p=2, dim=1) # features_norm : (batch_size, num_vote) features = features.div(features_norm.unsqueeze(1)) # features : (batch_size, out_dim, num_vote)，此时features已经归一化了特征 end_points['vote_xyz'] = xyz end_points['vote_features'] = features end_points = self.pnet(xyz, features, end_points) return end_points voting_module.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class VotingModule(nn.Module): def __init__(self, vote_factor, seed_feature_dim): &quot;&quot;&quot; Votes generation from seed point features. Args: vote_facotr: int number of votes generated from each seed point seed_feature_dim: int number of channels of seed point features vote_feature_dim: int number of channels of vote features &quot;&quot;&quot; super().__init__() self.vote_factor = vote_factor self.in_dim = seed_feature_dim self.out_dim = self.in_dim # due to residual feature, in_dim has to be == out_dim self.conv1 = torch.nn.Conv1d(self.in_dim, self.in_dim, 1) self.conv2 = torch.nn.Conv1d(self.in_dim, self.in_dim, 1) self.conv3 = torch.nn.Conv1d(self.in_dim, (3+self.out_dim) * self.vote_factor, 1) self.bn1 = torch.nn.BatchNorm1d(self.in_dim) self.bn2 = torch.nn.BatchNorm1d(self.in_dim) def forward(self, seed_xyz, seed_features): &quot;&quot;&quot; Forward pass. Arguments: seed_xyz: (batch_size, num_seed, 3) seed_features: (batch_size, feature_dim, num_seed) Returns: vote_xyz: (batch_size, num_seed*vote_factor, 3) vote_features: (batch_size, vote_feature_dim, num_seed*vote_factor) &quot;&quot;&quot; batch_size = seed_xyz.shape[0] num_seed = seed_xyz.shape[1] num_vote = num_seed * self.vote_factor net = F.relu(self.bn1(self.conv1(seed_features))) net = F.relu(self.bn2(self.conv2(net))) net = self.conv3(net) # (batch_size, feature_dim, num_seed) =&gt; (batch_size, (3+out_dim)*vote_factor, num_seed) net = net.transpose(2,1).view(batch_size, num_seed, self.vote_factor, 3+self.out_dim) # (batch_size, num_seed, vote_factor, 3+self.out_dim) # 对于每个seed，生成vote_factor个vote，每个vote的特征维度为3+self.out_dim（3+256） offset = net[:,:,:,0:3] # (batch_size, num_seed, vote_factor, 3) vote_xyz = seed_xyz.unsqueeze(2) + offset # (batch_size, num_seed, 1, 3) =&gt; (batch_size, num_seed, vote_factor, 3) vote_xyz = vote_xyz.contiguous().view(batch_size, num_vote, 3) # (batch_size, num_vote, 3) residual_features = net[:,:,:,3:] # seed_features: (batch_size, feature_dim, num_seed) vote_features = seed_features.transpose(2,1).unsqueeze(2) + residual_features # (batch_size, num_seed, feature_dim) =&gt; # (batch_size, num_seed, vote_factor, out_dim) vote_features = vote_features.contiguous().view(batch_size, num_vote, self.out_dim) vote_features = vote_features.transpose(2,1).contiguous() # (batch_size, out_dim, num_vote) return vote_xyz, vote_features proposal_module.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105class ProposalModule(nn.Module): def __init__(self, num_class, num_heading_bin, num_size_cluster, mean_size_arr, num_proposal, sampling, seed_feat_dim=256): super().__init__() ... # omit variable init ... # Vote clustering self.vote_aggregation = PointnetSAModuleVotes( npoint=self.num_proposal, radius=0.3, nsample=16, mlp=[self.seed_feat_dim, 128, 128, 128], use_xyz=True, normalize_xyz=True ) # Object proposal/detection # Objectness scores (2), center residual (3), # heading class+residual (num_heading_bin*2), size class+residual(num_size_cluster*4) self.conv1 = torch.nn.Conv1d(128,128,1) self.conv2 = torch.nn.Conv1d(128,128,1) self.conv3 = torch.nn.Conv1d(128,2 + 3 + num_heading_bin * 2 + num_size_cluster * 4 + self.num_class,1) self.bn1 = torch.nn.BatchNorm1d(128) self.bn2 = torch.nn.BatchNorm1d(128) def forward(self, xyz, features, end_points): &quot;&quot;&quot; Args: xyz: (batch_size, num_vote, 3) features: (batch_size, out_dim, num_vote) Returns: scores: (batch_size, num_proposal, 2 + 3 + NH * 2 + NS * 4) &quot;&quot;&quot; if self.sampling == 'vote_fps': # Farthest point sampling (FPS) on votes xyz, features, fps_inds = self.vote_aggregation(xyz, features) sample_inds = fps_inds elif self.sampling == 'seed_fps': # FPS on seed and choose the votes corresponding to the seeds # This gets us a slightly better coverage of *object* votes than vote_fps (which tends to get more cluster votes) sample_inds = pointnet2_utils.furthest_point_sample(end_points['seed_xyz'], self.num_proposal) xyz, features, _ = self.vote_aggregation(xyz, features, sample_inds) elif self.sampling == 'random': # Random sampling from the votes num_seed = end_points['seed_xyz'].shape[1] batch_size = end_points['seed_xyz'].shape[0] sample_inds = torch.randint(0, num_seed, (batch_size, self.num_proposal), dtype=torch.int).cuda() # (batch_size, num_proposal) xyz, features, _ = self.vote_aggregation(xyz, features, sample_inds) # xyz : (batch_size, num_proposal, 3) # features : (batch_size, 128, num_proposal) end_points['aggregated_vote_xyz'] = xyz # (batch_size, num_proposal, 3) end_points['aggregated_vote_inds'] = sample_inds # (batch_size, num_proposal,) # should be 0,1,2,...,num_proposal # --------- PROPOSAL GENERATION --------- net = F.relu(self.bn1(self.conv1(features))) net = F.relu(self.bn2(self.conv2(net))) net = self.conv3(net) # (batch_size, 2 + 3 + num_heading_bin * 2 + num_size_cluster * 4, num_proposal) end_points = decode_scores(net, end_points, self.num_class, self.num_heading_bin, self.num_size_cluster, self.mean_size_arr) return end_points def decode_scores(net, end_points, num_class, num_heading_bin, num_size_cluster, mean_size_arr): net_transposed = net.transpose(2,1) # (batch_size, num_proposal, 2 + 3 + num_heading_bin * 2 + num_size_cluster * 4) batch_size = net_transposed.shape[0] num_proposal = net_transposed.shape[1] objectness_scores = net_transposed[:,:,0:2] end_points['objectness_scores'] = objectness_scores # (batch_size, num_proposal, 2) base_xyz = end_points['aggregated_vote_xyz'] # (batch_size, num_proposal, 3) center = base_xyz + net_transposed[:, :, 2:5] # (batch_size, num_proposal, 3) end_points['center'] = center heading_scores = net_transposed[:, :, 5:5 + num_heading_bin] end_points['heading_scores'] = heading_scores # (batch_size, num_proposal, num_heading_bin) heading_residuals_normalized = net_transposed[:, :, 5 + num_heading_bin:5 + num_heading_bin * 2] end_points['heading_residuals_normalized'] = heading_residuals_normalized # (batch_size, num_proposal, num_heading_bin) (should be -1 to 1) end_points['heading_residuals'] = heading_residuals_normalized * (np.pi / num_heading_bin) # 因为residual的每个值在-1到1 # 所以这样做完以后heading_residuals的值就在[-np.pi / num_heading_bin, np.pi / num_heading_bin] size_scores = net_transposed[:, :, 5 + num_heading_bin * 2:5 + num_heading_bin * 2 + num_size_cluster] end_points['size_scores'] = size_scores # (batch_size, num_proposal, num_size_cluster) size_residuals_normalized = net_transposed[:, :, 5 + num_heading_bin * 2 + num_size_cluster:5 + num_heading_bin * 2 + num_size_cluster * 4].view([batch_size, num_proposal, num_size_cluster, 3]) end_points['size_residuals_normalized'] = size_residuals_normalized # (batch_size, num_proposal, num_size_cluster * 3) end_points['size_residuals'] = size_residuals_normalized * torch.from_numpy(mean_size_arr.astype(np.float32)).cuda().unsqueeze(0).unsqueeze(0) sem_cls_scores = net_transposed[:,:,5 + num_heading_bin * 2 + num_size_cluster * 4:] end_points['sem_cls_scores'] = sem_cls_scores # (batch_size, num_proposal, 10) return end_points ​ 在默认情况下，我们decode_scores对256个检测框每个输出12个heading分类和12个heading的res，10个size分类和10个size的res，3个中心点的坐标，2个代表有无目标，以及10类分类的置信度。 loss_helper.py​ VoteNet的总的Loss由vote_loss, objectness_loss, box loss, sem cls loss组成。 1234567891011121314151617181920212223242526272829303132333435363738394041def get_loss(end_points, config): # Loss functions # Vote loss vote_loss = compute_vote_loss(end_points) end_points['vote_loss'] = vote_loss # Obj loss objectness_loss, objectness_label, objectness_mask, object_assignment = compute_objectness_loss(end_points) end_points['objectness_loss'] = objectness_loss end_points['objectness_label'] = objectness_label end_points['objectness_mask'] = objectness_mask end_points['object_assignment'] = object_assignment total_num_proposal = objectness_label.shape[0]*objectness_label.shape[1] end_points['pos_ratio'] = torch.sum(objectness_label.float().cuda())/float(total_num_proposal) end_points['neg_ratio'] = torch.sum(objectness_mask.float())/float(total_num_proposal) - end_points['pos_ratio'] # Box loss and sem cls loss center_loss, heading_cls_loss, heading_reg_loss, size_cls_loss, size_reg_loss, sem_cls_loss = \\ compute_box_and_sem_cls_loss(end_points, config) end_points['center_loss'] = center_loss end_points['heading_cls_loss'] = heading_cls_loss end_points['heading_reg_loss'] = heading_reg_loss end_points['size_cls_loss'] = size_cls_loss end_points['size_reg_loss'] = size_reg_loss end_points['sem_cls_loss'] = sem_cls_loss box_loss = center_loss + 0.1*heading_cls_loss + heading_reg_loss + 0.1*size_cls_loss + size_reg_loss end_points['box_loss'] = box_loss # Final loss function loss = vote_loss + 0.5*objectness_loss + box_loss + 0.1*sem_cls_loss loss *= 10 end_points['loss'] = loss # -------------------------------------------- # Some other statistics obj_pred_val = torch.argmax(end_points['objectness_scores'], 2) # B,K obj_acc = torch.sum((obj_pred_val==objectness_label.long()).float()*objectness_mask)/(torch.sum(objectness_mask)+1e-6) end_points['obj_acc'] = obj_acc return loss, end_points vote loss12345678910111213141516171819202122232425262728293031323334353637def compute_vote_loss(end_points): &quot;&quot;&quot; Compute vote loss: Match predicted votes to GT votes. Overall idea: 如果我们的seed point属于一个物体（votes_label_mask == 1），那么我们需要它向着物体中心投票 每个seed point可能投票出多个translation v1,v2,v3 一个seed point也可能在多个物体o1,o2,o3的bounding box中，对应的GT vote为 c1, c2, c3 对于这个seed point的loss为： min(d(v_i,c_j)) for i=1,2,3 and j=1,2,3 &quot;&quot;&quot; # Load ground truth votes and assign them to seed points batch_size = end_points['seed_xyz'].shape[0] num_seed = end_points['seed_xyz'].shape[1] # (B, num_seed, 3) vote_xyz = end_points['vote_xyz'] # (B, num_seed * vote_factor, 3) seed_inds = end_points['seed_inds'].long() # (B, num_seed) in [0,num_points-1] # Get groundtruth votes for the seed points # vote_label_mask: Use gather to select B,num_seed from B,num_point # non-object point has no GT vote mask = 0, object point has mask = 1 # vote_label: Use gather to select B,num_seed,9 from B,num_point,9 # with inds in shape B,num_seed,9 and 9 = GT_VOTE_FACTOR * 3 seed_gt_votes_mask = torch.gather(end_points['vote_label_mask'], 1, seed_inds) seed_inds_expand = seed_inds.view(batch_size,num_seed,1).repeat(1, 1, 3 * GT_VOTE_FACTOR) seed_gt_votes = torch.gather(end_points['vote_label'], 1, seed_inds_expand) seed_gt_votes += end_points['seed_xyz'].repeat(1, 1, 3) # Compute the min of min of distance vote_xyz_reshape = vote_xyz.view(batch_size*num_seed, -1, 3) # (B, num_seed * vote_factor, 3) =&gt; (B * num_seed, vote_factor, 3) seed_gt_votes_reshape = seed_gt_votes.view(batch_size*num_seed, GT_VOTE_FACTOR, 3) # (B, num_seed, 3 * GT_VOTE_FACTOR) =&gt; (B * num_seed, GT_VOTE_FACTOR, 3) # A predicted vote to no where is not penalized as long as there is a good vote near the GT vote. dist1, _, dist2, _ = nn_distance(vote_xyz_reshape, seed_gt_votes_reshape, l1=True) votes_dist, _ = torch.min(dist2, dim=1) votes_dist = votes_dist.view(batch_size, num_seed) # (B * num_seed, vote_factor) =&gt; (B * num_seed,) =&gt; (B, num_seed) vote_loss = torch.sum(votes_dist * seed_gt_votes_mask.float()) / (torch.sum(seed_gt_votes_mask.float()) + 1e-6) return vote_loss objectness loss12345678910111213141516171819202122232425262728293031323334353637383940def compute_objectness_loss(end_points): &quot;&quot;&quot; Compute objectness loss for the proposals. Args: end_points: dict (read-only) Returns: objectness_loss: scalar Tensor objectness_label: (batch_size, num_seed) Tensor with value 0 or 1 objectness_mask: (batch_size, num_seed) Tensor with value 0 or 1 object_assignment: (batch_size, num_seed) Tensor with long int within [0,num_gt_object-1] &quot;&quot;&quot; # Associate proposal and GT objects by point-to-point distances aggregated_vote_xyz = end_points['aggregated_vote_xyz'] gt_center = end_points['center_label'][:,:,0:3] B = gt_center.shape[0] K = aggregated_vote_xyz.shape[1] K2 = gt_center.shape[1] dist1, ind1, dist2, _ = nn_distance(aggregated_vote_xyz, gt_center) # dist1: BxK, dist2: BxK2 # Generate objectness label and mask # objectness_label: 1 if pred object center is within NEAR_THRESHOLD of any GT object # objectness_mask: 0 if pred object center is in gray zone (DONOTCARE), 1 otherwise euclidean_dist1 = torch.sqrt(dist1+1e-6) objectness_label = torch.zeros((B,K), dtype=torch.long).cuda() objectness_mask = torch.zeros((B,K)).cuda() objectness_label[euclidean_dist1&lt;NEAR_THRESHOLD] = 1 objectness_mask[euclidean_dist1&lt;NEAR_THRESHOLD] = 1 objectness_mask[euclidean_dist1&gt;FAR_THRESHOLD] = 1 # Compute objectness loss objectness_scores = end_points['objectness_scores'] criterion = nn.CrossEntropyLoss(torch.Tensor(OBJECTNESS_CLS_WEIGHTS).cuda(), reduction='none') objectness_loss = criterion(objectness_scores.transpose(2,1), objectness_label) objectness_loss = torch.sum(objectness_loss * objectness_mask)/(torch.sum(objectness_mask)+1e-6) # Set assignment object_assignment = ind1 # (B,K) with values in 0,1,...,K2-1 return objectness_loss, objectness_label, objectness_mask, object_assignment box and sem cls loss论文中的描述如下： 1The max-pooled features are further processed by MLP2 with output sizes of 128, 128, 5+2NH+4NS+NC where the output consists of 2 objectness scores, 3 center regression values, 2NH numbers for heading regression (NH heading bins) and 4NS numbers for box size regression (NS box anchors) and NC numbers for semantic classification ​ 其实，根据VoteNet引用的Frustum PointNet所提到的，两篇文章都只考虑了Up-axis轴上的角度作为heading angle，对于3D Bounding Box做了2个自由度的简化。那么其实就很容易懂了，把180度分成12份，先预测在哪个bin中，再回归bin内的偏移量是多少。而size bin以及size residual是类似的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586def compute_box_and_sem_cls_loss(end_points, config): &quot;&quot;&quot; Compute 3D bounding box and semantic classification loss. Args: end_points: dict (read-only) Returns: center_loss heading_cls_loss heading_reg_loss size_cls_loss size_reg_loss sem_cls_loss &quot;&quot;&quot; num_heading_bin = config.num_heading_bin num_size_cluster = config.num_size_cluster num_class = config.num_class mean_size_arr = config.mean_size_arr object_assignment = end_points['object_assignment'] batch_size = object_assignment.shape[0] # Compute center loss # 因为是K个点，K2个物体，我们把Loss分成两部分 # centroid_reg_loss1 (B, K)： 距离每个点最近的GT物体的距离 # centroid_reg_loss2 (B, K1)： 距离每个GT物体最近的center的距离 # 这样的话就把对应关系考虑进去了 pred_center = end_points['center'] gt_center = end_points['center_label'][:, :, 0:3] dist1, ind1, dist2, _ = nn_distance(pred_center, gt_center) # dist1: BxK, dist2: BxK2 box_label_mask = end_points['box_label_mask'] objectness_label = end_points['objectness_label'].float() centroid_reg_loss1 = torch.sum(dist1 * objectness_label) / (torch.sum(objectness_label) + 1e-6) centroid_reg_loss2 = torch.sum(dist2 * box_label_mask) / (torch.sum(box_label_mask) + 1e-6) center_loss = centroid_reg_loss1 + centroid_reg_loss2 # Compute heading loss heading_class_label = torch.gather(end_points['heading_class_label'], 1, object_assignment) # select (B,K) from (B,K2)，得到每个点的GT heading class # end_points['heading_scores'].transpose(2,1): (batch_size, num_heading_bin, K) criterion_heading_class = nn.CrossEntropyLoss(reduction='none') heading_class_loss = criterion_heading_class(end_points['heading_scores'].transpose(2,1), heading_class_label) # (B,K) heading_class_loss = torch.sum(heading_class_loss * objectness_label) / (torch.sum(objectness_label) + 1e-6) # 这里就是对heading bin做了多分类，并且在Loss和的时候只考虑了objectness_label == 1的情况，并且归一化 heading_residual_label = torch.gather(end_points['heading_residual_label'], 1, object_assignment) # select (B,K) from (B,K2) heading_residual_normalized_label = heading_residual_label / (np.pi/num_heading_bin) # Ref: https://discuss.pytorch.org/t/convert-int-into-one-hot-format/507/3 heading_label_one_hot = torch.cuda.FloatTensor(batch_size, heading_class_label.shape[1], num_heading_bin).zero_() heading_label_one_hot.scatter_(2, heading_class_label.unsqueeze(-1), 1) # src==1 so it's *one-hot* (B,K,num_heading_bin) heading_residual_normalized_loss = huber_loss(torch.sum(end_points['heading_residuals_normalized'] * heading_label_one_hot, -1) - heading_residual_normalized_label, delta=1.0) # (B,K) heading_residual_normalized_loss = torch.sum(heading_residual_normalized_loss * objectness_label) / (torch.sum(objectness_label)+1e-6) # Compute size loss size_class_label = torch.gather(end_points['size_class_label'], 1, object_assignment) # select (B,K) from (B,K2) criterion_size_class = nn.CrossEntropyLoss(reduction='none') size_class_loss = criterion_size_class(end_points['size_scores'].transpose(2,1), size_class_label) # (B,K) size_class_loss = torch.sum(size_class_loss * objectness_label) / (torch.sum(objectness_label) + 1e-6) size_residual_label = torch.gather(end_points['size_residual_label'], 1, object_assignment.unsqueeze(-1).repeat(1,1,3)) # select (B,K,3) from (B,K2,3) size_label_one_hot = torch.cuda.FloatTensor(batch_size, size_class_label.shape[1], num_size_cluster).zero_() size_label_one_hot.scatter_(2, size_class_label.unsqueeze(-1), 1) # src==1 so it's *one-hot* (B,K,num_size_cluster) size_label_one_hot_tiled = size_label_one_hot.unsqueeze(-1).repeat(1,1,1,3) # (B,K,num_size_cluster,3) predicted_size_residual_normalized = torch.sum(end_points['size_residuals_normalized']*size_label_one_hot_tiled, 2) # (B,K,3) mean_size_arr_expanded = torch.from_numpy(mean_size_arr.astype(np.float32)).cuda().unsqueeze(0).unsqueeze(0) # (1,1,num_size_cluster,3) mean_size_label = torch.sum(size_label_one_hot_tiled * mean_size_arr_expanded, 2) # (B,K,3) size_residual_label_normalized = size_residual_label / mean_size_label # (B,K,3) size_residual_normalized_loss = torch.mean(huber_loss(predicted_size_residual_normalized - size_residual_label_normalized, delta=1.0), -1) # (B,K,3) -&gt; (B,K) size_residual_normalized_loss = torch.sum(size_residual_normalized_loss * objectness_label) / (torch.sum(objectness_label) + 1e-6) # Compute Semantic cls loss，此处就是算预测的多分类交叉熵 sem_cls_label = torch.gather(end_points['sem_cls_label'], 1, object_assignment) # select (B,K) from (B,K2) criterion_sem_cls = nn.CrossEntropyLoss(reduction='none') sem_cls_loss = criterion_sem_cls(end_points['sem_cls_scores'].transpose(2,1), sem_cls_label) # (B,K) sem_cls_loss = torch.sum(sem_cls_loss * objectness_label) / (torch.sum(objectness_label) + 1e-6) return center_loss, heading_class_loss, heading_residual_normalized_loss, size_class_loss, size_residual_normalized_loss, sem_cls_loss","link":"/blog/2021/12/22/pointnet-and-related-works/"},{"title":"(CVPR2020)GraspNet-1Billion","text":"​ 最近在研究各种各样的GraspNet，组内浩树学长的GraspNet虽然我已经用了很久，但是在方法论和具体实现上一直没有时间去研究，借着寒假的机会来集中学习一下。 ​ 其实GraspNet-1Billion包含三部分，大规模的数据集、网络和benchmark。 数据集部分​ 数据集包含88个日常常见的物体的3D模型。数据是从190个clutter scene中收集的，每个scene中，2个深度摄像机拍512张深度图，共97280张图片。每张图片中，我们通过计算force closure的方法稠密标注了6D grasp pose。每个场景中通常会有300万到900万个Grasp Pose。 数据收集​ 我原以为是仿真器里渲染的，居然真的是在现实世界中采集的，这样的话就不存在什么sim2real的问题了。两个深度相机同时拍摄场景并且合成成一个点云。摄像机所固连的机械臂的末端执行器在单位球面上找到了256个位置拍摄。地面上还放置了ArUco marker来协助摄像机标定，这样避免了计算fk所带来的误差。 数据标定​ 有了这97280张图以后，因为每256个位置的相对位置都是已知的，所以我们只需要标定每个的第一帧即可，即380张。但是Grasp Pose分布在一个大型的连续搜索空间中，是标不完的，手动标是巨大的工作量。因为我们的每个物体是已知的，文章中提出了一个2阶段的Grasp Pose标注方法。 ​ 首先，我们在单个物体上采样并标注Grasp Pose。为了达到这个目的，我们先把单个物体的网格模型降采样成均匀分布的Voxel space，其中的每个点称为grasp point。对于每个Grasp Point，我们从单位球找到V个均匀分布的approaching vector。然后，我们在二维网格$D\\times A$上搜索（其中D是夹爪深度，而A是in-plane旋转角度）。 ​ 我们使用理论计算的方法来对每个Grasp打分，force-closure指标是很有用的：给定一个Grasp Pose、相关的Object以及摩擦系数$\\mu$，它可以输出一个二分类的标签来判断这个Grasp是否可以在对应的摩擦系数下被抓起来。因为force-closure是基于物理的，所以比较鲁棒。此处我们采用24所提到的一个改进版本，使用$\\Delta\\mu=0.1$作为间隔，我们逐渐从1递减到0.1，直到Grasp不再antipodal。因为具有更低摩擦系数$\\mu$的成功抓取更容易成功，所以我们定义Grasp的成功率为：$s=1.1-\\mu$。 ​ 对于每个scene，我们把对应单个物体的Grasp投影到clutter的场景中，此外还做了collision-check来避免非法的情况。在这两部以后，我们就对每个scene创建了稠密的抓取集合$G_{(w)}$。根据统计，数据集中正例和反例的比例为1:2。 数据集评估​ 对于190个场景，100个用来训练、90个用来测试。我们进一步把测试集分类为3种： 30 scenes with seen objects, 30 with unseen but similar objects and 30 for novel objects。 ​ 为了评估Grasp Pose的预测表现，之前的方法通常认为一个正确的Grasp需要满足： rotation error小于$30^{\\circ}$ 矩形的IOU（Intersection over Union）大于0.25 ​ 但是这样的度量指标有一些问题，比如它只能评估Grasp Pose的矩阵表示方法，并且它的错误容忍度太高了，康奈尔数据集已经可以达到99%的进度了。这篇工作提出了一个在线的评估算法来评估Grasp的精度。 ​ 我们首先演示如何来分类是否一个Grasp Pose是true positive的。对于每个预测出来的Grasp Pose$\\hat{P}_i$，我们首先做collision-checking，第二部就是我们通过force-closure来判断在给定不同的摩擦系数$\\mu$下是否可以抓取。 ​ 对于cluttered scene，我们的Grasp预测算法会预测出多个Grasp Pose。因为对于抓取来说，我们通常是预测以后再执行的，所以我们认为true positive的比例是最重要的。因此，我们采用Precision@k作为我们的评估指标，也就是前k个抓取的精度。$AP_\\mu$在摩擦系数$\\mu$代表Precision@k的均值（k从1取到50）。和COCO数据集类似，我们在不同的摩擦系数$\\mu$来计算$AP_\\mu$。为了避免相同的Grasp Pose，在评估之前，我们对Grasp Pose来使用pose-NMS。 ​ 网络结构整体上我觉得和VoteNet非常接近，也是生成M个seeds。然后通过ApproachNet把Grasp的接近向量做一个多分类出来。然后我们对这M个grasp proposal做KNN聚类，聚出来K个grasp proposal。对于每个接近向量，我们继续做多分类，在$D\\times A$上找到一个分数最大的夹爪渐进距离和角度。 Loss Function​ 对于每个候选点，我们为它分配一个二分类标签来表示它是否是可以抓取的。 对于那些不在物体上的点，我们直接认为是失败的样本。 对于在物体上的点，我们在5mm半径内寻找它是否存在至少一个graspable的GT。如果存在，对应的graspable标签即为1。 如果物体上的点附近5mm半径内没有graspable的GT，那么我们忽略掉这种情况，因为对我们的训练没有贡献。 ​ 对于每个点，我们有V个approaching vector，我们定义第i个点的第j个approaching vector为$v_{ij}$。我们然后寻找它对应的GT向量：$\\hat{v}_{ij}$。类似地，我们只考虑相差在5度以内的。最终，我们的损失函数定义如下：$$L^A({c_i},{s_{ij}})=\\frac{1}{N_{cls}}\\sum_iL_{cls}(c_i,c_i^*)+\\lambda_1\\frac{1}{N_{reg}}\\sum_i\\sum_j c_i^*\\mathbf{1}(|v_{ij},v_{ij}^*|&lt;5^{\\circ})L_{reg}(s_{ij},s_{ij}^*))$$​ 其中$s_{ij}$代表网络预测的第i个点的confidence score。而$s_{ij}^*$是对应的GT，是通过EQ2选出来的最大的grasp的置信度。$|v_{ij},v^*_{ij}|$代表的就是角度差。指示函数$\\mathbf{1}()$约束了一个approaching vector所产生的loss是通过它附近5度内的GT所提供的。 ​ 这个损失函数前半部分就是说，希望对每个点可以正确地二分类到graspable还是not graspable。后半部分就是说，对于每个graspable的点（满足$c_i^*==1$），它存在V个approaching vector，我们希望通过稠密的Grasp GT来让每个approaching vector都能预测出一个confidence score，也就是对应Grasp数据集创建时候，所计算出来的成功率。 Operation Network​ 在得到了graspable points的approaching vector之后，我们需要进一步预测in-plane rotation, approaching distance, gripper width和grasp confidence。 Cylinder Region Transformation​ 文章提出了一个统一的Grasp表示方式。因为approaching distance相对不那么敏感，所以分为了K个bin，对于每个给定的distance $d_k$，我们在圆柱中沿着approaching vector采样一些点。这些采样的点会转化到一个新的坐标系下，其原点是Grasp point，而z轴就是approaching vector $v_{ij}$。 Rotation和Width​ 在之前的文章中，证明了预测in-plane rotation时，分类比起回归有更好的效果。所以，rotation network把对齐过的点云作为输入，输出分类的分数、对每个rotation bin的归一化过的残差，以及对应的grasp width和grasp confidence。因为夹爪是对称的，所以我们只需要预测0~180度即可。目标函数如下：$$L^R(R_{ij},S_{ij},{W_{ij}})=\\sum_{d=1}^K\\left(\\frac{1}{N_{cls}}\\sum_{ij}L^d_{cls}(R_{ij},R_{ij}^*)+\\lambda_2\\frac{1}{N_{reg}}\\sum_{ij}L^d_{reg}(S_{ij},S^*_{ij})+\\lambda_3\\frac{1}{N_{reg}}\\sum_{ij}L^d_{reg}(W_{ij}, W^*_{ij})\\right)$$​ 其中$R_{ij}$代表binned rotation degree, $S_{ij}$代表grasp confidence score，$W_{ij}$代表夹爪闭合宽度，$d$代表approaching distance。其中$L^d$代表的是第d个binned distance的loss。此处的$L_{cls}$代表的是多分类任务的交叉熵损失。 Tolerance Network​ 现在我们已经有了一个end-to-end的网络了。本文进一步提出了Grasp Affinity Field（GAFs）的概念，它可以提升预测出来的Grasp的鲁棒性。因为合法的Grasp Pose是无限的，我们希望能够挑选出那些可以容忍更大error的鲁棒的Grasp。所以，GAFs就学习的是每个grasp对于扰动的鲁棒性。 ​ 给定一个GT的grasp pose，我们在球空间中搜索它的邻域，来找到满足grasp score &gt; 0.5的最远的距离作为GATs。损失函数如下：$$L^F(A_{ij})=\\frac{1}{N_{reg}}\\sum_{d=1}^K\\sum_{ij}L^d_{reg}(T_{ij},T_{ij}^*)$$​ 其中$T_{ij}$代表了grasp pose可以忍受的最大的扰动。 ​ 在训练过程中，网络的总的目标函数如下$$L=L^A({c_i},{s_{ij}})+\\alpha L^R(R_{ij}, S_{ij}, W_{ij})+\\beta L^F(T_{ij})$$​ 在推理阶段，我们把grasp根据分数分成10个bin，然后在每个bin中根据tolerance network的计算出来的扰动程度来排序。 ExperimentsGT Evaluation​ 为了评估预测的Grasp Pose，本文设立了一个真机实验，因为在现实中需要获取到物体的6D Pose才能做投影，把ArUco code贴在物体上。 ​ ​ 对于训练过程，我们把in-plane rotational angle分成了12个bin、approaching distance分成了4个bin(0.01, 0.02, 0.03, 0.04)m。我们设置$M=1024$和$V=300$。我们的ApproachNet有MLP(256, 302, 302)，OperationNet有MLP(128, 128, 36)和ToleranceNet有MLP(128, 64, 12)。 具体代码实现数据集格式​ 数据集的官网说明如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869701. Download, unzip all the files and place them in the following structure, the train images and test images contain the 190 scenes in total.|-- graspnet |-- scenes | |-- scene_0000/ | |-- scene_0001/ | |-- ... ... | `-- scene_0189/ | | |-- models | |-- 000/ | |-- 001/ | |-- ... | `-- 087/ | | |-- dex_models(optional but strongly recommended for accelerating evaluation) | |-- 000.pkl | |-- 001.pkl | |-- ... | `-- 087.pkl | | |-- grasp_label | |-- 000_labels.npz | |-- 001_labels.npz | |-- ... | `-- 087_labels.npz | | `-- collision_label |-- scene_0000/ |-- scene_0001/ |-- ... ... `-- scene_0189/2. Detail structure of each scene|-- scenes |-- scene_0000 | |-- object_id_list.txt # objects' id that appear in this scene, 0-indexed | |-- rs_wrt_kn.npy # realsense camera pose with respect to kinect, shape: 256x(4x4) | |-- kinect # data of kinect camera | | |-- rgb | | | |-- 0000.png to 0255.png # 256 rgb images | | `-- depth | | | |-- 0000.png to 0255.png # 256 depth images | | `-- label | | | |-- 0000.png to 0255.png # 256 object mask images, 0 is background, 1-88 denotes each object (1-indexed), same format as YCB-Video dataset | | `-- annotations | | | |-- 0000.xml to 0255.xml # 256 object 6d pose annotation. ‘pos_in_world' and'ori_in_world' denotes position and orientation w.r.t the camera frame. | | `-- meta | | | |-- 0000.mat to 0255.mat # 256 object 6d pose annotation, same format as YCB-Video dataset for easy usage | | `-- rect | | | |-- 0000.npy to 0255.npy # 256 2D planar grasp labels | | | | | `-- camK.npy # camera intrinsic, shape: 3x3, [[f_x,0,c_x], [0,f_y,c_y], [0,0,1]] | | `-- camera_poses.npy # 256 camera poses with respect to the first frame, shape: 256x(4x4) | | `-- cam0_wrt_table.npy # first frame's camera pose with respect to the table, shape: 4x4 | | | `-- realsense | |-- same structure as kinect | | `-- scene_0001 | `-- ... ... | `-- scene_0189 ​ 我们可以看到190个场景，每个都有对应的256张RGB, depth，mask，以及每个场景中10个物体的id、Pose、以及两个摄像机的外参矩阵。 ​ 而Grasp Label的格式可以通过API官网找到，注意到是88种物体，每一个都有一组Grasp Label。 12345678910111213141516&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; l = np.load('000_labels.npz') # GRASPNET_ROOT/grasp_label/000_labels.npz&gt;&gt;&gt; l.files['points', 'offsets', 'collision', 'scores']&gt;&gt;&gt; l['points'].shape(3459, 3)&gt;&gt;&gt; l['offsets'].shape(3459, 300, 12, 4, 3)&gt;&gt;&gt; l['collision'].shape(3459, 300, 12, 4)&gt;&gt;&gt; l['collision'].dtypedtype('bool')&gt;&gt;&gt; l['scores'].shape(3459, 300, 12, 4)&gt;&gt;&gt; l['scores'][0][0][0][0]-1.0 [‘points’] 记录了模型坐标系下的Grasp Center Point。 [‘offsets’] 记录了对应Grasp的in-plane rotation，夹爪深度和夹爪宽度。 [‘collision’] 记录了对应Grasp是否和物体模型存在碰撞。 [‘scores’] 记录了达到稳定Grasp时最小的摩擦系数。 ​ 还有一个比较重要的数据就是每个场景的collision_label，在官网中也被成为Collision Masks on Each Scene。具体地，因为我们每个场景中维护了物体的6D Pose，我们是知道每个Grasp的位置在哪里的，我们可以预先地把我们的夹爪模型放上去做碰撞检测。如果Gripper和场景中的Model有碰撞，对于的collision_label就设置为True，我们要在训练的时候把存在碰撞的Grasp Pose的score设置为0。 12345678910&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; c = np.load('collision_labels.npz') # GRASPNET_ROOT/collision_label/scene_0000/collision_labels.npz&gt;&gt;&gt; c.files['arr_0', 'arr_4', 'arr_5', 'arr_2', 'arr_3', 'arr_7', 'arr_1', 'arr_8', 'arr_6']&gt;&gt;&gt; c['arr_0'].shape(487, 300, 12, 4)&gt;&gt;&gt; c['arr_0'].dtypedtype('bool')&gt;&gt;&gt; c['arr_0'][10][20][3]array([ True, True, True, True]) DataLoader123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167class GraspNetDataset(Dataset): def __init__(self, root, valid_obj_idxs, grasp_labels, camera='kinect', split='train', num_points=20000, remove_outlier=False, remove_invisible=True, augment=False, load_label=True): assert(num_points&lt;=50000) # ignore some self.x = x if split == 'train': self.sceneIds = list( range(100) ) elif split == 'test': self.sceneIds = list( range(100,190) ) elif split == 'test_seen': self.sceneIds = list( range(100,130) ) elif split == 'test_similar': self.sceneIds = list( range(130,160) ) elif split == 'test_novel': self.sceneIds = list( range(160,190) ) self.sceneIds = ['scene_{}'.format(str(x).zfill(4)) for x in self.sceneIds] # ignore some dir concat def scene_list(self): return self.scenename def __len__(self): return len(self.depthpath) def __getitem__(self, index): if self.load_label: return self.get_data_label(index) else: return self.get_data(index) def get_data(self, index, return_raw_cloud=False): # 仅读入depth, pointcloud, mask，不读入grasp_label， color = np.array(Image.open(self.colorpath[index]), dtype=np.float32) / 255.0 depth = np.array(Image.open(self.depthpath[index])) seg = np.array(Image.open(self.labelpath[index])) meta = scio.loadmat(self.metapath[index]) scene = self.scenename[index] intrinsic = meta['intrinsic_matrix'] factor_depth = meta['factor_depth'] camera = CameraInfo(1280.0, 720.0, intrinsic[0][0], intrinsic[1][1], intrinsic[0][2], intrinsic[1][2], factor_depth) # 合成点云 cloud = create_point_cloud_from_depth_image(depth, camera, organized=True) # 得到两个mask depth_mask = (depth &gt; 0) seg_mask = (seg &gt; 0) if self.remove_outlier: camera_poses = np.load(os.path.join(self.root, 'scenes', scene, self.camera, 'camera_poses.npy')) align_mat = np.load(os.path.join(self.root, 'scenes', scene, self.camera, 'cam0_wrt_table.npy')) trans = np.dot(align_mat, camera_poses[self.frameid[index]]) workspace_mask = get_workspace_mask(cloud, seg, trans=trans, organized=True, outlier=0.02) mask = (depth_mask &amp; workspace_mask) else: mask = depth_mask cloud_masked = cloud[mask] color_masked = color[mask] seg_masked = seg[mask] if return_raw_cloud: return cloud_masked, color_masked # 如果点太多，那么就采样固定的点输出 if len(cloud_masked) &gt;= self.num_points: idxs = np.random.choice(len(cloud_masked), self.num_points, replace=False) else: idxs1 = np.arange(len(cloud_masked)) idxs2 = np.random.choice(len(cloud_masked), self.num_points-len(cloud_masked), replace=True) idxs = np.concatenate([idxs1, idxs2], axis=0) cloud_sampled = cloud_masked[idxs] color_sampled = color_masked[idxs] ret_dict = {} ret_dict['point_clouds'] = cloud_sampled.astype(np.float32) ret_dict['cloud_colors'] = color_sampled.astype(np.float32) return ret_dict def get_data_label(self, index): # 省略了get_data的所有逻辑，基于get_data的函数之后，这个函数做了如下的处理 cloud_sampled = cloud_masked[idxs] color_sampled = color_masked[idxs] seg_sampled = seg_masked[idxs] objectness_label = seg_sampled.copy() objectness_label[objectness_label&gt;1] = 1 object_poses_list = [] grasp_points_list = [] grasp_offsets_list = [] grasp_scores_list = [] grasp_tolerance_list = [] for i, obj_idx in enumerate(obj_idxs): # 枚举场景中的10类物体 if obj_idx not in self.valid_obj_idxs: continue if (seg_sampled == obj_idx).sum() &lt; 50: # 拍到的对应物体点云数量太少 continue object_poses_list.append(poses[:, :, i]) # 加入到合法的object_poses_list中 points, offsets, scores, tolerance = self.grasp_labels[obj_idx] # 得到物体坐标系下创建的Grasp Pose collision = self.collision_labels[scene][i] # 得到场景中的点的collision mask(Np, V, A, D) # remove invisible grasp points if self.remove_invisible: visible_mask = remove_invisible_grasp_points(cloud_sampled[seg_sampled==obj_idx], points, poses[:,:,i], th=0.01) points = points[visible_mask] offsets = offsets[visible_mask] scores = scores[visible_mask] tolerance = tolerance[visible_mask] collision = collision[visible_mask] idxs = np.random.choice(len(points), min(max(int(len(points)/4),300),len(points)), replace=False) grasp_points_list.append(points[idxs]) grasp_offsets_list.append(offsets[idxs]) collision = collision[idxs].copy() scores = scores[idxs].copy() tolerance = tolerance[idxs].copy() scores[collision] = 0 tolerance[collision] = 0 # 场景中存在collision的情况，我们设置Grasp分数为0 grasp_scores_list.append(scores) grasp_tolerance_list.append(tolerance) # 如果设置了这个，每次__getitem__的时候得到的数据也会有不一样，会有轻微的扰动，但是这样就不需要在创建Label的时候就加随机，降低的数据集的大小和处理的复杂度 if self.augment: cloud_sampled, object_poses_list = self.augment_data(cloud_sampled, object_poses_list) ret_dict = {} ret_dict['point_clouds'] = cloud_sampled.astype(np.float32) ret_dict['cloud_colors'] = color_sampled.astype(np.float32) ret_dict['objectness_label'] = objectness_label.astype(np.int64) ret_dict['object_poses_list'] = object_poses_list ret_dict['grasp_points_list'] = grasp_points_list ret_dict['grasp_offsets_list'] = grasp_offsets_list ret_dict['grasp_labels_list'] = grasp_scores_list ret_dict['grasp_tolerance_list'] = grasp_tolerance_list return ret_dict def augment_data(self, point_clouds, object_poses_list): # Flipping along the YZ plane if np.random.random() &gt; 0.5: flip_mat = np.array([[-1, 0, 0], [ 0, 1, 0], [ 0, 0, 1]]) point_clouds = transform_point_cloud(point_clouds, flip_mat, '3x3') for i in range(len(object_poses_list)): object_poses_list[i] = np.dot(flip_mat, object_poses_list[i]).astype(np.float32) # 沿着z轴旋转 rot_angle = (np.random.random()*np.pi/3) - np.pi/6 # -30 ~ +30 degree c, s = np.cos(rot_angle), np.sin(rot_angle) rot_mat = np.array([[1, 0, 0], [0, c,-s], [0, s, c]]) point_clouds = transform_point_cloud(point_clouds, rot_mat, '3x3') for i in range(len(object_poses_list)): object_poses_list[i] = np.dot(rot_mat, object_poses_list[i]).astype(np.float32) return point_clouds, object_poses_listdef collate_fn(batch): # 在train.py创建DataLoader的时候会使用到这个函数。 # collate_fn：如何取样本的，我们可以定义自己的函数来准确地实现想要的功能 if type(batch[0]).__module__ == 'numpy': return torch.stack([torch.from_numpy(b) for b in batch], 0) elif isinstance(batch[0], container_abcs.Mapping): return {key:collate_fn([d[key] for d in batch]) for key in batch[0]} elif isinstance(batch[0], container_abcs.Sequence): return [[torch.from_numpy(sample) for sample in b] for b in batch] raise TypeError(&quot;batch must contain tensors, dicts or lists; found {}&quot;.format(type(batch[0]))) graspnet.py​ 我们首先来看GraspNet这个类。我们可以看到，GraspNetStage1其实就是我们的PointNet++ backbone和ApproachNet部分。而GraspNetStage2主要是OperationNet和ToleranceNet部分。 12345678910111213class GraspNet(nn.Module): def __init__(self, input_feature_dim=0, num_view=300, num_angle=12, num_depth=4, cylinder_radius=0.05, hmin=-0.02, hmax_list=[0.01,0.02,0.03,0.04], is_training=True): super().__init__() self.is_training = is_training self.view_estimator = GraspNetStage1(input_feature_dim, num_view) self.grasp_generator = GraspNetStage2(num_angle, num_depth, cylinder_radius, hmin, hmax_list, is_training) def forward(self, end_points): end_points = self.view_estimator(end_points) if self.is_training: end_points = process_grasp_labels(end_points) end_points = self.grasp_generator(end_points) return end_points ​ 根据forward的顺序，我们先来看看GarspNetStage1的逻辑： 1234567891011class GraspNetStage1(nn.Module): def __init__(self, input_feature_dim=0, num_view=300): super().__init__() self.backbone = Pointnet2Backbone(input_feature_dim) self.vpmodule = ApproachNet(num_view, 256) def forward(self, end_points): pointcloud = end_points['point_clouds'] seed_features, seed_xyz, end_points = self.backbone(pointcloud, end_points) end_points = self.vpmodule(seed_xyz, seed_features, end_points) return end_points ​ 其实就是走了一个Pointnet2Backbone和ApproachNet，关于PointNet不再赘述。我们来看看ApproachNet 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859class ApproachNet(nn.Module): def __init__(self, num_view, seed_feature_dim): &quot;&quot;&quot; Approach vector estimation from seed point features. Input: num_view: [int] number of views generated from each each seed point seed_feature_dim: [int] number of channels of seed point features &quot;&quot;&quot; super().__init__() self.num_view = num_view self.in_dim = seed_feature_dim self.conv1 = nn.Conv1d(self.in_dim, self.in_dim, 1) self.conv2 = nn.Conv1d(self.in_dim, 2+self.num_view, 1) self.conv3 = nn.Conv1d(2+self.num_view, 2+self.num_view, 1) self.bn1 = nn.BatchNorm1d(self.in_dim) self.bn2 = nn.BatchNorm1d(2+self.num_view) def forward(self, seed_xyz, seed_features, end_points): &quot;&quot;&quot; Forward pass. Input: seed_xyz: (B, Ns, 3), coordinates of seed points seed_features: (batch_size,feature_dim,num_seed) features of seed points end_points: [dict] Output: end_points: [dict] &quot;&quot;&quot; B, num_seed, _ = seed_xyz.size() features = F.relu(self.bn1(self.conv1(seed_features)), inplace=True) # (B, C, Ns) features = F.relu(self.bn2(self.conv2(features)), inplace=True) # (B, 2 + V, Ns) features = self.conv3(features) # (B, 2 + V, Ns) objectness_score = features[:, :2, :] # (B, 2, Ns) view_score = features[:, 2:2+self.num_view, :].transpose(1,2).contiguous() # (B, Ns, V) end_points['objectness_score'] = objectness_score end_points['view_score'] = view_score # 找到一个最大Grasp score的approach vector的编号 top_view_scores, top_view_inds = torch.max(view_score, dim=2) # (B, Ns) top_view_inds_ = top_view_inds.view(B, num_seed, 1, 1).expand(-1, -1, -1, 3).contiguous() # expand代表把(B, M, 1, 1)复制成(B, M, 1, 3)，最后一维共享内存 template_views = generate_grasp_views(self.num_view).to(features.device) # (V, 3) template_views = template_views.view(1, 1, self.num_view, 3).expand(B, num_seed, -1, -1).contiguous() #(B, Ns, V, 3) vp_xyz = torch.gather(template_views, 2, top_view_inds_).squeeze(2) #(B, Ns, 3) # torch.gather意味着我们可以使用index把V那一维的对应index选出来，然后我们再用squeeze压缩维度 # 也就是 (B, M, V, 3) =&gt; (B, M, 1, 3) =&gt; (B, M, 3) vp_xyz_ = vp_xyz.view(-1, 3) # (B * M, 3) batch_angle = torch.zeros(vp_xyz_.size(0), dtype=vp_xyz.dtype, device=vp_xyz.device) # batch_angle : (B * M, 3)，此时的in-plane rotation vp_rot = batch_viewpoint_params_to_matrix(-vp_xyz_, batch_angle).view(B, num_seed, 3, 3) # batch_viewpoint_params_to_matrix是把approach vector和in-plane rotation转化为旋转矩阵 end_points['grasp_top_view_inds'] = top_view_inds end_points['grasp_top_view_score'] = top_view_scores end_points['grasp_top_view_xyz'] = vp_xyz end_points['grasp_top_view_rot'] = vp_rot return end_points ​ 注意到在GraspNet的forward过程中，我们的end_points需要经过process_grasp_labels这个函数。我们先来看看这个函数在做什么，它主要做了几件事情： 把物体坐标系下的Grasp投射到标准场景坐标系下，并且聚类到最近的场景系下的标准模板向量（V个approaching vector） 根据聚类结果对(Ns, V, A, D, 3)的第2维度V重排顺序，使得第二维满足相同的顺序，即不同物体的Grasp可以合成一个tensor：(Np’, V, A, D, 3)，其中满足$N_P’=\\sum N_P$。 对物体的原先标签Grasp Label做转换，原先的意义是达到稳定抓取的最小摩擦系数，转换成Score时需要满足摩擦系数越大的Grasp的成功率越小，所以其中做了一个归一化的倒数处理。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131def process_grasp_labels(end_points): &quot;&quot;&quot; 根据场景中的点和场景中物体的6D Pose处理grasp labels&quot;&quot;&quot; clouds = end_points['input_xyz'] #(B, N, 3) seed_xyzs = end_points['fp2_xyz'] #(B, Ns, 3) batch_size, num_samples, _ = seed_xyzs.size() batch_grasp_points = [] batch_grasp_views = [] batch_grasp_views_rot = [] batch_grasp_labels = [] batch_grasp_offsets = [] batch_grasp_tolerance = [] for i in range(batch_size): # batch中的每个数据单独处理 seed_xyz = seed_xyzs[i] # (Ns, 3) poses = end_points['object_poses_list'][i] # [&lt;=10, (3, 4)] # get merged grasp points for label computation grasp_points_merged = [] grasp_views_merged = [] grasp_views_rot_merged = [] grasp_labels_merged = [] grasp_offsets_merged = [] grasp_tolerance_merged = [] for obj_idx, pose in enumerate(poses): # 枚举当前场景中的所有物体 # 得到每个物体单独时候的Grasps，数量为Np * V * A * D grasp_points = end_points['grasp_points_list'][i][obj_idx] #(Np, 3) grasp_labels = end_points['grasp_labels_list'][i][obj_idx] #(Np, V, A, D) grasp_offsets = end_points['grasp_offsets_list'][i][obj_idx] #(Np, V, A, D, 3) grasp_tolerance = end_points['grasp_tolerance_list'][i][obj_idx] #(Np, V, A, D) _, V, A, D = grasp_labels.size() num_grasp_points = grasp_points.size(0) # 得到 Np # generate and transform template grasp views # 我们需要把物体mesh上的模板Grasp投射到当前场景的6D Pose下 grasp_views = generate_grasp_views(V).to(pose.device) # 使用单位球均匀采样方法，得到V个方向向量(V, 3) grasp_points_trans = transform_point_cloud(grasp_points, pose, '3x4')、 # 3x4就是既要旋转又要平移，根据物体在场景中的6D Pose，把grasp的投影到场景中, (Np, 3) grasp_views_trans = transform_point_cloud(grasp_views, pose[:3,:3], '3x3') # 3x3就是只旋转，我们现在得到了V个场景中对齐的方向向量 angles = torch.zeros(grasp_views.size(0), dtype=grasp_views.dtype, device=grasp_views.device) # 我们把单位向量转化为旋转矩阵，这个转换需要一个额外的自由度，也就in-plane rotation，这也是为什么我们传入了一个全零的angles的理由 grasp_views_rot = batch_viewpoint_params_to_matrix(-grasp_views, angles) #(V, 3, 3) # 这个函数内部，无非就就是把方向向量和标准坐标轴的某一维度对齐，再根据数学算出一个基矢量，然后计算基变换矩阵 grasp_views_rot_trans = torch.matmul(pose[:3,:3], grasp_views_rot) #(V, 3, 3) # 这个矩阵乘以物体的Pose矩阵，就得到了这个Grasp的Approach Vector在场景系下的绝对orn grasp_views_ = grasp_views.transpose(0, 1).contiguous().unsqueeze(0) # (1, 3, V) grasp_views_trans_ = grasp_views_trans.transpose(0, 1).contiguous().unsqueeze(0) #(1, 3, V) view_inds = knn(grasp_views_trans_, grasp_views_, k=1).squeeze() - 1 # 注意此处ref = grasp_views_trans_(物体坐标系的模板) # query = grasp_views_(物体坐标系) # 所以是要把物体坐标系下的方向向量分配到场景坐标系的模板方向向量上 # 因为物体在场景中的6D Pose是任意的，所以导致在场景对齐后，V个模板方向向量也是任意的，此处的KNN就是把场景对齐后的任意方向的V个approaching vector，重新聚类回场景系下的模板(V, 3)，这样场景中所有Grasp的GT都会属于绝对场景坐标系下的V个approaching vector中。 grasp_views_trans = torch.index_select(grasp_views_trans, 0, view_inds) #(V, 3) grasp_views_trans = grasp_views_trans.unsqueeze(0).expand(num_grasp_points, -1, -1) # (Np, V, 3)，重新计算Np个Grasp的新的方向向量 grasp_views_rot_trans = torch.index_select(grasp_views_rot_trans, 0, view_inds) #(V, 3, 3) grasp_views_rot_trans = grasp_views_rot_trans.unsqueeze(0).expand(num_grasp_points, -1, -1, -1) # (Np, V, 3, 3)，重新计算Np个Grasp的新的orn grasp_labels = torch.index_select(grasp_labels, 1, view_inds) #(Np, V, A, D) grasp_offsets = torch.index_select(grasp_offsets, 1, view_inds) #(Np, V, A, D, 3) grasp_tolerance = torch.index_select(grasp_tolerance, 1, view_inds) #(Np, V, A, D) # 上面三行其实都是在dim=1上重排序号，使其重新满足V这一维度的顺序等于模板Approaching Vector的创建顺序 # add to list grasp_points_merged.append(grasp_points_trans) grasp_views_merged.append(grasp_views_trans) grasp_views_rot_merged.append(grasp_views_rot_trans) grasp_labels_merged.append(grasp_labels) grasp_offsets_merged.append(grasp_offsets) grasp_tolerance_merged.append(grasp_tolerance) # Np' = sum(Np) for all object in the scene grasp_points_merged = torch.cat(grasp_points_merged, dim=0) #(Np', 3) grasp_views_merged = torch.cat(grasp_views_merged, dim=0) #(Np', V, 3) grasp_views_rot_merged = torch.cat(grasp_views_rot_merged, dim=0) #(Np', V, 3, 3) grasp_labels_merged = torch.cat(grasp_labels_merged, dim=0) #(Np', V, A, D) grasp_offsets_merged = torch.cat(grasp_offsets_merged, dim=0) #(Np', V, A, D, 3) grasp_tolerance_merged = torch.cat(grasp_tolerance_merged, dim=0) #(Np', V, A, D) # compute nearest neighbors seed_xyz_ = seed_xyz.transpose(0, 1).contiguous().unsqueeze(0) #(1, 3, Ns) grasp_points_merged_ = grasp_points_merged.transpose(0, 1).contiguous().unsqueeze(0) #(1, 3, Np') nn_inds = knn(grasp_points_merged_, seed_xyz_, k=1).squeeze() - 1 # 一般来说，Np'通常会有很多个，而Ns = M = 1024，所以此处就是在把每个seed分配到最近的一个grasp point中，这样每个seed就保留了一个最接近的grasp point，这样我们对于每个点有稠密的V * A * D个抓点的标注。 grasp_points_merged = torch.index_select(grasp_points_merged, 0, nn_inds) # (Ns, 3) grasp_views_merged = torch.index_select(grasp_views_merged, 0, nn_inds) # (Ns, V, 3) grasp_views_rot_merged = torch.index_select(grasp_views_rot_merged, 0, nn_inds) #(Ns, V, 3, 3) grasp_labels_merged = torch.index_select(grasp_labels_merged, 0, nn_inds) # (Ns, V, A, D) grasp_offsets_merged = torch.index_select(grasp_offsets_merged, 0, nn_inds) # (Ns, V, A, D, 3) grasp_tolerance_merged = torch.index_select(grasp_tolerance_merged, 0, nn_inds) # (Ns, V, A, D) # 以上就是得到了当前这个Batch的稠密标注 # add to batch batch_grasp_points.append(grasp_points_merged) batch_grasp_views.append(grasp_views_merged) batch_grasp_views_rot.append(grasp_views_rot_merged) batch_grasp_labels.append(grasp_labels_merged) batch_grasp_offsets.append(grasp_offsets_merged) batch_grasp_tolerance.append(grasp_tolerance_merged) batch_grasp_points = torch.stack(batch_grasp_points, 0) #(B, Ns, 3) batch_grasp_views = torch.stack(batch_grasp_views, 0) #(B, Ns, V, 3) batch_grasp_views_rot = torch.stack(batch_grasp_views_rot, 0) #(B, Ns, V, 3, 3) batch_grasp_labels = torch.stack(batch_grasp_labels, 0) #(B, Ns, V, A, D) batch_grasp_offsets = torch.stack(batch_grasp_offsets, 0) #(B, Ns, V, A, D, 3) batch_grasp_tolerance = torch.stack(batch_grasp_tolerance, 0) #(B, Ns, V, A, D) # 得到了所有batch的稠密标注 # process labels batch_grasp_widths = batch_grasp_offsets[:,:,:,:,:,2] label_mask = (batch_grasp_labels &gt; 0) &amp; (batch_grasp_widths &lt;= GRASP_MAX_WIDTH) u_max = batch_grasp_labels.max() # 此时的grasp_labels的物理意义为：达到一个稳定抓取所需要的最小摩擦系数 batch_grasp_labels[label_mask] = torch.log(u_max / batch_grasp_labels[label_mask]) # 我们让摩擦系数最大的Grasp的分数为0，因为所需要的摩擦系数越大，Grasp的可能性越低 # 因为摩擦系数分为了0.1~1，所以torch.log里最大的情况就是10，也就是grasp_score的最大值是torch.log(10) batch_grasp_labels[~label_mask] = 0 batch_grasp_view_scores, _ = batch_grasp_labels.view(batch_size, num_samples, V, A*D).max(dim=-1) end_points['batch_grasp_point'] = batch_grasp_points end_points['batch_grasp_view'] = batch_grasp_views end_points['batch_grasp_view_rot'] = batch_grasp_views_rot end_points['batch_grasp_label'] = batch_grasp_labels end_points['batch_grasp_offset'] = batch_grasp_offsets end_points['batch_grasp_tolerance'] = batch_grasp_tolerance end_points['batch_grasp_view_label'] = batch_grasp_view_scores.float() return end_points ​ GraspNetStage2的结构如下，它主要包含三个模块： match_grasp_view_and_label CloudCrop 并列的两个论文中画出来的网络：OperationNet和ToleranceNet 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253class GraspNetStage2(nn.Module): def __init__(self, num_angle=12, num_depth=4, cylinder_radius=0.05, hmin=-0.02, hmax_list=[0.01,0.02,0.03,0.04], is_training=True): super().__init__() self.num_angle = num_angle self.num_depth = num_depth self.is_training = is_training self.crop = CloudCrop(nsample=64, seed_feature_dim=3, cylinder_radius=cylinder_radius, hmin=hmin, hmax_list=hmax_list) self.operation = OperationNet(num_angle, num_depth) self.tolerance = ToleranceNet(num_angle, num_depth) def forward(self, end_points): pointcloud = end_points['input_xyz'] if self.is_training: grasp_top_views_rot, _, _, _, end_points = match_grasp_view_and_label(end_points) seed_xyz = end_points['batch_grasp_point'] else: grasp_top_views_rot = end_points['grasp_top_view_rot'] seed_xyz = end_points['fp2_xyz'] vp_features = self.crop(seed_xyz, pointcloud, grasp_top_views_rot) end_points = self.operation(vp_features, end_points) end_points = self.tolerance(vp_features, end_points) return end_pointsdef match_grasp_view_and_label(end_points): # 这个函数在做的事情就是，我们已经预测出来了V个approaching vector中最高分的index，那么我们就要从batch_grasp中把对应的Grasp的参数找出来作为当前点的GT往后传递 top_view_inds = end_points['grasp_top_view_inds'] # (B, Ns) template_views_rot = end_points['batch_grasp_view_rot'] # (B, Ns, V, 3, 3) grasp_labels = end_points['batch_grasp_label'] # (B, Ns, V, A, D) grasp_offsets = end_points['batch_grasp_offset'] # (B, Ns, V, A, D, 3) grasp_tolerance = end_points['batch_grasp_tolerance'] # (B, Ns, V, A, D) B, Ns, V, A, D = grasp_labels.size() top_view_inds_ = top_view_inds.view(B, Ns, 1, 1, 1).expand(-1, -1, -1, 3, 3) # 此时top_view_inds_为(B, Ns, 1, 3, 3)，也就是在V这一维中把最高分的选出来 top_template_views_rot = torch.gather(template_views_rot, 2, top_view_inds_).squeeze(2) top_view_inds_ = top_view_inds.view(B, Ns, 1, 1, 1).expand(-1, -1, -1, A, D) top_view_grasp_labels = torch.gather(grasp_labels, 2, top_view_inds_).squeeze(2) top_view_grasp_tolerance = torch.gather(grasp_tolerance, 2, top_view_inds_).squeeze(2) top_view_inds_ = top_view_inds.view(B, Ns, 1, 1, 1, 1).expand(-1, -1, -1, A, D, 3) top_view_grasp_offsets = torch.gather(grasp_offsets, 2, top_view_inds_).squeeze(2) # top_template_views_rot (B, Ns, 3, 3) # top_view_grasp_labels (B, Ns, A, D) # top_view_grasp_tolerance (B, Ns, A, D, 3) # top_view_grasp_offsets (B, Ns, A, D) end_points['batch_grasp_view_rot'] = top_template_views_rot end_points['batch_grasp_label'] = top_view_grasp_labels end_points['batch_grasp_offset'] = top_view_grasp_offsets end_points['batch_grasp_tolerance'] = top_view_grasp_tolerance # 从此以后，这些数据从原先的V个，只保留了分数最高的那一个了 return top_template_views_rot, top_view_grasp_labels, top_view_grasp_offsets, top_view_grasp_tolerance, end_points ​ 接下来CloudCrop模块： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354class CloudCrop(nn.Module): &quot;&quot;&quot; Cylinder group and align for grasp configure estimation. Return a list of grouped points with different cropping depths. Input: nsample: [int] sample number in a group seed_feature_dim: [int] number of channels of grouped points cylinder_radius: [float] radius of the cylinder space hmin: [float] height of the bottom surface hmax_list: [list of float] list of heights of the upper surface &quot;&quot;&quot; def __init__(self, nsample, seed_feature_dim, cylinder_radius=0.05, hmin=-0.02, hmax_list=[0.01,0.02,0.03,0.04]): super().__init__() self.nsample = nsample self.in_dim = seed_feature_dim self.cylinder_radius = cylinder_radius mlps = [self.in_dim, 64, 128, 256] self.groupers = [] for hmax in hmax_list: self.groupers.append(CylinderQueryAndGroup(cylinder_radius, hmin, hmax, nsample, use_xyz=True)) self.mlps = pt_utils.SharedMLP(mlps, bn=True) def forward(self, seed_xyz, pointcloud, vp_rot): &quot;&quot;&quot; Forward pass. Input: seed_xyz: (B, Ns, 3), coordinates of seed points pointcloud: (B, Ns, 3), the points to be cropped vp_rot: (B, Ns, 3, 3), rotation matrices generated from approach vectors Output: vp_features: (B, num_features, Ns,num_depth), features of grouped points in different depths &quot;&quot;&quot; B, num_seed, _, _ = vp_rot.size() num_depth = len(self.groupers) grouped_features = [] for grouper in self.groupers: # 枚举D个grouper grouped_features.append(grouper(pointcloud, seed_xyz, vp_rot)) # (B, 3, Ns, nsample) grouped_features = torch.stack(grouped_features, dim=3) # (B, 3, Ns, D, nsample) grouped_features = grouped_features.view(B, -1, num_seed*num_depth, self.nsample) # (B, 3, Ns * D, nsample) vp_features = self.mlps(grouped_features) # (B, mlps[-1], Ns * D, nsample) vp_features = F.max_pool2d(vp_features, kernel_size=[1, vp_features.size(3)]) # 圆筒聚类后，对聚类中的点做max_pool2d来聚合成一个cluster feature # (B, mlps[-1], Ns * D, 1) vp_features = vp_features.view(B, -1, num_seed, num_depth) # (B, mlps[-1], Ns, D) # 这个模组在做的事情就是把D维根据不同的深度提取出不同的特征，放到dim=1中。 return vp_features ​ 在此之中，用到了一个CylinderQueryAndGroup模块，我们进一步往下探索。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586class CylinderQueryAndGroup(nn.Module): r&quot;&quot;&quot; Groups with a cylinder query of radius and height Parameters --------- radius : float32 Radius of cylinder hmin, hmax: float32 endpoints of cylinder height in x-rotation axis nsample : int32 Maximum number of features to gather in the ball &quot;&quot;&quot; def __init__(self, radius, hmin, hmax, nsample, use_xyz=True, ret_grouped_xyz=False, normalize_xyz=False, rotate_xyz=True, sample_uniformly=False, ret_unique_cnt=False): super(CylinderQueryAndGroup, self).__init__() self.radius, self.nsample, self.hmin, self.hmax, = radius, nsample, hmin, hmax self.use_xyz = use_xyz self.ret_grouped_xyz = ret_grouped_xyz self.normalize_xyz = normalize_xyz self.rotate_xyz = rotate_xyz self.sample_uniformly = sample_uniformly self.ret_unique_cnt = ret_unique_cnt if self.ret_unique_cnt: assert(self.sample_uniformly) def forward(self, xyz, new_xyz, rot, features=None): &quot;&quot;&quot; Parameters ---------- xyz : 点云坐标 (B, N, 3) new_xyz : 抓点中心 (B, npoint, 3) rot : 抓点的旋转矩阵 (B, npoint, 3, 3) features : torch.Tensor Descriptors of the features (B, C, N) Returns ------- new_features : torch.Tensor (B, 3 + C, npoint, nsample) tensor &quot;&quot;&quot; B, npoint, _ = new_xyz.size() idx = cylinder_query(self.radius, self.hmin, self.hmax, self.nsample, xyz, new_xyz, rot.view(B, npoint, 9)) if self.sample_uniformly: unique_cnt = torch.zeros((idx.shape[0], idx.shape[1])) for i_batch in range(idx.shape[0]): for i_region in range(idx.shape[1]): unique_ind = torch.unique(idx[i_batch, i_region, :]) num_unique = unique_ind.shape[0] unique_cnt[i_batch, i_region] = num_unique sample_ind = torch.randint(0, num_unique, (self.nsample - num_unique,),dtype=torch.long) all_ind = torch.cat((unique_ind, unique_ind[sample_ind])) idx[i_batch, i_region, :] = all_ind xyz_trans = xyz.transpose(1, 2).contiguous() grouped_xyz = grouping_operation(xyz_trans, idx) # (B, 3, npoint, nsample) grouped_xyz -= new_xyz.transpose(1, 2).unsqueeze(-1) if self.normalize_xyz: grouped_xyz /= self.radius if self.rotate_xyz: grouped_xyz_ = grouped_xyz.permute(0, 2, 3, 1).contiguous() # (B, npoint, nsample, 3) grouped_xyz_ = torch.matmul(grouped_xyz_, rot) grouped_xyz = grouped_xyz_.permute(0, 3, 1, 2).contiguous() if features is not None: grouped_features = grouping_operation(features, idx) if self.use_xyz: new_features = torch.cat( [grouped_xyz, grouped_features], dim=1 ) # (B, C + 3, npoint, nsample) else: new_features = grouped_features else: assert (self.use_xyz), &quot;Cannot have not features and not use xyz as a feature!&quot; new_features = grouped_xyz ret = [new_features] if self.ret_grouped_xyz: ret.append(grouped_xyz) if self.ret_unique_cnt: ret.append(unique_cnt) if len(ret) == 1: return ret[0] else: return tuple(ret) ​ 为了理解其中cylinder_query的作用，我们看到其CUDA实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788//我们先尝试理解PointNet++中的ball_query的实现__global__ void query_ball_point_kernel(int b, int n, int m, float radius, int nsample, const float *__restrict__ new_xyz, //query const float *__restrict__ xyz, //原先点 int *__restrict__ idx) { int batch_index = blockIdx.x; xyz += batch_index * n * 3; new_xyz += batch_index * m * 3; idx += m * nsample * batch_index; int index = threadIdx.x; int stride = blockDim.x; float radius2 = radius * radius; for (int j = index; j &lt; m; j += stride) { float new_x = new_xyz[j * 3 + 0]; float new_y = new_xyz[j * 3 + 1]; float new_z = new_xyz[j * 3 + 2]; for (int k = 0, cnt = 0; k &lt; n &amp;&amp; cnt &lt; nsample; ++k) { //枚举原先的每个点 float x = xyz[k * 3 + 0]; float y = xyz[k * 3 + 1]; float z = xyz[k * 3 + 2]; float d2 = (new_x - x) * (new_x - x) + (new_y - y) * (new_y - y) + (new_z - z) * (new_z - z); //计算和Query的距离 if (d2 &lt; radius2) { if (cnt == 0) { for (int l = 0; l &lt; nsample; ++l) { // 小于ball半径的情况下，就加到idx中，并且第一次把idx中的值全部赋值为k idx[j * nsample + l] = k; } } idx[j * nsample + cnt] = k; ++cnt; } } }}__global__ void query_cylinder_point_kernel(int b, int n /*xyz.size(1)*/, int m/*new_xyz.size(1)*/, float radius, float hmin, float hmax, int nsample, const float *__restrict__ new_xyz, //抓点中心 const float *__restrict__ xyz, //点云 const float *__restrict__ rot, int *__restrict__ idx) { int batch_index = blockIdx.x; xyz += batch_index * n * 3; // 访问到当前batch的点云数据偏移量，也就是xyz[batch*n*3] new_xyz += batch_index * m * 3; // 访问到当前batch的抓点数据偏移量，也就是new_xyz[batch*m*3] rot += batch_index * m * 9; idx += m * nsample * batch_index; int index = threadIdx.x; //当前Block中的线程ID int stride = blockDim.x; //当前Grid中的Thread Block ID，注意调用这个kernel函数的时候传入的Grid和Block都是一维的 float radius2 = radius * radius; for (int j = index; j &lt; m; j += stride) { //Query中的每个Grasp Center Point float new_x = new_xyz[j * 3 + 0]; float new_y = new_xyz[j * 3 + 1]; float new_z = new_xyz[j * 3 + 2]; float r0 = rot[j * 9 + 0]; float r1 = rot[j * 9 + 1]; float r2 = rot[j * 9 + 2]; float r3 = rot[j * 9 + 3]; float r4 = rot[j * 9 + 4]; float r5 = rot[j * 9 + 5]; float r6 = rot[j * 9 + 6]; float r7 = rot[j * 9 + 7]; float r8 = rot[j * 9 + 8]; for (int k = 0, cnt = 0; k &lt; n &amp;&amp; cnt &lt; nsample; ++k) { //枚举点云中的每个点 float x = xyz[k * 3 + 0] - new_x; float y = xyz[k * 3 + 1] - new_y; float z = xyz[k * 3 + 2] - new_z; float x_rot = r0 * x + r3 * y + r6 * z; float y_rot = r1 * x + r4 * y + r7 * z; float z_rot = r2 * x + r5 * y + r8 * z; // 把裁剪采样过的点云变换到夹爪坐标系里 float d2 = y_rot * y_rot + z_rot * z_rot; //沿着圆柱切向量的半径计算 if (d2 &lt; radius2 &amp;&amp; x_rot &gt; hmin &amp;&amp; x_rot &lt; hmax) { //沿着圆柱法向量的裁剪 if (cnt == 0) { for (int l = 0; l &lt; nsample; ++l) { idx[j * nsample + l] = k; } } idx[j * nsample + cnt] = k; ++cnt; } } }} ​ 所以上述CylinderQuery的意义就是在圆柱范围内对点做聚类。 ​ 接下来是OperationNet和ToleranceNet： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990class OperationNet(nn.Module): &quot;&quot;&quot; Grasp configure estimation. Input: num_angle: [int] number of in-plane rotation angle classes the value of the i-th class --&gt; i*PI/num_angle (i=0,...,num_angle-1) num_depth: [int] number of gripper depth classes &quot;&quot;&quot; def __init__(self, num_angle, num_depth): # Output: # scores(num_angle) # angle class (num_angle) # width (num_angle) super().__init__() self.num_angle = num_angle self.num_depth = num_depth self.conv1 = nn.Conv1d(256, 128, 1) self.conv2 = nn.Conv1d(128, 128, 1) self.conv3 = nn.Conv1d(128, 3*num_angle, 1) self.bn1 = nn.BatchNorm1d(128) self.bn2 = nn.BatchNorm1d(128) def forward(self, vp_features, end_points): &quot;&quot;&quot; Forward pass. Input: vp_features: (B, mlps[-1], Ns, D) features of grouped points in different depths end_points: [dict] Output: end_points: [dict] &quot;&quot;&quot; # 在这个函数中，我们已经确定了Ns和D的情况下，尝试把in-plane angle多分类出来。 B, _, num_seed, num_depth = vp_features.size() vp_features = vp_features.view(B, -1, num_seed*num_depth) # (B, mlps[-1], Ns * D)，在默认情况下为(B, 256, Ns * 4) vp_features = F.relu(self.bn1(self.conv1(vp_features)), inplace=True) # (B, 256, 256) =&gt; (B, 128, 256) vp_features = F.relu(self.bn2(self.conv2(vp_features)), inplace=True) # (B, 128, 256) =&gt; (B, 128, 256) vp_features = self.conv3(vp_features) # (B, 128, 256) =&gt; (B, 3 * A, 256) 也就是 (B, 3 * A, Ns * D) vp_features = vp_features.view(B, -1, num_seed, num_depth) # (B, 3 * A, Ns, D) # split prediction end_points['grasp_score_pred'] = vp_features[:, 0:self.num_angle] end_points['grasp_angle_cls_pred'] = vp_features[:, self.num_angle:2*self.num_angle] end_points['grasp_width_pred'] = vp_features[:, 2*self.num_angle:3*self.num_angle] # 拆成了三个(B, A, Ns, D) return end_points class ToleranceNet(nn.Module): &quot;&quot;&quot; Grasp tolerance prediction. Input: num_angle: [int] number of in-plane rotation angle classes the value of the i-th class --&gt; i*PI/num_angle (i=0,...,num_angle-1) num_depth: [int] number of gripper depth classes &quot;&quot;&quot; def __init__(self, num_angle, num_depth): # Output: # tolerance (num_angle) super().__init__() self.conv1 = nn.Conv1d(256, 128, 1) self.conv2 = nn.Conv1d(128, 128, 1) self.conv3 = nn.Conv1d(128, num_angle, 1) self.bn1 = nn.BatchNorm1d(128) self.bn2 = nn.BatchNorm1d(128) def forward(self, vp_features, end_points): &quot;&quot;&quot; Forward pass. Input: vp_features: [torch.FloatTensor, (batch_size,num_seed,3)] features of grouped points in different depths end_points: [dict] Output: end_points: [dict] &quot;&quot;&quot; B, _, num_seed, num_depth = vp_features.size() vp_features = vp_features.view(B, -1, num_seed*num_depth) vp_features = F.relu(self.bn1(self.conv1(vp_features)), inplace=True) vp_features = F.relu(self.bn2(self.conv2(vp_features)), inplace=True) vp_features = self.conv3(vp_features) vp_features = vp_features.view(B, -1, num_seed, num_depth) end_points['grasp_tolerance_pred'] = vp_features # (B, num_angle, Ns, num_depth) return end_points ​ 到这里为止，GraspNet的前向传播过程全部结束。我们接下来需要通过pred_decode对预测的end_points字典解码。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273def pred_decode(end_points): batch_size = len(end_points['point_clouds']) grasp_preds = [] for i in range(batch_size): ## load predictions objectness_score = end_points['objectness_score'][i].float() grasp_score = end_points['grasp_score_pred'][i].float() grasp_angle_class_score = end_points['grasp_angle_cls_pred'][i] grasp_width = 1.2 * end_points['grasp_width_pred'][i] grasp_width = torch.clamp(grasp_width, min=0, max=GRASP_MAX_WIDTH) grasp_tolerance = end_points['grasp_tolerance_pred'][i] # 以上四个都是网络预测的(A, Ns, D)的Grasp参数 grasp_center = end_points['fp2_xyz'][i].float() # (Ns, 3) approaching = -end_points['grasp_top_view_xyz'][i].float() # (Ns, 3) ## slice preds by angle # grasp angle grasp_angle_class = torch.argmax(grasp_angle_class_score, 0) # 得到最高分数的in-plane rotation angle类，也就是(1, Ns, D) grasp_angle = grasp_angle_class.float() / 12 * np.pi # 转换成对应的in-plane rotation angle # grasp score &amp; width &amp; tolerance grasp_angle_class_ = grasp_angle_class.unsqueeze(0) # 这是 (Ns, D) 的indx grasp_score = torch.gather(grasp_score, 0, grasp_angle_class_).squeeze(0) grasp_width = torch.gather(grasp_width, 0, grasp_angle_class_).squeeze(0) grasp_tolerance = torch.gather(grasp_tolerance, 0, grasp_angle_class_).squeeze(0) # 对每个Ns和D得到了一个angle, (Ns, D) ## slice preds by score/depth # grasp depth grasp_depth_class = torch.argmax(grasp_score, 1, keepdims=True) # 在D这个维度，根据grasp_score来选出分数最高的深度 # (Ns, 1) grasp_depth = (grasp_depth_class.float()+1) * 0.01 # 得到(Ns, 1)的depth # grasp score &amp; angle &amp; width &amp; tolerance grasp_score = torch.gather(grasp_score, 1, grasp_depth_class) grasp_angle = torch.gather(grasp_angle, 1, grasp_depth_class) grasp_width = torch.gather(grasp_width, 1, grasp_depth_class) grasp_tolerance = torch.gather(grasp_tolerance, 1, grasp_depth_class) # 以上四个都是(Ns， 1)，每个seed对应唯一一个分数最高的Grasp ## slice preds by objectness objectness_pred = torch.argmax(objectness_score, 0) # 对每个点的objectness做判断 objectness_mask = (objectness_pred==1) # 根据objectness预测结果得到mask grasp_score = grasp_score[objectness_mask] grasp_width = grasp_width[objectness_mask] grasp_depth = grasp_depth[objectness_mask] approaching = approaching[objectness_mask] grasp_angle = grasp_angle[objectness_mask] grasp_center = grasp_center[objectness_mask] grasp_tolerance = grasp_tolerance[objectness_mask] # 如果objectness预测没有物体，那么Grasp的所有参数置为空 grasp_score = grasp_score * grasp_tolerance / GRASP_MAX_TOLERANCE # 真正Grasp的分数，要考虑到grasp_tolerance的影响，这里就简单的加权乘了一下 ## convert to rotation matrix Ns = grasp_angle.size(0) approaching_ = approaching.view(Ns, 3) grasp_angle_ = grasp_angle.view(Ns) rotation_matrix = batch_viewpoint_params_to_matrix(approaching_, grasp_angle_) rotation_matrix = rotation_matrix.view(Ns, 9) #根据我们的预测，把Ns个Grasp的approaching vector和grasp_angle_组合得到(Ns, 9)的旋转矩阵 # merge preds grasp_height = 0.02 * torch.ones_like(grasp_score) # 此处ones_like就是创建Ns个0.02的元素，也就是我们认为Grasp_height恒为2cm obj_ids = -1 * torch.ones_like(grasp_score) grasp_preds.append(torch.cat([grasp_score, grasp_width, grasp_height, grasp_depth, rotation_matrix, grasp_center, obj_ids], axis=-1)) return grasp_preds loss.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143def get_loss(end_points): # 正如论文所提到的，最终的loss分为三部分 objectness_loss, end_points = compute_objectness_loss(end_points) view_loss, end_points = compute_view_loss(end_points) grasp_loss, end_points = compute_grasp_loss(end_points) loss = objectness_loss + view_loss + 0.2 * grasp_loss end_points['loss/overall_loss'] = loss return loss, end_pointsdef compute_objectness_loss(end_points): criterion = nn.CrossEntropyLoss(reduction='mean') objectness_score = end_points['objectness_score'] # (B, Ns, 1) objectness_label = end_points['objectness_label'] # (B, M, 1) fp2_inds = end_points['fp2_inds'].long() objectness_label = torch.gather(objectness_label, 1, fp2_inds) # (B, Ns, 1) loss = criterion(objectness_score, objectness_label) # 此处就是对Ns个点做二分类的交叉熵 end_points['loss/stage1_objectness_loss'] = loss objectness_pred = torch.argmax(objectness_score, 1) end_points['stage1_objectness_acc'] = (objectness_pred == objectness_label.long()).float().mean() # 精度 end_points['stage1_objectness_prec'] = (objectness_pred == objectness_label.long())[objectness_pred == 1].float().mean() # 精确率：预测正确的objectness个数占总的正类预测个数的比例 end_points['stage1_objectness_recall'] = (objectness_pred == objectness_label.long())[objectness_label == 1].float().mean() # 召回率：被预测为正类占所有标注的个数 return loss, end_pointsdef compute_view_loss(end_points): criterion = nn.MSELoss(reduction='none') view_score = end_points['view_score'] # (B, Ns, V) view_label = end_points['batch_grasp_view_label'] # (B, Ns, V, A, D) objectness_label = end_points['objectness_label'] # (B, M, 1) fp2_inds = end_points['fp2_inds'].long() # (B, Ns) V = view_label.size(2) objectness_label = torch.gather(objectness_label, 1, fp2_inds) # (B, Ns, 1) objectness_mask = (objectness_label &gt; 0) objectness_mask = objectness_mask.unsqueeze(-1).repeat(1, 1, V) # (B, Ns, V) pos_view_pred_mask = ((view_score &gt;= THRESH_GOOD) &amp; objectness_mask) # 满足这个点附近有物体，并且approaching vector的分数满足阈值 loss = criterion(view_score, view_label) #均方误差 loss = loss[objectness_mask].mean() # 只取有objectness的部分，其余的如论文所说，忽略掉 end_points['loss/stage1_view_loss'] = loss end_points['stage1_pos_view_pred_count'] = pos_view_pred_mask.long().sum() return loss, end_pointsdef compute_grasp_loss(end_points, use_template_in_training=True): top_view_inds = end_points['grasp_top_view_inds'] # (B, Ns) vp_rot = end_points['grasp_top_view_rot'] # (B, Ns, view_factor, 3, 3) objectness_label = end_points['objectness_label'] fp2_inds = end_points['fp2_inds'].long() objectness_mask = torch.gather(objectness_label, 1, fp2_inds).bool() # (B, Ns) # 从数据集中得到的，每个batch的Ns个点，都有A * D个Grasp batch_grasp_label = end_points['batch_grasp_label'] # (B, Ns, A, D) batch_grasp_offset = end_points['batch_grasp_offset'] # (B, Ns, A, D, 3) batch_grasp_tolerance = end_points['batch_grasp_tolerance'] # (B, Ns, A, D) B, Ns, A, D = batch_grasp_label.size() # 以下这段都是在从数据集中取出对应的GT top_view_grasp_angles = batch_grasp_offset[:, :, :, :, 0] #(B, Ns, A, D) top_view_grasp_depths = batch_grasp_offset[:, :, :, :, 1] #(B, Ns, A, D) top_view_grasp_widths = batch_grasp_offset[:, :, :, :, 2] #(B, Ns, A, D) # 对于每个深度，取出对应的angle-bin分数最高的inds，并且取出对应的GT target_labels_inds = torch.argmax(batch_grasp_label, dim=2, keepdim=True) # (B, Ns, 1, D) target_labels = torch.gather(batch_grasp_label, 2, target_labels_inds).squeeze(2) # (B, Ns, D) target_angles = torch.gather(top_view_grasp_angles, 2, target_labels_inds).squeeze(2) # (B, Ns, D) target_depths = torch.gather(top_view_grasp_depths, 2, target_labels_inds).squeeze(2) # (B, Ns, D) target_widths = torch.gather(top_view_grasp_widths, 2, target_labels_inds).squeeze(2) # (B, Ns, D) target_tolerance = torch.gather(batch_grasp_tolerance, 2, target_labels_inds).squeeze(2) # (B, Ns, D) graspable_mask = (target_labels &gt; THRESH_BAD) objectness_mask = objectness_mask.unsqueeze(-1).expand_as(graspable_mask) loss_mask = (objectness_mask &amp; graspable_mask).float() # 1. grasp score loss target_labels_inds_ = target_labels_inds.transpose(1, 2) # (B, 1, Ns, D) grasp_score = torch.gather(end_points['grasp_score_pred'], 1, target_labels_inds_).squeeze(1) # (B, Ns, D) grasp_score_loss = huber_loss(grasp_score - target_labels, delta=1.0) grasp_score_loss = torch.sum(grasp_score_loss * loss_mask) / (loss_mask.sum() + 1e-6) # 只考虑mask==1的情况 end_points['loss/stage2_grasp_score_loss'] = grasp_score_loss # 2. inplane rotation cls loss target_angles_cls = target_labels_inds.squeeze(2) # (B, Ns, D) criterion_grasp_angle_class = nn.CrossEntropyLoss(reduction='none') grasp_angle_class_score = end_points['grasp_angle_cls_pred'] grasp_angle_class_loss = criterion_grasp_angle_class(grasp_angle_class_score, target_angles_cls) grasp_angle_class_loss = torch.sum(grasp_angle_class_loss * loss_mask) / (loss_mask.sum() + 1e-6) # 多分类交叉熵 end_points['loss/stage2_grasp_angle_class_loss'] = grasp_angle_class_loss grasp_angle_class_pred = torch.argmax(grasp_angle_class_score, 1) # 得到分数最高的in-place rotation end_points['stage2_grasp_angle_class_acc/0_degree'] = (grasp_angle_class_pred==target_angles_cls)[loss_mask.bool()].float().mean() acc_mask_15 = ((torch.abs(grasp_angle_class_pred-target_angles_cls) &lt;= 1) | (torch.abs(grasp_angle_class_pred-target_angles_cls) &gt;= A - 1)) end_points['stage2_grasp_angle_class_acc/15_degree'] = acc_mask_15[loss_mask.bool()].float().mean() acc_mask_30 = ((torch.abs(grasp_angle_class_pred-target_angles_cls) &lt;= 2) | (torch.abs(grasp_angle_class_pred-target_angles_cls) &gt;= A - 2)) end_points['stage2_grasp_angle_class_acc/30_degree'] = acc_mask_30[loss_mask.bool()].float().mean() # 因为一类相差15度，此处就根据分类的具体位置是否相差1个或2个来计算15度/30度的accuracy # 3. width reg loss grasp_width_pred = torch.gather(end_points['grasp_width_pred'], 1, target_labels_inds_).squeeze(1) # (B, num_angle, Ns, D) =&gt; (B, Ns, D) grasp_width_loss = huber_loss((grasp_width_pred-target_widths)/GRASP_MAX_WIDTH, delta=1) grasp_width_loss = torch.sum(grasp_width_loss * loss_mask) / (loss_mask.sum() + 1e-6) # 对width做回归 end_points['loss/stage2_grasp_width_loss'] = grasp_width_loss # 4. tolerance reg loss grasp_tolerance_pred = torch.gather(end_points['grasp_tolerance_pred'], 1, target_labels_inds_).squeeze(1) # (B, num_angle, Ns, D) =&gt; (B, Ns, D) grasp_tolerance_loss = huber_loss((grasp_tolerance_pred-target_tolerance)/GRASP_MAX_TOLERANCE, delta=1) grasp_tolerance_loss = torch.sum(grasp_tolerance_loss * loss_mask) / (loss_mask.sum() + 1e-6) end_points['loss/stage2_grasp_tolerance_loss'] = grasp_tolerance_loss grasp_loss = grasp_score_loss + grasp_angle_class_loss\\ + grasp_width_loss + grasp_tolerance_loss return grasp_loss, end_pointsdef huber_loss(error, delta=1.0): &quot;&quot;&quot; Args: error: Torch tensor (d1,d2,...,dk) Returns: loss: Torch tensor (d1,d2,...,dk) x = error = pred - gt or dist(pred,gt) 0.5 * |x|^2 if |x|&lt;=d 0.5 * d^2 + d * (|x|-d) if |x|&gt;d Author: Charles R. Qi Ref: https://github.com/charlesq34/frustum-pointnets/blob/master/models/model_util.py &quot;&quot;&quot; abs_error = torch.abs(error) quadratic = torch.clamp(abs_error, max=delta) linear = (abs_error - quadratic) loss = 0.5 * quadratic**2 + delta * linear return loss 附言​ 和浩树学长聊了一下，他很明确地指出这其实是一个多分类任务。并且因为论文中提到了使用离散的Bin来代替回归出一个向量的idea，我就继续请教了一下这样做的原因。因为这一点其实在VoteNet中尝试预测物体的6D pose的时候，也是有体现的。他认为回归在方法论上就不太对，因为正常的情况下Valid Approach Vector / Operations 应当是一个分布，而不是回归出来的一个值。多分类对于每个离散的bin预测出一个score，在我的理解下，就是用细粒度的均匀分布去拟合这个我们所希望预测的分布，这样训练效果就会好。听罢，感叹自己科研的路上还有很多路要走…因为各种各样的论文只会告诉你它的优点是什么，真正的硬伤都不会写在论文里。而要把握领域前沿动向，并且做出正确的选择，那就必须要在这种抽象层面上建构出自己稳定的理论体系， 但是我觉得我自己还差了很多这一块的知识积累。真是道阻且长啊。好在，我在努力开发出很多未来可以使用的方法论、视野、论文积累、代码积累等，希望可以在这条路上继续努力！","link":"/blog/2022/01/18/GraspNet-1Billion/"}],"tags":[{"name":"CHOMP","slug":"CHOMP","link":"/blog/tags/CHOMP/"},{"name":"planning","slug":"planning","link":"/blog/tags/planning/"},{"name":"lab","slug":"lab","link":"/blog/tags/lab/"},{"name":"GraspNet","slug":"GraspNet","link":"/blog/tags/GraspNet/"},{"name":"Affordance","slug":"Affordance","link":"/blog/tags/Affordance/"},{"name":"ROS","slug":"ROS","link":"/blog/tags/ROS/"},{"name":"moveit","slug":"moveit","link":"/blog/tags/moveit/"},{"name":"SE3353","slug":"SE3353","link":"/blog/tags/SE3353/"},{"name":"Docker","slug":"Docker","link":"/blog/tags/Docker/"},{"name":"MySQL","slug":"MySQL","link":"/blog/tags/MySQL/"},{"name":"WebService","slug":"WebService","link":"/blog/tags/WebService/"},{"name":"Search","slug":"Search","link":"/blog/tags/Search/"},{"name":"MongoDB","slug":"MongoDB","link":"/blog/tags/MongoDB/"},{"name":"Neo4j","slug":"Neo4j","link":"/blog/tags/Neo4j/"},{"name":"IJCAI","slug":"IJCAI","link":"/blog/tags/IJCAI/"},{"name":"BCO","slug":"BCO","link":"/blog/tags/BCO/"},{"name":"behavior tree","slug":"behavior-tree","link":"/blog/tags/behavior-tree/"},{"name":"compliers","slug":"compliers","link":"/blog/tags/compliers/"},{"name":"ikfast","slug":"ikfast","link":"/blog/tags/ikfast/"},{"name":"pybullet","slug":"pybullet","link":"/blog/tags/pybullet/"},{"name":"VoteNet","slug":"VoteNet","link":"/blog/tags/VoteNet/"},{"name":"mmdection","slug":"mmdection","link":"/blog/tags/mmdection/"},{"name":"segmentation","slug":"segmentation","link":"/blog/tags/segmentation/"},{"name":"deeplearning","slug":"deeplearning","link":"/blog/tags/deeplearning/"},{"name":"OpenCV","slug":"OpenCV","link":"/blog/tags/OpenCV/"},{"name":"OpenGL","slug":"OpenGL","link":"/blog/tags/OpenGL/"},{"name":"Camera","slug":"Camera","link":"/blog/tags/Camera/"},{"name":"Pybind11","slug":"Pybind11","link":"/blog/tags/Pybind11/"},{"name":"Pybullet","slug":"Pybullet","link":"/blog/tags/Pybullet/"},{"name":"IROS","slug":"IROS","link":"/blog/tags/IROS/"},{"name":"SOIL","slug":"SOIL","link":"/blog/tags/SOIL/"},{"name":"ICRA","slug":"ICRA","link":"/blog/tags/ICRA/"},{"name":"RSS","slug":"RSS","link":"/blog/tags/RSS/"},{"name":"ICCV","slug":"ICCV","link":"/blog/tags/ICCV/"},{"name":"ScrewNet","slug":"ScrewNet","link":"/blog/tags/ScrewNet/"},{"name":"PointNet","slug":"PointNet","link":"/blog/tags/PointNet/"},{"name":"PointNet++","slug":"PointNet","link":"/blog/tags/PointNet/"},{"name":"CVPR","slug":"CVPR","link":"/blog/tags/CVPR/"}],"categories":[]}