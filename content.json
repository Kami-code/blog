{"pages":[],"posts":[{"title":"SE3353_assignment4","text":"简单总结一下这次作业。基于Lucene的全文搜索功能和WebService的封装。 在你的项目中增加基于Solr或Lucene的针对书籍简介的全文搜索功能，用户可以在搜索界面输入搜索关键词，你可以通过全文搜索引擎找到书籍简介中包含该关键词的书籍列表。为了实现起来方便，你可以自己设计文本文件格式来存储书籍简介信息。例如，你可以将所有书籍的简介信息存储成为JSON对象，包含书的ID和简介文本，每行存储一本书的JSON对象。 请将上述全文搜索功能开发并部署为Web Service。 一、开发基于Lucene的全文搜索功能我们实现一个ApplicationRunner接口，这样它会在SpringBoot启动的时候自动执行一次，把数据库中的书籍及其间接建立索引。 为了测试这个功能，我们添加了一个用于测试的Controller。 我们分别输入“你”和“奥秘”作为关键词，我们可以清楚地看见确实返回了所有包含关键词的书籍。 二、把全文搜索功能整合到WebService中 如果说第一部分在十分钟内就可以基本上完成的话，这部分就非常冗长了。首先是WebService的选型，我首先选了Jax-ws作为后端webservice的实现。后面实现起来一切正常，但是当我想整合前端时出现了巨大的问题。简而言之就是，前端因为要组装text/xml作为SOAP协议的body，但是在firefox和Chrome中，这属于“非简单请求”，会自动先发送一个preflight。 非简单请求的CORS请求，会在正式通信之前，增加一次HTTP查询请求，称为”预检”请求（preflight）”预检”请求用的请求方法是OPTIONS，表示这个请求是用来询问的。头信息里面，关键字段是Origin，表示请求来自哪个源。浏览器先询问服务器，当前网页所在的域名是否在服务器的许可名单之中，以及可以使用哪些HTTP动词和头信息字段。只有得到肯定答复，浏览器才会发出正式的XMLHttpRequest请求，否则就报错。 就是因为这个OPTION请求让Jax-ws用不了，发请求的时候SpringBoot会报出： com.sun.xml.internal.ws.server.http : 无法处理 HTTP 方法: OPTIONS 我们需要定义interceptor去拦截这个OPTIONS请求，并且返回一个200:OK，但是找了半天不知道这个接口在哪里。使用SpringBoot的HandlerInterceptor也拦不到这个OPTIONS请求，陷入僵局。 后来看到了这篇文章js调用webservice接口时后台无法处理OPTIONS请求的解决方法，只能弃用Jax-ws，启用cxf。 拦截器参考网上的抄了一份过来，简单而言就是如果拦截到的是preflight，那就设置对应的Header后返回200:OK。 具体的webservice倒显得很容易了，如下是Service层的代码，还有一个CXFConfig类。 我们可以在对应暴露出的url后添加?wsdl，来得到这个webservice对应的wsdl文件。 理论上前端应该是要parse这个wsdl文件然后生成对应的SOAP消息的，不过限于时间，和具体业务其实不需要这么大的灵活度，所以就没做，固定了前端的格式。 值得一提的是，前端的SOAP消息格式是使用了SOAPUI这个软件，传入wsdl文件后自动生成出来的，节省了我很多debug的时间。 剩下的无非就是前端的一些工作了，封装出对应的SOAP消息生成和解析器即可。 搜索“你”和“奥秘”，结果与之前我们在controller中测试的结果完全相同。任务结束。","link":"/2021/10/20/SE3353-assignment4/"},{"title":"moveit_configuration","text":"在配置moveit_noetic的时候，不可避免地遇到了一些问题。在此记录一下。 官网配置教程地址： https://ros-planning.github.io/moveit_tutorials/doc/getting_started/getting_started.html Catkin是ROS官方的构建编译系统，是原先的ROS编译构建系统rosbuild的继承者，它组合了cmake宏和python脚本来在基本的cmake工作流之上提供一些额外的功能。Catkin设计得比rosbuild更加便捷，允许更好地分配package，支持更好的交叉编译以及更好的便捷性。Catkin的工作流和cmake的工作流很像，但是还添加了自动find package的基础结构，并且在同时构建多个相互依赖的项目。 catkin编译的工作流程如下： 首先在工作空间catkin_ws/src/下递归的查找其中每一个ROS的package。package中会有package.xml和CMakeLists.txt文件，Catkin(CMake)编译系统依据CMakeLists.txt文件,从而生成makefiles(放在catkin_ws/build/)。 然后make刚刚生成的makefiles等文件，编译链接生成可执行文件(放在catkin_ws/devel)也就是说，Catkin就是将cmake与make指令做了一个封装从而完成整个编译过程的工具。catkin有比较突出的优点，主要是： 1.操作更加简单 2.一次配置，多次使用 3.跨依赖项目编译 使用catkin_make进行编译 $ cd ~/catkin_ws #回到工作空间,catkin_make必须在工作空间下执行 $ catkin_make #开始编译 $ source ~/catkin_ws/devel/setup.bash #刷新坏境 编译完成后，如果有新的目标文件产生（原来没有），那么一般紧跟着要source刷新环境，使得系统能够找到刚才编译生成的ROS可执行文件。这个细节比较容易遗漏，致使后面出现可执行文件无法打开等错误。 第一次catkin build的时候，我们遇到如下错误： 根据这里，我们修改原先的catkin_build指令，修改为catkin build -DPYTHON_EXECUTABLE=/usr/bin/python3 -DPYTHON_INCLUDE_DIR=/usr/include/python3.7m。换句话说，ROS的catkin默认的是用python2.7去编译的，哪怕我们安装了python3-empy它依然报错。告诉它使用python3后这个报错就解决了。 第二个问题是tinyxml.h找不到tinystr.h文件。我们找到对应位置后在源码中添加TIXML_USE_STL即可解决这个问题，使用标准的STL库来代替tinystr.h。 第三处build的问题是 它提示我们找不到pyconfig.h，根据这篇博客的提示，我先使用sudo find / -name pyconfig.h，发现在路径/usr/include/python3.8下存在pyconfig.h，原来是解决问题1的时候复制的路径/usr/include/python3.7m根本不存在，把命令改为 catkin build -DPYTHON_EXECUTABLE=/usr/bin/python3 -DPYTHON_INCLUDE_DIR=/usr/include/python3.8 即可正常编译成功。 接下来我们使用命令roslaunch panda_moveit_config demo.launch rviz_tutorial:=true，这里会再次遇到一个问题，根据此问题即可解决。 自此我们可以正常运行并且看到Rviz界面，添加MotionPlanning后就可以在RViz界面中见到我们的panda机器人。 我们首先需要知道4种重叠的可视化： \\1. robot在planning environment中的configuration（各个关节的当前角度）。 \\2. robot的plan出来的路径 \\3. 绿色：robot motion planning的开始状态 \\4. 橙色：robot motion planning的结束状态 这些展示的状态可以在上面的motionplanning的下拉框中勾选。Planning scene在show robot visual-&gt;Scene Robot中；planning path在show robot visual-&gt;Planned Path中；开始状态在Query Start State-&gt;Planning Request中；结束状态的设置在Query Start State-&gt;Planning Request。 我们可以拖动各个关节的orientation来设置开始状态和结束状态。然后点击plan可以查看整个的运动轨迹。我们勾选use collision-aware IK就可以使得求解器尝试找到一个collision-free的轨迹；如果没有勾选的话，求解器会允许在过程中碰到collision。不过在可视化的时候，存在碰撞的link无论我们有没有勾选use collision-aware IK，都会被标识成红色。 在MotionPlanning的joint窗口下，我们可以拖动nullspace exploration，可以看到在末端执行器的位姿不变的情况下，其他机械臂的configuration是可以变化的（满足连续关系）。 现在我们可以开始使用panda机器人在moveit rviz插件中进行运动规划。步骤如下： \\1. 把start state设置到期望的位置 \\2. 把goal state设置到期望的位置 \\3. 确保这两个state都没有自己和自己碰撞 \\4. 确保planned path是可见的 \\5. 按下plan键。 我们在Planning窗口中可以选择不同的start和goal states，eg：当前的状态，之前的状态（之前的planning attempt的start state），一个随机采样的configuration，一个有名字的在srdf中定义的state。 我们也可以查看轨迹路径点，只需要在导航栏中打开panels-&gt;motionplanning-slider，即可看到对应的路径点并拖动。 如果我们勾选use cartesian path的话，robot会尝试直线地移动end effector。 [1]Catkin工作原理 https://blog.csdn.net/qq_33876441/article/details/102958248 [2]Catkin conceptual http://wiki.ros.org/catkin/conceptual_overview [3]https://blog.csdn.net/num8owl/article/details/108689843","link":"/2021/10/20/moveit-configuration/"},{"title":"behavior_tree_pre","text":"A internal talk of MVIG lab","link":"/2021/10/20/behavior-tree-pre/"},{"title":"openGL和openCV中的摄像机参数的转化","text":"本文章的主题为研究openGL和openCV中的摄像机参数的转化。 首先要搞清楚，坐标旋转和坐标系旋转的概念。 坐标旋转（点的运动） 如上图，二维坐标系中的绕原点逆时针旋转度，则得到的B点满足： 把上式写成矩阵乘法的形式： 这是点的运动，坐标系（参照系）并没有发生变化 坐标系旋转（基变换） 如上图，把原先的xy基逆时针旋转度到st基，p在xy基下的坐标为，p在st基下的坐标为，求变换关系。 我们有 即 2是坐标系的旋转，点是不动的，得到的是不动的点在变化了的坐标系下的表示 点旋转β相当于坐标系旋转了-β。所以可以直接在1的基础上，把角度反转，就成了坐标系的旋转。 基变换和坐标变换平面解析几何中的直角坐标系有时候需要坐旋转，这实际上是坐标向量绕原点坐旋转，设坐标轴逆时针旋转的角度为，那么不难有，新坐标向量和原坐标向量之间的关系为： OpenCV中的内参矩阵K（计算机视觉——算法和应用P41） 有几种方式来描述上三角矩阵K，一种是 上式使用相互独立的参数来描述x和y维度的焦距和，s项刻画任何可能的传感器轴间的倾斜，这由传感器的安装没有与光轴垂直所引起，而是以像素坐标表达的光心。 在实践中，通过设置和s=0，在很多应用中会使用如下更简单的形式： 通常情况下，通过将原点大致设置在图像的中心，即，其中W和H是图像的高和宽（用像素表示，如600*480），就可以得到仅含一个未知量焦距f的完全可用的摄像机模型。注意上式中的焦距都是以像素为单位表示的，要与现实相机中的毫米焦距等区分开来。 要转化到现实中的距离，我们首先需要知道图像的现实宽度，如，然后我们可以通过公式：，这里的就是视场角，也就是FOV，下图就展示了一个的情况。 为了更好地理解OpenGL中的透视投影，我们先来讨论函数glFrustum。根据OpenGL的官方文档，“glFrustum描述了一个透视矩阵，提供了一个透视投影。”这句话没错，但是只说出了一半实情。事实上，glFrustum做了两件事情，首先它进行透视投影，然后它把结果转换到归一化设备坐标系(NDC)上。前者是投影几何中的常规操作，但是后者是OpenGL中特有的实现细节。 为了讲明白这件事情，我们需要把投影矩阵（Projection Matrix）分成两部分，也就是透视矩阵(Prespective Matrix)和NDC矩阵。 我们的相机内参矩阵可以描述透视投影，所以它是求解出Perspective Matrix的关键。而对于NDC矩阵，我们会使用OpenGL的glOrtho。 第一步：投影变换 我们的3x3内参矩阵K为了能在OpenGL中使用需要两个小的变更，一个是为了正确的裁剪，位置3,3的元素必须为-1，因为OpenGL的摄像机是从原点向z的负半轴看的，所以如果位置3,3的元素为正，摄像机前方的顶点在投影后将具有负的w坐标。原则上，这是可以的，但是由于 OpenGL 执行裁剪的方式，所有这些点都会被裁剪。 所以我们现在有了 对于第二个更改，我们需要保护失去的Z轴的深度信息，所以我们会在内参矩阵的基础上添加一行和一列，即： ，其中 新的第三行保持了Z值的顺序的同时，把-near和-far映射到它们自己（在归一化了w后）。这个结果就是在裁剪平面之间的点依旧在乘上了Perspective Matrix后依旧保持在裁剪平面之间。 第二步：变换到NDC NDC矩阵可以通过glOrtho函数提供。Perspective Matrix把一个视锥空间转化为了一个长方体空间，而glOrtho把长方体空间转化为归一化设备空间。 调用glOrtho需要六个参数left,right,bottom,top,near,far ，其中 调用它的时候，far和near就和前述的一样。而top,bottom,left,right的裁剪平面的选取对应原图像的维度和标定时的坐标规范。 举个例子，如果你的摄像机用WxH的左上角为零点的图像标定了，那么就该使用left = 0, right = W, bottom = H, top = 0，注意到H作为了bottom参数而0作为了top参数，这意味着y轴正半轴是向下的规范。 如果标定时使用的是y轴向上的坐标系，并且原点在图像中心的话，那么就是left = -W/2, right = W/2, bottom = -H/2, top = H/2. 注意到其实glOrtho的参数和透视矩阵有很大的关系，比如说把视景体(viewing volume)向左平移X等价于把主轴向右平移X。而让翻倍就等于让left和right参数减半。很明显，用这两个矩阵来描述这个投影是冗余的，但是分别去考察这两个矩阵允许我们分离相机几何学和图像几何学。 根据文献 https://stackoverflow.com/questions/60430958/understanding-the-view-and-projection-matrix-from-pybullet 在pybullet中， 内参矩阵K为： 和是光心，通常是图像中心。不过有以下不同， \\1. 维度，pybullet保持了第三行和第四列来保持深度信息，这和之前提到的OpenGL相机相同。 \\2. 第四行第三列的元素不是1而是-1 \\3. Pybullet中s=0 \\4. Pybullet中 首先，pybullet使用OpenGL，所以它使用的是列优先的顺序，所以从pybullet中读到的真正的projection matrix应当转置，或者使用numpyarray的order=’F’。 其次，把FOV转化为f的方程如下： 和 因此，pybullet把焦距乘以了2/h，这是因为Pybullet使用归一化设备坐标系（也就是对x除以图像宽度，来归一化到01，再乘以2到02，所以如果我们的光心在图像中间x=1的位置时，那么裁剪平面就归一化到了-1~1）。因此，pybullet的焦距是使用NDC下的正确的焦距长度。 在内参矩阵K中，k和l是mm/px的比例，在使用pybullet时，我们可以认为k=l=1 mm/px，换句话说，在pybullet形式的内参矩阵中，和是以像素为单位的，而整个矩阵的每个元素都是以mm为单位的。 考虑到以上的所有条件，在pybullet中，内参矩阵为： ，其中 把h=1000和FOV=90代入， def computeProjectionMatrixFOV(*args, **kwargs): # real signature unknown “”” Compute a camera projection matrix from fov, aspect ratio, near, far values “”” pass def computeViewMatrix(*args, **kwargs): # real signature unknown “”” Compute a camera viewmatrix from camera eye, target position and up vector “”” pass http://ksimek.github.io/2013/06/03/calibrated_cameras_in_opengl/ https://amytabb.com/ts/2019_06_28/#conversion-corner-1 http://www.info.hiroshima-cu.ac.jp/~miyazaki/knowledge/teche0092.html https://zhuanlan.zhihu.com/p/339199471","link":"/2021/10/15/opencv-opengl-camera-conversion/"},{"title":"pybind11初探","text":"最近项目过程中经常遇到底层为C++代码的情况，作为一个契机，详尽地研究一下pybind11。 虽然无关紧要，但是很在意的问题是11这个数字是哪来的，因为pybind11的介绍是 Seamless operability between C++11 and Python，据称其主要内核代码使用了C++11的语言特性，如匿名函数、元组等。 如上图所示，这是符合pybind11编程要求的c++代码。 图 1 https://zhuanlan.zhihu.com/p/92120645 根据pybind11官网教程，可以使用如下代码进行编译： c++ -O3 -Wall -shared -std=c++11 -fPIC $(python3 -m pybind11 –includes) example.cpp -o example$(python3-config –extension-suffix) -Wall 这个编译选项会强制输出所有警告，用于调试。 -o output_filename 确定输出文件的名称为output_filename。同时这个名称不能和源文件同名。如果不给出这个选项，gcc就给出默认的可执行文件a.out。 -fPIC 生成位置无关代码。 -I/home/baochen/anaconda3/include/python3.8 -I/home/baochen/anaconda3/lib/python3.8/site-packages/pybind11/include .cpython-38-x86_64-linux-gnu.so 案例分析：pybullet-planning中的ikfast编译件 我们考察如何将一个新的机械臂添加到pybullet-planning中。 首先考察已有的franka_panda机械臂，文件夹内容如下： setup.py内容如下： 调用的compile文件如下： 基本上就是传入模块名称和cpp文件的路径，然后构建出一个python可以调用的模块。 同目录的ik.py提供了一个向python暴露的接口，其中的IKFastInfo是一个namedtuple，其实就是指定了我们编译出来的模块名字(setup.py中的robot_name和compile_ikfast中的module_name)","link":"/2021/10/15/pybind11/"},{"title":"mmdection_hammer_segmentation","text":"The first time for me to construct a custom dataset and apply it in the mmdection. The whole pipeline is as follows: I. Collect and prepare the raw dataset. II. Convert the raw dataset into COCO style. III. Add and movify the mask-rcnn configuration in mmdection API to fit our requirement. IV. Train the model by mmdection. V. Evaluate and visualize the test dataset. VI. Modify the result into mask image so that we can insert it smoothly in our pipeline. Because of the fact that the project is still ongoing, the part I and part VI will NOT be described detailedly. 1.The dataset As shown above, it’s a sample from my created dataset which contains a rgb image and corresponding mask. The foreground is a tool(the hammer), and the background contains other unrelated things. At the very beginning when there was no relationship to how to training, it is of top priority to create enough data and split them into train, validate and test dataset. We collected 4535 imgs, 464 imgs and 454 imgs for training, validating and evaluating. 2. The format of COCOAs said by a famous blog, we use COCO to reconstruct our data is not because it is the best format, but it is the most widely used and accepted format. So we need to arrange our dataset in that format. To our relief, there are a lot of tools to help us automatically conduct such procedure, for example, pycococreator. What we need to do is only to set the related INFO, LICENSES, CATEGORIES and corresponding directory. 1234567891011121314151617181920212223242526272829303132333435363738INFO = { &quot;description&quot;: &quot;Hammer Segmentation Dataset&quot;, &quot;url&quot;: &quot;https://github.com/Kami-code&quot;, &quot;version&quot;: &quot;0.1.0&quot;, &quot;year&quot;: 2022, &quot;contributor&quot;: &quot;Kami-code&quot;, &quot;date_created&quot;: datetime.datetime.utcnow().isoformat(' ')}LICENSES = [ { &quot;id&quot;: 1, &quot;name&quot;: &quot;Attribution-NonCommercial-ShareAlike License&quot;, &quot;url&quot;: &quot;http://creativecommons.org/licenses/by-nc-sa/2.0/&quot; }]CATEGORIES = [ { 'id': 1, 'name': 'hammer', 'supercategory': 'none', },]...def main(): ROOT_DIR = 'hammer_' + MODE IMAGE_DIR = os.path.join(ROOT_DIR, &quot;rgb&quot;) ANNOTATION_DIR = os.path.join(ROOT_DIR, &quot;mask&quot;) coco_output = { &quot;info&quot;: INFO, &quot;licenses&quot;: LICENSES, &quot;categories&quot;: CATEGORIES, &quot;images&quot;: [], &quot;annotations&quot;: [] }... The structure is from the blog. You can refer to it to create your custom dataset. What I want to mention is if we have a lot of mask of different objects on a single image(though not in our current settings), the annotation_id should start from 0 and increase for each mask, while the meaning of image_id is easily understood. By the way, it seems no need to make annotation_id the same as “id” in CATEGORIES, so making it start from zero is necessary. After running the script of pycococreator, we get a single json file named “instance_train2022.json”. We can use the visualizer script provided in that repo to visualize to check whether we successfully get a COCO json file. COCO will contain the information which provided in the mask.png file using contour algorithm.(Finally, they will be stored in polygons format.) Since now, we have successfully got our first COCO dataset! 3. MMDectionMMDetection is an open source object detection toolbox based on PyTorch. It is a part of the OpenMMLab project. It provides a abstraction on the PyTorch which enables me train my network without writing any code. Frankly speaking, I will not use it if I have enough time to dig deep in mask-rcnn, cause it makes me tightly rely on the API, which do no good for my career. For now, given limited time, I have to use it. To use it, I mainly refer to the proceduce in this zhihu blog. But it also contains some frustrating bug, which I will explain below. This is the files need to add and modify if we want to add a new dataset to train by mask_rcnn. (marked in green means the newly-added file and marked in brown means the file needed to be modified) Let’s find how exactly these files works. mmdection/configs/mask_rcnn/my_mask_rcnn.py 123456_base_ = [ '../_base_/models/my_mask_rcnn_r50_fpn.py', '../common/my_coco_instance.py','../_base_/schedules/schedule_2x.py' ,'../_base_/default_runtime.py',] The file just links all the files we added. Next, we should define our own dataset structure, since we’ve got the dataset in COCO format. We can replicate the coco.py and change a little to get our “keto_coco.py”. mmdection/mmdet/datasets/coco.py 123456789101112131415161718@DATASETS.register_module()class CocoDataset(CustomDataset): CLASSES = ('person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush') ... mmdection/mmdet/datasets/keto_coco.py 12345@DATASETS.register_module()class KpCocoDataset(CustomDataset): CLASSES = ['hammer',] ... As we can see, what we do is just set the CLASSES variable into a list only containing one class hammer. The reason that we change the tuple to list seems to be a version-related problem that the low-level function will misleadingly parse the wrong type. mmdection/mmdet/datasets/init.py 12345678910__all__ = [ 'CustomDataset', 'XMLDataset', 'CocoDataset', 'DeepFashionDataset', 'VOCDataset', 'CityscapesDataset', 'LVISDataset', 'LVISV05Dataset', 'LVISV1Dataset', 'GroupSampler', 'DistributedGroupSampler', 'DistributedSampler', 'build_dataloader', 'ConcatDataset', 'RepeatDataset', 'ClassBalancedDataset', 'WIDERFaceDataset', 'DATASETS', 'PIPELINES', 'build_dataset', 'replace_ImageToTensor', 'get_loading_pipeline', 'NumClassCheckHook', 'CocoPanopticDataset', 'MultiImageMixDataset', 'KpCocoDataset'] Since we defined our KpCocoDataset, to make it register into the dataset collection, we need to add the structure name into its init.py and recomplie the module. In class_names.py, we should also add a simple function to get the classes of our defined dataset. mmdection/mmdet/core/evaluation/class_name.py 1234...def kp_coco_classes(): return ['hammer']... Also, a change in init.py is necessary. But there is no explicit call of this function, I GUESS the function may not be called or is called by its name in some format. The my_mask_rcnn_r50_fpn.py is replicated from the mask_rcnn_r50_fpn.py. We just change the class_num parameter 80 to 1. In my_coco_instance.py, we defined the path of the three COCO json files, and train and test pipeline. mmdection/configs/common/my_coco_instance.py 123456789101112131415161718192021222324...data = dict( samples_per_gpu=2, workers_per_gpu=2, train=dict( type='RepeatDataset', times=4, dataset=dict( type=dataset_type, ann_file=data_root + 'annotations/instances_train2022.json', img_prefix=data_root + 'train2022/', pipeline=train_pipeline)), val=dict( type=dataset_type, ann_file=data_root + 'annotations/instances_validate2022.json', img_prefix=data_root + 'validate2022/', pipeline=test_pipeline), test=dict( type=dataset_type, ann_file=data_root + 'annotations/instances_test2022.json', img_prefix=data_root + 'test2022/', pipeline=test_pipeline),)... And we will train the data on a single GPU with 2 samples_per_gpu, so we should downsize the learning rate defined in schedule_2x.py 8 times, because it’s default value is assuming the training is on 8 GPUs with 2 samples_per_gpu. 4. Train our modelWe can use the following command line to start training process. Though I have spent much time in debugging in the process. If we correctly config the mentioned files, it will finally works. 1python tools/train.py configs/mask_rcnn/my_mask_rcnn.py When training, we can read the logs to make sure we are on the right track. 12021-10-27 15:16:22,077 - mmdet - INFO - Epoch [1][2400/9070] lr: 2.500e-03, eta: 17:30:32, time: 0.288, data_time: 0.010, memory: 7875, loss_rpn_cls: 0.0216, loss_rpn_bbox: 0.0081, loss_cls: 0.0925, acc: 97.2070, loss_bbox: 0.1120, loss_mask: 0.1732, loss: 0.4074 How are the batches each epoch 9070 calculated? Recall that we set the training set to be repeat dataset 4, which means the total training dataset is 4535 * 4 = 18140. And we configure the training process on a single with samples_per_gpu = 2. So we get batch_size = 18140 / 2 = 9070. 1234567891011121314151617181920212223242021-10-27 22:03:28,363 - mmdet - INFO - Evaluating segm...Loading and preparing results...DONE (t=0.04s)creating index...index created!Running per image evaluation...Evaluate annotation type *segm*DONE (t=0.46s).Accumulating evaluation results...DONE (t=0.05s). Average Precision (AP) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.367 Average Precision (AP) @[ IoU=0.50 | area= all | maxDets=1000 ] = 0.416 Average Precision (AP) @[ IoU=0.75 | area= all | maxDets=1000 ] = 0.415 Average Precision (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = -1.000 Average Precision (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.365 Average Precision (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.369 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=100 ] = 0.373 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=300 ] = 0.373 Average Recall (AR) @[ IoU=0.50:0.95 | area= all | maxDets=1000 ] = 0.373 Average Recall (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = -1.000 Average Recall (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.369 Average Recall (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.3752021-10-27 22:03:28,939 - mmdet - INFO - Exp name: my_mask_rcnn.py2021-10-27 22:03:28,939 - mmdet - INFO - Epoch(val) [9][464] bbox_mAP: 0.3800, bbox_mAP_50: 0.4210, bbox_mAP_75: 0.4090, bbox_mAP_s: -1.0000, bbox_mAP_m: 0.3880, bbox_mAP_l: 0.3760, bbox_mAP_copypaste: 0.380 0.421 0.409 -1.000 0.388 0.376, segm_mAP: 0.3670, segm_mAP_50: 0.4160, segm_mAP_75: 0.4150, segm_mAP_s: -1.0000, segm_mAP_m: 0.3650, segm_mAP_l: 0.3690, segm_mAP_copypaste: 0.367 0.416 0.415 -1.000 0.365 0.369 Some results are as shown in the sheet. In this article, we don’t detailedly find out each number means. 5. Eval and visualize our modelWe can run the following script to visualize our model. 12345678910111213from mmdet.apis import init_detector, inference_detector, show_result_pyplotif __name__ == '__main__': config_file = 'mmdetection/configs/mask_rcnn/my_mask_rcnn.py' # url: https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth checkpoint_file = 'mmdetection/work_dirs/my_mask_rcnn/latest.pth' device = 'cpu' # init a detector model = init_detector(config_file, checkpoint_file, device=device) # inference the demo image img = '../tool_seg_data/neee/rgb/1906.jpeg' result = inference_detector(model, img) show_result_pyplot(model, img, result, score_thr=0.3) Thank God we finally successfully train the segmentation network.","link":"/2021/10/29/mmdection-hammer-segmentation/"},{"title":"pybullet-planning源码解析","text":"我们尝试来阅读一下pybullet-planning中和ik、planning相关的源码。 我们尝试从test_franka.py入手 首先是start_pose是起始的关于tool_link的6D位姿，也就是得到pos, orn 同理，end_pose是对于start_pose施加了一个相对位移z=-distance(1m)以后的位置 我们可以发现，每一次移动的位置确实是沿着z轴相对位置后退0.1m(因为我们分了十步去处理后退1m的最终目标) 分析multiply的源码，有比较陌生的poses参数，单个的意思其实就是按照数组形式处理传入的不定长个参数 术语表： solve joints: 需要用来求解ik的关节。 free joints: 需要在ik计算前指定的关节，这些值在运行时是已知的，但是在ik创建的时候是未知。(not known at IK generation time) 以panda_franka为例，传入的ikfast_info为IKFastInfo(module_name=’franla_panda.ikfast_panda_arm’, base_link=’panda_link0’, ee_link=’panda_link8’, free_joints=[‘panda_joint7’])。因为panda是7个自由度的机器人，所以需要指定一个joint为free joint。 以UR5为例，传入的infast_info为IKFastInfo(module_name=’ur5.ikfast_ur5_arm’, base_link=’base_link’, ee_link=’ee_link’, free_joints=[])。因为UR5是6个自由度的机器人，所以不需要指定free joint。 link_from_name：根据name得到link id 换句话说，对于一个link，施加get_ordered_ancesters以后可以得到包括这个link在内的其祖先link 以panda为例： get_ordered_ancestors(panda, ee_link)：[0, 1, 2, 3, 4, 5, 6, 7] get_ordered_ancesters(panda, tool_link)：[0, 1, 2, 3, 4, 5, 6, 7, 8] first_joint：连接了base_link的joint prune_fixed_joints：去除掉固定的joint以后，需要纳入计算的joint。 ik_joints：[0, 1, 2, 3, 4, 5, 6] free_joints：[6] assert set(free_joints) &lt;= set(ik_joints)：保证需要设置的free_joint包含在ik_joints内。 assert len(ik_joints) == 6 + len(free_joints)：保证剔除掉free_joints后为6个自由度，比如7自由度关节的机械臂需要1个free_joint，6自由度关节的机械臂需要0个free_joint。 difference_fn是一个求差值的函数，对于普通的joint来说，是直接两个参数相减，对于circular joint会有一些特殊的计算方式，但是circular joint在代码中的定义是upper limit &lt; lower limit的情况，并且google也并没有找到什么有用的信息，考虑到我们目前使用的机器人都没有circular joint，就直接当做两个值相减就可以了。 get_length函数就是单纯的求n-范数，范数通过norm参数传进去。 其实closet_inverse_kinematics的后半部分只是在对求出来的解进行排序，找到一个各关节位姿变化的距离最小的解输出。而前半部分是通过generator = ikfast_inverse_kinematics(…..)求解得到了所有的可行解。 在研究ikfast_inverse_kinematics之前，我们先来搞懂interval_generator Np.random.uniform从一个均匀分布[0,1)中随机采样，获得d个数据 Halton sequence:Halton序列是一种为数值方法（如蒙特卡洛模拟算法）产生顶点的系列生成算法。虽然这些序列是以确定的方法算出来的，但它们的偏差很小。也就是说，在大多数情况下这些序列可以看成是随机的。Halton系列于1960年提出，当时是作为quasi-random 数字序列的一个例子。 所以unit_generator是从均匀分布[0,1)中采样d个数据。而在interval_generator这个函数中，也就是随机采样d个0~1之间的权重，传入convex_conbination中。 所以其实就是在lower和upper之间随机取d组点，如果joint lower limit = joint upper limit，那么自然就直接取相等。 所以，调用interval_generator会返回一个满足条件的joints的各个角度。 接下来，我们再来尝试搞懂ikfast_inverse_kinematics \\1. 首先import_ikfast函数会根据我们传入的ikfast_info去import对应机器人的cpp编译出来的模组，每次需要使用新的机器人时，都要准备好这件事情。 \\2. ik_joints和free_joints相对比较容易理解，上文中有提到过这件事情。 \\3. lower_limits和upper_limits就是joint的上下界，free_deltas不详。 \\4. Islice(generator, max_attempts)，迭代器generator生成max_attempt个proposal后结束 所以generator迭代器将原先的joint pose和随机数采样得到的joint pose可行解传入compute_inverse_kinematics中，会继续被传入到每个机械臂特有的ikfast cpp中，具体作用不详，在ur5的ikfast代码中，似乎是在某些条件下会使用采样数据作为某个joint的值。 在ikfast cpp的源码中，我们可以看到这样的表述Computes all IK solutions given a end effector coordinates and the free joints. pfree is an array specifying the free joints of the chain. 换句话说，我们传入的采样得到的sampled参数实际上是指定了对应free joint的本次求解中的值，因为free joint是需要我们指定的，这无可厚非。 \\5. 在传入对应参数以后，满足条件的解会yield出来，所以其实ikfast_inverse_kinematics生成了一个产生对应configuration下的ik解的迭代器。solutions = list(generator)，也就是把这些解都放进list中，因为实现传入的时候，我们保证了要么max_attempts不为INF，要么max_times不为INF，所以不会出现generator会成为一个无限长的迭代器的情况，如果max_times设大了，可能会影响实时性。 接下来，我们来考察pybullet_inverse_kinematics函数。 具体流程是调用multiple_sub_inverse_kinematics，然后最后可以得到一堆解。 它实现逻辑是这样的： 每次需要计算ik的时候，sample一些在各个joint limit范围内的整个机械臂的姿态，创建对应的subrobot，然后subrobot会以ik target pose为目标，迭代式地求解这个ik，也就是求解一次ik，set到对应位置，再求解ik……，迭代约200次，然后最后check最终的位姿和ik的target位姿的差值是不是小于某个阈值，如果小于则成功。 其中sub_robot其实就是在Pybullet环境中创建一个不考虑碰撞的、不可见的、相同pos的robot，只用来计算ik，在计算完成后，就会把sub_robot删除。 Motion-planning 首先是refine_path这个函数，这个函数比较简单，也就是传入一组waypoint（每个waypoint是joint的一组值），refine_path会在每相邻两个waypoint中插入num_steps个中间点，作为新的path。 get_extend_fn(resolution)这个函数也是返回一个函数是根据resolutions确定在两个waypoint之间需要插值几个waypoint点，resolution默认值为3度，也就是0.05弧度。然后调用之前所提到的get_refine_fn函数。resolution对于不同的joint可以设置不同的joint。 remove_redundant其实就是把位姿path中太近（2范数小于1e-3）的点移除。 模拟一下这个情况，首先把path[0]加入waypoint中，然后考察下一个path[i]。对于对于相邻的两次，考察相邻path[i - 1]-&gt;path[i]和path[i]-&gt;path[i+1]这两个向量，我们规范化后考察其方向，如果方向一致的话就不再加到waypoints中。所以相对于path是path中一些平均相距为resolution的位姿。 interpolate_joint_waypoints，也就是对给定的waypoint，通过不同joint的resolution来插值得到一个均匀resolution的waypoints的路径。并且如果其中插值的路径中检测到了碰撞，直接输出空路径。 其实也是带了碰撞检测的interpolate_joint_waypoint。碰撞检测发生在枚举waypoint的需要插值的节点上。注意和interpolate_joint_waypoints中碰撞检测的对象的区分（一个是需要插值的节点，一个是插值后的新增节点）。 可以看到机器学习分为两个阶段，比如倒数第三段，check_link_pair循环中，就是判断自己的link之间的碰撞，也就是self-collision。在倒数第二段，其实就是check我们的body和obstacles之间的collision。 关于这个函数中提供的碰撞检测，有aabb碰撞检测和pair_wise_collision碰撞检测。Aabb比较容易理解，而pair_wise_collision主要是通过枚举需要判断的两个物体之间的笛卡尔积，分别通过p.getClosestPoints是否等于0来判断。","link":"/2021/10/15/pybullet-planning/"},{"title":"SE335_Compliers_lab2","text":"总而言之前前后后好几天时间都在搞这个lab，不过主要花的时间可能是大半天时间。为了留下点东西，姑且总结一下这个lab。 其实这个lab可以说是烦而不难，基本上搞清楚flex c++的执行流，就完全可以干掉这个lab。 上面是一些无关痛痒的指令，算是第一步就可以很轻易写出来的东西。 一、处理嵌套注解 嵌套注解的处理其实从原理上就和括号匹配一样，至少要维护一个整形作为匹配的进度（/加一、/减一，最终为0时匹配完毕）。但是这样实现就要区分出在匹配完毕时（非注解情况下）和匹配进行时（注解情况下）的不同执行流，需要更改全部代码。其实，仔细查看lab2的提示和flex c++的文档就可以发现它提供了start condition来处理不同情况下的处理，这是一个很好用的工具。如上图所示，一旦匹配到了开始符号/*，那么自动push一个COMMENT状态即可；一旦匹配到了结束符号，那么就从栈中pop一个状态。 二、字符串的处理这部分是我debug了最久的地方，因为经常会出现匹配上了，但是开始的位置和标准答案差一个的情况、或者字符串后的各种符号平移了几位的情况，这是因为多种原因造成的： \\1. 比如A=”5”的情况，在匹配第一个”的时候，如果没有adjust，最终的开始位置就会在等号处。 \\2. 匹配转义字符时，不能根据最终转义完成的字符串的长度来计算adjust，这样会导致adjust数量过少。 首先是匹配普通字符串的情况，lex代码如下： 由于adjust函数会自动根据matched的字符串的长度来做调整位置，所以我们自己添加了一个adjustByLen，里面添加了对转义字符的offset。接下来是对转义字符的支持，主要可以分为四类： \\1. ^A, ^B, …., ^Z，具体其含义我们不细究，反正查ASCII表可知其值为1~26。 \\2. \\xxx，其中x为0~9的数字，也就是直接指定ASCII码。 \\3. \\n, \\t, \\”等，直接转义为对应字符即可。 \\4. \\跟了一个换行符的情况，我们需要支付字符串跨行输入，这个下一节再说。 总体逻辑就是trim我们的matched string，把转义使用的\\x都修剪掉，换成真的对应的转义字符。注意，我们需要累积计算我们的offset，这样我们才能知道最终我们需要移动多少位。 接下来是例子52中出现的这种情况： 我们自然也需要给予支持，也就是+一个换行符，自动进入ignore状态，等到读到下一个\\后，才退出到正常的字符串处理状态。注意ignore了多少个字符也需要添加进offset中。","link":"/2021/10/17/complier-lab2/"}],"tags":[{"name":"SE3353","slug":"SE3353","link":"/tags/SE3353/"},{"name":"WebService","slug":"WebService","link":"/tags/WebService/"},{"name":"Search","slug":"Search","link":"/tags/Search/"},{"name":"ROS","slug":"ROS","link":"/tags/ROS/"},{"name":"moveit","slug":"moveit","link":"/tags/moveit/"},{"name":"behavior tree","slug":"behavior-tree","link":"/tags/behavior-tree/"},{"name":"OpenCV","slug":"OpenCV","link":"/tags/OpenCV/"},{"name":"OpenGL","slug":"OpenGL","link":"/tags/OpenGL/"},{"name":"Camera","slug":"Camera","link":"/tags/Camera/"},{"name":"Pybind11","slug":"Pybind11","link":"/tags/Pybind11/"},{"name":"mmdection","slug":"mmdection","link":"/tags/mmdection/"},{"name":"segmentation","slug":"segmentation","link":"/tags/segmentation/"},{"name":"deeplearning","slug":"deeplearning","link":"/tags/deeplearning/"},{"name":"Pybullet","slug":"Pybullet","link":"/tags/Pybullet/"},{"name":"compliers","slug":"compliers","link":"/tags/compliers/"},{"name":"lab","slug":"lab","link":"/tags/lab/"}],"categories":[]}