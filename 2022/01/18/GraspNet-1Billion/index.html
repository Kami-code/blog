<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>(CVPR2020)GraspNet-1Billion - Ryan &#039;s website</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Ryan&#039;s blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Ryan&#039;s blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="​    最近在研究各种各样的GraspNet，组内浩树学长的GraspNet虽然我已经用了很久，但是在方法论和具体实现上一直没有时间去研究，借着寒假的机会来集中学习一下。"><meta property="og:type" content="blog"><meta property="og:title" content="(CVPR2020)GraspNet-1Billion"><meta property="og:url" content="https://kami-code.com/2022/01/18/GraspNet-1Billion/"><meta property="og:site_name" content="Ryan &#039;s website"><meta property="og:description" content="​    最近在研究各种各样的GraspNet，组内浩树学长的GraspNet虽然我已经用了很久，但是在方法论和具体实现上一直没有时间去研究，借着寒假的机会来集中学习一下。"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://kami-code.com/2022/01/18/GraspNet-1Billion/image-20220118155632080.png"><meta property="article:published_time" content="2022-01-18T06:33:35.000Z"><meta property="article:modified_time" content="2022-02-04T11:44:08.540Z"><meta property="article:author" content="Kami-code"><meta property="article:tag" content="GraspNet"><meta property="article:tag" content="CVPR"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/2022/01/18/GraspNet-1Billion/image-20220118155632080.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://kami-code.com/2022/01/18/GraspNet-1Billion/"},"headline":"(CVPR2020)GraspNet-1Billion","image":["https://kami-code.com/2022/01/18/GraspNet-1Billion/image-20220118155632080.png"],"datePublished":"2022-01-18T06:33:35.000Z","dateModified":"2022-02-04T11:44:08.540Z","author":{"@type":"Person","name":"Kami-code"},"publisher":{"@type":"Organization","name":"Ryan 's website","logo":{"@type":"ImageObject","url":"https://kami-code.com/img/logo.svg"}},"description":"​    最近在研究各种各样的GraspNet，组内浩树学长的GraspNet虽然我已经用了很久，但是在方法论和具体实现上一直没有时间去研究，借着寒假的机会来集中学习一下。"}</script><link rel="canonical" href="https://kami-code.com/2022/01/18/GraspNet-1Billion/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Ryan &#039;s website" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-01-18T06:33:35.000Z" title="2022/1/18 下午2:33:35">2022-01-18</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-02-04T11:44:08.540Z" title="2022/2/4 下午7:44:08">2022-02-04</time></span><span class="level-item">33 minutes read (About 4946 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">(CVPR2020)GraspNet-1Billion</h1><div class="content"><p>​    最近在研究各种各样的GraspNet，组内浩树学长的<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Fang_GraspNet-1Billion_A_Large-Scale_Benchmark_for_General_Object_Grasping_CVPR_2020_paper.pdf">GraspNet</a>虽然我已经用了很久，但是在方法论和具体实现上一直没有时间去研究，借着寒假的机会来集中学习一下。</p>
<span id="more"></span>

<p>​    其实GraspNet-1Billion包含三部分，大规模的数据集、网络和benchmark。</p>
<h1 id="数据集部分"><a href="#数据集部分" class="headerlink" title="数据集部分"></a>数据集部分</h1><p>​    数据集包含88个日常常见的物体的3D模型。数据是从190个clutter scene中收集的，每个scene中，2个深度摄像机拍512张深度图，共97280张图片。每张图片中，我们通过计算force closure的方法稠密标注了6D grasp pose。每个场景中通常会有300万到900万个Grasp Pose。</p>
<h2 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a>数据收集</h2><p>​    我原以为是仿真器里渲染的，居然真的是在现实世界中采集的，这样的话就不存在什么sim2real的问题了。两个深度相机同时拍摄场景并且合成成一个点云。摄像机所固连的机械臂的末端执行器在单位球面上找到了256个位置拍摄。地面上还放置了ArUco marker来协助摄像机标定，这样避免了计算fk所带来的误差。</p>
<h2 id="数据标定"><a href="#数据标定" class="headerlink" title="数据标定"></a>数据标定</h2><p>​    有了这97280张图以后，因为每256个位置的相对位置都是已知的，所以我们只需要标定每个的第一帧即可，即380张。但是Grasp Pose分布在一个大型的连续搜索空间中，是标不完的，手动标是巨大的工作量。因为我们的每个物体是已知的，文章中提出了一个2阶段的Grasp Pose标注方法。</p>
<p>​    首先，我们在单个物体上采样并标注Grasp Pose。为了达到这个目的，我们先把单个物体的网格模型降采样成均匀分布的Voxel space，其中的每个点称为grasp point。对于每个Grasp Point，我们从单位球找到V个均匀分布的approaching vector。然后，我们在二维网格$D\times A$上搜索（其中D是夹爪深度，而A是in-plane旋转角度）。</p>
<p>​    我们使用理论计算的方法来对每个Grasp打分，force-closure指标是很有用的：给定一个Grasp Pose、相关的Object以及摩擦系数$\mu$，它可以输出一个二分类的标签来判断这个Grasp是否可以在对应的摩擦系数下被抓起来。因为force-closure是基于物理的，所以比较鲁棒。此处我们采用<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.06267">24</a>所提到的一个改进版本，使用$\Delta\mu=0.1$作为间隔，我们逐渐从1递减到0.1，直到Grasp不再antipodal。因为具有更低摩擦系数$\mu$的成功抓取更容易成功，所以我们定义Grasp的成功率为：$s=1.1-\mu$。</p>
<p>​    对于每个scene，我们把对应单个物体的Grasp投影到clutter的场景中，此外还做了collision-check来避免非法的情况。在这两部以后，我们就对每个scene创建了稠密的抓取集合$G_{(w)}$。根据统计，数据集中正例和反例的比例为1:2。</p>
<h2 id="数据集评估"><a href="#数据集评估" class="headerlink" title="数据集评估"></a>数据集评估</h2><p>​    对于190个场景，100个用来训练、90个用来测试。我们进一步把测试集分类为3种： 30 scenes with seen objects, 30 with unseen but similar objects and 30 for novel objects。</p>
<p>​    为了评估Grasp Pose的预测表现，之前的方法通常认为一个正确的Grasp需要满足：</p>
<ul>
<li><p>rotation error小于$30^{\circ}$</p>
</li>
<li><p>矩形的IOU（Intersection over Union）大于0.25</p>
</li>
</ul>
<p>​    但是这样的度量指标有一些问题，比如它只能评估Grasp Pose的矩阵表示方法，并且它的错误容忍度太高了，康奈尔数据集已经可以达到99%的进度了。这篇工作提出了一个在线的评估算法来评估Grasp的精度。</p>
<p>​    我们首先演示如何来分类是否一个Grasp Pose是true positive的。对于每个预测出来的Grasp Pose$\hat{P}_i$，我们首先做collision-checking，第二部就是我们通过force-closure来判断在给定不同的摩擦系数$\mu$下是否可以抓取。</p>
<p>​    对于cluttered scene，我们的Grasp预测算法会预测出多个Grasp Pose。因为对于抓取来说，我们通常是预测以后再执行的，所以我们认为true positive的比例是最重要的。因此，我们采用Precision@k作为我们的评估指标，也就是前k个抓取的精度。$AP_\mu$在摩擦系数$\mu$代表Precision@k的均值（k从1取到50）。和COCO数据集类似，我们在不同的摩擦系数$\mu$来计算$AP_\mu$。为了避免相同的Grasp Pose，在评估之前，我们对Grasp Pose来使用pose-NMS。</p>
<p><img src="/2022/01/18/GraspNet-1Billion/image-20220118155632080.png"></p>
<p>​        网络结构整体上我觉得和VoteNet非常接近，也是生成M个seeds。然后通过ApproachNet把Grasp的接近向量做一个多分类出来。然后我们对这M个grasp proposal做KNN聚类，聚出来K个grasp proposal。对于每个接近向量，我们继续做多分类，在$D\times A$上找到一个分数最大的夹爪渐进距离和角度。</p>
<h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><p>​    对于每个候选点，我们为它分配一个二分类标签来表示它是否是可以抓取的。</p>
<ul>
<li>对于那些不在物体上的点，我们直接认为是失败的样本。</li>
<li>对于在物体上的点，我们在5mm半径内寻找它是否存在至少一个graspable的GT。如果存在，对应的graspable标签即为1。</li>
<li>如果物体上的点附近5mm半径内没有graspable的GT，那么我们忽略掉这种情况，因为对我们的训练没有贡献。</li>
</ul>
<p>​    对于每个点，我们有V个approaching vector，我们定义第i个点的第j个approaching vector为$v_{ij}$。我们然后寻找它对应的GT向量：$\hat{v}<em>{ij}$。类似地，我们只考虑相差在5度以内的。最终，我们的损失函数定义如下：<br>$$<br>L^A({c_i},{s</em>{ij}})=\frac{1}{N_{cls}}\sum_iL_{cls}(c_i,c_i^*)+\lambda_1\frac{1}{N_{reg}}\sum_i\sum_jc^*<em>i\mathbf{1}(|v</em>{ij},v^*_{ij}|&lt;5^{\circ})L_{reg}(s_{ij},s_{ij}^*)<br>$$<br>​    其中$s_{ij}$代表网络预测的第i个点的confidence score。而$s_{ij}^*$是对应的GT，是通过EQ2选出来的最大的grasp的置信度。$|v_{ij},v^*_{ij}|$代表的就是角度差。指示函数$\mathbf{1}()$约束了一个approaching vector所产生的loss是通过它附近5度内的GT所提供的。    </p>
<p>​    这个损失函数前半部分就是说，希望对每个点可以正确地二分类到graspable还是not graspable。后半部分就是说，对于每个graspable的点（满足$c_i^*==1$），它存在V个approaching vector，我们希望通过稠密的Grasp GT来让每个approaching vector都能预测出一个confidence score，也就是对应Grasp数据集创建时候，所计算出来的成功率。</p>
<h2 id="Operation-Network"><a href="#Operation-Network" class="headerlink" title="Operation Network"></a>Operation Network</h2><p>​    在得到了graspable points的approaching vector之后，我们需要进一步预测in-plane rotation, approaching distance, gripper width和grasp confidence。</p>
<h3 id="Cylinder-Region-Transformation"><a href="#Cylinder-Region-Transformation" class="headerlink" title="Cylinder Region Transformation"></a>Cylinder Region Transformation</h3><p>​    文章提出了一个统一的Grasp表示方式。因为approaching distance相对不那么敏感，所以分为了K个bin，对于每个给定的distance $d_k$，我们在圆柱中沿着approaching vector采样一些点。这些采样的点会转化到一个新的坐标系下，其原点是Grasp point，而z轴就是approaching vector $v_{ij}$。</p>
<h3 id="Rotation和Width"><a href="#Rotation和Width" class="headerlink" title="Rotation和Width"></a>Rotation和Width</h3><p>​    在之前的文章中，证明了预测in-plane rotation时，分类比起回归有更好的效果。所以，rotation network把对齐过的点云作为输入，输出分类的分数、对每个rotation bin的归一化过的残差，以及对应的grasp width和grasp confidence。因为夹爪是对称的，所以我们只需要预测0~180度即可。目标函数如下：<br>$$<br>L^R(R_{ij},S_{ij},{W_{ij}})=\sum_{d=1}^K\left(\frac{1}{N_{cls}}\sum_{ij}L^d_{cls}(R_{ij},R_{ij}^*)+\lambda_2\frac{1}{N_{reg}}\sum_{ij}L^d_{reg}(S_{ij},S^*_{ij})+\lambda_3\frac{1}{N_{reg}}\sum_{ij}L^d_{reg}(W_{ij}, W^*_{ij})\right)<br>$$<br>​    其中$R_{ij}$代表binned rotation degree, $S_{ij}$代表grasp confidence score，$W_{ij}$代表夹爪闭合宽度，$d$代表approaching distance。其中$L^d$代表的是第d个binned distance的loss。此处的$L_{cls}$代表的是多分类任务的交叉熵损失。</p>
<h3 id="Tolerance-Network"><a href="#Tolerance-Network" class="headerlink" title="Tolerance Network"></a>Tolerance Network</h3><p>​    现在我们已经有了一个end-to-end的网络了。本文进一步提出了Grasp Affinity Field（GAFs）的概念，它可以提升预测出来的Grasp的鲁棒性。因为合法的Grasp Pose是无限的，我们希望能够挑选出那些可以容忍更大error的鲁棒的Grasp。所以，GAFs就学习的是每个grasp对于扰动的鲁棒性。</p>
<p>​    给定一个GT的grasp pose，我们在球空间中搜索它的邻域，来找到满足grasp score &gt; 0.5的最远的距离作为GATs。损失函数如下：<br>$$<br>L^F(A_{ij})=\frac{1}{N_{reg}}\sum_{d=1}^K\sum_{ij}L^d_{reg}(T_{ij},T_{ij}^*)<br>$$<br>​    其中$T_{ij}$代表了grasp pose可以忍受的最大的扰动。</p>
<p>​    在训练过程中，网络的总的目标函数如下<br>$$<br>L=L^A({c_i},{s_{ij}})+\alpha L^R(R_{ij}, S_{ij}, W_{ij})+\beta L^F(T_{ij})<br>$$<br>​    在推理阶段，我们把grasp根据分数分成10个bin，然后在每个bin中根据tolerance network的计算出来的扰动程度来排序。</p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="GT-Evaluation"><a href="#GT-Evaluation" class="headerlink" title="GT Evaluation"></a>GT Evaluation</h3><p>​    为了评估预测的Grasp Pose，本文设立了一个真机实验，因为在现实中需要获取到物体的6D Pose才能做投影，把ArUco code贴在物体上。</p>
<p>​    </p>
<p>​    对于训练过程，我们把in-plane rotational angle分成了12个bin、approaching distance分成了4个bin(0.01, 0.02, 0.03, 0.04)m。我们设置$M=1024$和$V=300$。我们的ApproachNet有MLP(256, 302, 302)，OperationNet有MLP(128, 128, 36)和ToleranceNet有MLP(128, 64, 12)。</p>
<h1 id="具体代码实现"><a href="#具体代码实现" class="headerlink" title="具体代码实现"></a>具体代码实现</h1><h2 id="数据集格式"><a href="#数据集格式" class="headerlink" title="数据集格式"></a>数据集格式</h2><p>​        数据集的官网说明如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">1. Download, unzip all the files and place them in the following structure, </span><br><span class="line">   the train images and test images contain the 190 scenes in total.</span><br><span class="line">|-- graspnet</span><br><span class="line">    |-- scenes</span><br><span class="line">    |   |-- scene_0000/</span><br><span class="line">    |   |-- scene_0001/</span><br><span class="line">    |   |-- ... ...</span><br><span class="line">    |   `-- scene_0189/</span><br><span class="line">    |</span><br><span class="line">    |</span><br><span class="line">    |-- models</span><br><span class="line">    |   |-- 000/</span><br><span class="line">    |   |-- 001/</span><br><span class="line">    |   |-- ...</span><br><span class="line">    |   `-- 087/</span><br><span class="line">    |</span><br><span class="line">    |</span><br><span class="line">    |-- dex_models(optional but strongly recommended for accelerating evaluation)</span><br><span class="line">    |   |-- 000.pkl</span><br><span class="line">    |   |-- 001.pkl</span><br><span class="line">    |   |-- ...</span><br><span class="line">    |   `-- 087.pkl</span><br><span class="line">    |   </span><br><span class="line">    |</span><br><span class="line">    |-- grasp_label</span><br><span class="line">    |   |-- 000_labels.npz</span><br><span class="line">    |   |-- 001_labels.npz</span><br><span class="line">    |   |-- ...</span><br><span class="line">    |   `-- 087_labels.npz</span><br><span class="line">    |</span><br><span class="line">    |</span><br><span class="line">    `-- collision_label</span><br><span class="line">        |-- scene_0000/</span><br><span class="line">        |-- scene_0001/</span><br><span class="line">        |-- ... ...</span><br><span class="line">        `-- scene_0189/</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2. Detail structure of each scene</span><br><span class="line">|-- scenes</span><br><span class="line">    |-- scene_0000</span><br><span class="line">    |   |-- object_id_list.txt              # objects&#x27; id that appear in this scene, 0-indexed</span><br><span class="line">    |   |-- rs_wrt_kn.npy                   # realsense camera pose with respect to kinect, shape: 256x(4x4)</span><br><span class="line">    |   |-- kinect                          # data of kinect camera</span><br><span class="line">    |   |   |-- rgb                         </span><br><span class="line">    |   |   |   |-- 0000.png to 0255.png    # 256 rgb images</span><br><span class="line">    |   |   `-- depth</span><br><span class="line">    |   |   |   |-- 0000.png to 0255.png    # 256 depth images</span><br><span class="line">    |   |   `-- label</span><br><span class="line">    |   |   |   |-- 0000.png to 0255.png    # 256 object mask images, 0 is background, 1-88 denotes each object (1-indexed), same format as YCB-Video dataset</span><br><span class="line">    |   |   `-- annotations</span><br><span class="line">    |   |   |   |-- 0000.xml to 0255.xml    # 256 object 6d pose annotation. ‘pos_in_world&#x27; and&#x27;ori_in_world&#x27; denotes position and orientation w.r.t the camera frame. </span><br><span class="line">    |   |   `-- meta</span><br><span class="line">    |   |   |   |-- 0000.mat to 0255.mat    # 256 object 6d pose annotation, same format as YCB-Video dataset for easy usage</span><br><span class="line">    |   |   `-- rect</span><br><span class="line">    |   |   |   |-- 0000.npy to 0255.npy    # 256 2D planar grasp labels</span><br><span class="line">    |   |   |   </span><br><span class="line">    |   |   `-- camK.npy                    # camera intrinsic, shape: 3x3, [[f_x,0,c_x], [0,f_y,c_y], [0,0,1]]</span><br><span class="line">    |   |   `-- camera_poses.npy            # 256 camera poses with respect to the first frame, shape: 256x(4x4)</span><br><span class="line">    |   |   `-- cam0_wrt_table.npy          # first frame&#x27;s camera pose with respect to the table, shape: 4x4</span><br><span class="line">    |   |</span><br><span class="line">    |   `-- realsense</span><br><span class="line">    |       |-- same structure as kinect</span><br><span class="line">    |</span><br><span class="line">    |</span><br><span class="line">    `-- scene_0001</span><br><span class="line">    |</span><br><span class="line">    `-- ... ...</span><br><span class="line">    |</span><br><span class="line">    `-- scene_0189</span><br></pre></td></tr></table></figure>

<p>​    我们可以看到190个场景，每个都有对应的256张RGB, depth，mask，以及每个场景中10个物体的id、Pose、以及两个摄像机的外参矩阵。</p>
<p>​    而Grasp Label的格式可以通过<a target="_blank" rel="noopener" href="https://graspnetapi.readthedocs.io/en/latest/grasp_format.html">API官网</a>找到，注意到是88种物体，每一个都有一个Grasp Label。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>l = np.load(<span class="string">&#x27;000_labels.npz&#x27;</span>) <span class="comment"># GRASPNET_ROOT/grasp_label/000_labels.npz</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>l.files</span><br><span class="line">[<span class="string">&#x27;points&#x27;</span>, <span class="string">&#x27;offsets&#x27;</span>, <span class="string">&#x27;collision&#x27;</span>, <span class="string">&#x27;scores&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>l[<span class="string">&#x27;points&#x27;</span>].shape</span><br><span class="line">(<span class="number">3459</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>l[<span class="string">&#x27;offsets&#x27;</span>].shape</span><br><span class="line">(<span class="number">3459</span>, <span class="number">300</span>, <span class="number">12</span>, <span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>l[<span class="string">&#x27;collision&#x27;</span>].shape</span><br><span class="line">(<span class="number">3459</span>, <span class="number">300</span>, <span class="number">12</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>l[<span class="string">&#x27;collision&#x27;</span>].dtype</span><br><span class="line">dtype(<span class="string">&#x27;bool&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>l[<span class="string">&#x27;scores&#x27;</span>].shape</span><br><span class="line">(<span class="number">3459</span>, <span class="number">300</span>, <span class="number">12</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>l[<span class="string">&#x27;scores&#x27;</span>][<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">-<span class="number">1.0</span></span><br></pre></td></tr></table></figure>

<ul>
<li>[‘points’] 记录了模型坐标系下的Grasp Center Point。</li>
<li>[‘offsets’] 记录了对应Grasp的in-plane rotation，夹爪深度和夹爪宽度。</li>
<li>[‘collision’] 记录了对应Grasp是否和物体模型存在碰撞。</li>
<li>[‘scores’] 记录了达到稳定Grasp时最小的摩擦系数。 </li>
</ul>
<p>​    还有一个比较重要的数据就是每个场景的collision_label，在官网中也被成为<a target="_blank" rel="noopener" href="https://graspnetapi.readthedocs.io/en/latest/grasp_format.html#collision-masks-on-each-scene">Collision Masks on Each Scene</a>。具体地，因为我们每个场景中维护了物体的6D Pose，我们是知道每个Grasp的位置在哪里的，我们可以预先地把我们的夹爪模型放上去做碰撞检测。如果Gripper和场景中的Model有碰撞，对于的collision_label就设置为True，我们要在训练的时候把存在碰撞的Grasp Pose的score设置为0。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = np.load(<span class="string">&#x27;collision_labels.npz&#x27;</span>) <span class="comment"># GRASPNET_ROOT/collision_label/scene_0000/collision_labels.npz</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c.files</span><br><span class="line">[<span class="string">&#x27;arr_0&#x27;</span>, <span class="string">&#x27;arr_4&#x27;</span>, <span class="string">&#x27;arr_5&#x27;</span>, <span class="string">&#x27;arr_2&#x27;</span>, <span class="string">&#x27;arr_3&#x27;</span>, <span class="string">&#x27;arr_7&#x27;</span>, <span class="string">&#x27;arr_1&#x27;</span>, <span class="string">&#x27;arr_8&#x27;</span>, <span class="string">&#x27;arr_6&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c[<span class="string">&#x27;arr_0&#x27;</span>].shape</span><br><span class="line">(<span class="number">487</span>, <span class="number">300</span>, <span class="number">12</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c[<span class="string">&#x27;arr_0&#x27;</span>].dtype</span><br><span class="line">dtype(<span class="string">&#x27;bool&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c[<span class="string">&#x27;arr_0&#x27;</span>][<span class="number">10</span>][<span class="number">20</span>][<span class="number">3</span>]</span><br><span class="line">array([ <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>])</span><br></pre></td></tr></table></figure>

<h2 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot; GraspNet dataset processing.</span></span><br><span class="line"><span class="string">    Author: chenxi-wang</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.io <span class="keyword">as</span> scio</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch._six <span class="keyword">import</span> container_abcs</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line">BASE_DIR = os.path.dirname(os.path.abspath(__file__))</span><br><span class="line">ROOT_DIR = os.path.dirname(BASE_DIR)</span><br><span class="line">sys.path.append(os.path.join(ROOT_DIR, <span class="string">&#x27;utils&#x27;</span>))</span><br><span class="line"><span class="keyword">from</span> data_utils <span class="keyword">import</span> CameraInfo, transform_point_cloud, create_point_cloud_from_depth_image,\</span><br><span class="line">                            get_workspace_mask, remove_invisible_grasp_points</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GraspNetDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, root, valid_obj_idxs, grasp_labels, camera=<span class="string">&#x27;kinect&#x27;</span>, split=<span class="string">&#x27;train&#x27;</span>, num_points=<span class="number">20000</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 remove_outlier=<span class="literal">False</span>, remove_invisible=<span class="literal">True</span>, augment=<span class="literal">False</span>, load_label=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="keyword">assert</span>(num_points&lt;=<span class="number">50000</span>)</span><br><span class="line">		<span class="comment"># ignore some self.x = x</span></span><br><span class="line">        <span class="keyword">if</span> split == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">            self.sceneIds = <span class="built_in">list</span>( <span class="built_in">range</span>(<span class="number">100</span>) )</span><br><span class="line">        <span class="keyword">elif</span> split == <span class="string">&#x27;test&#x27;</span>:</span><br><span class="line">            self.sceneIds = <span class="built_in">list</span>( <span class="built_in">range</span>(<span class="number">100</span>,<span class="number">190</span>) )</span><br><span class="line">        <span class="keyword">elif</span> split == <span class="string">&#x27;test_seen&#x27;</span>:</span><br><span class="line">            self.sceneIds = <span class="built_in">list</span>( <span class="built_in">range</span>(<span class="number">100</span>,<span class="number">130</span>) )</span><br><span class="line">        <span class="keyword">elif</span> split == <span class="string">&#x27;test_similar&#x27;</span>:</span><br><span class="line">            self.sceneIds = <span class="built_in">list</span>( <span class="built_in">range</span>(<span class="number">130</span>,<span class="number">160</span>) )</span><br><span class="line">        <span class="keyword">elif</span> split == <span class="string">&#x27;test_novel&#x27;</span>:</span><br><span class="line">            self.sceneIds = <span class="built_in">list</span>( <span class="built_in">range</span>(<span class="number">160</span>,<span class="number">190</span>) )</span><br><span class="line">        self.sceneIds = [<span class="string">&#x27;scene_&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">str</span>(x).zfill(<span class="number">4</span>)) <span class="keyword">for</span> x <span class="keyword">in</span> self.sceneIds]</span><br><span class="line">        <span class="comment"># ignore some dir concat</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">scene_list</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.scenename</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.depthpath)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.load_label:</span><br><span class="line">            <span class="keyword">return</span> self.get_data_label(index)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.get_data(index)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_data</span>(<span class="params">self, index, return_raw_cloud=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="comment"># 仅读入depth, pointcloud, mask，不读入grasp_label，</span></span><br><span class="line">        color = np.array(Image.<span class="built_in">open</span>(self.colorpath[index]), dtype=np.float32) / <span class="number">255.0</span></span><br><span class="line">        depth = np.array(Image.<span class="built_in">open</span>(self.depthpath[index]))</span><br><span class="line">        seg = np.array(Image.<span class="built_in">open</span>(self.labelpath[index]))</span><br><span class="line">        meta = scio.loadmat(self.metapath[index])</span><br><span class="line">        scene = self.scenename[index]</span><br><span class="line">        intrinsic = meta[<span class="string">&#x27;intrinsic_matrix&#x27;</span>]</span><br><span class="line">        factor_depth = meta[<span class="string">&#x27;factor_depth&#x27;</span>]</span><br><span class="line">        camera = CameraInfo(<span class="number">1280.0</span>, <span class="number">720.0</span>, intrinsic[<span class="number">0</span>][<span class="number">0</span>], intrinsic[<span class="number">1</span>][<span class="number">1</span>], intrinsic[<span class="number">0</span>][<span class="number">2</span>], intrinsic[<span class="number">1</span>][<span class="number">2</span>], factor_depth)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 合成点云</span></span><br><span class="line">        cloud = create_point_cloud_from_depth_image(depth, camera, organized=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 得到两个mask</span></span><br><span class="line">        depth_mask = (depth &gt; <span class="number">0</span>)</span><br><span class="line">        seg_mask = (seg &gt; <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> self.remove_outlier:</span><br><span class="line">            camera_poses = np.load(os.path.join(self.root, <span class="string">&#x27;scenes&#x27;</span>, scene, self.camera, <span class="string">&#x27;camera_poses.npy&#x27;</span>))</span><br><span class="line">            align_mat = np.load(os.path.join(self.root, <span class="string">&#x27;scenes&#x27;</span>, scene, self.camera, <span class="string">&#x27;cam0_wrt_table.npy&#x27;</span>))</span><br><span class="line">            trans = np.dot(align_mat, camera_poses[self.frameid[index]])</span><br><span class="line">            workspace_mask = get_workspace_mask(cloud, seg, trans=trans, organized=<span class="literal">True</span>, outlier=<span class="number">0.02</span>)</span><br><span class="line">            mask = (depth_mask &amp; workspace_mask)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mask = depth_mask</span><br><span class="line">        cloud_masked = cloud[mask]</span><br><span class="line">        color_masked = color[mask]</span><br><span class="line">        seg_masked = seg[mask]</span><br><span class="line">        <span class="keyword">if</span> return_raw_cloud:</span><br><span class="line">            <span class="keyword">return</span> cloud_masked, color_masked</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果点太多，那么就采样固定的点输出</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(cloud_masked) &gt;= self.num_points:</span><br><span class="line">            idxs = np.random.choice(<span class="built_in">len</span>(cloud_masked), self.num_points, replace=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            idxs1 = np.arange(<span class="built_in">len</span>(cloud_masked))</span><br><span class="line">            idxs2 = np.random.choice(<span class="built_in">len</span>(cloud_masked), self.num_points-<span class="built_in">len</span>(cloud_masked), replace=<span class="literal">True</span>)</span><br><span class="line">            idxs = np.concatenate([idxs1, idxs2], axis=<span class="number">0</span>)</span><br><span class="line">        cloud_sampled = cloud_masked[idxs]</span><br><span class="line">        color_sampled = color_masked[idxs]</span><br><span class="line">        </span><br><span class="line">        ret_dict = &#123;&#125;</span><br><span class="line">        ret_dict[<span class="string">&#x27;point_clouds&#x27;</span>] = cloud_sampled.astype(np.float32)</span><br><span class="line">        ret_dict[<span class="string">&#x27;cloud_colors&#x27;</span>] = color_sampled.astype(np.float32)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ret_dict</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_data_label</span>(<span class="params">self, index</span>):</span></span><br><span class="line">		<span class="comment"># 省略了get_data的所有逻辑，基于get_data的函数之后，这个函数做了如下的处理</span></span><br><span class="line">        cloud_sampled = cloud_masked[idxs]</span><br><span class="line">        color_sampled = color_masked[idxs]</span><br><span class="line">        seg_sampled = seg_masked[idxs]</span><br><span class="line">        objectness_label = seg_sampled.copy()</span><br><span class="line">        objectness_label[objectness_label&gt;<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        object_poses_list = []</span><br><span class="line">        grasp_points_list = []</span><br><span class="line">        grasp_offsets_list = []</span><br><span class="line">        grasp_scores_list = []</span><br><span class="line">        grasp_tolerance_list = []</span><br><span class="line">        <span class="keyword">for</span> i, obj_idx <span class="keyword">in</span> <span class="built_in">enumerate</span>(obj_idxs):	<span class="comment"># 枚举场景中的10类物体</span></span><br><span class="line">            <span class="keyword">if</span> obj_idx <span class="keyword">not</span> <span class="keyword">in</span> self.valid_obj_idxs:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> (seg_sampled == obj_idx).<span class="built_in">sum</span>() &lt; <span class="number">50</span>:	<span class="comment"># 拍到的对应物体点云数量太少</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            object_poses_list.append(poses[:, :, i])	<span class="comment"># 加入到合法的object_poses_list中</span></span><br><span class="line">            points, offsets, scores, tolerance = self.grasp_labels[obj_idx]	<span class="comment"># 得到物体坐标系下创建的Grasp Pose</span></span><br><span class="line">            collision = self.collision_labels[scene][i] <span class="comment"># 得到场景中的点的collision mask(Np, V, A, D)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># remove invisible grasp points</span></span><br><span class="line">            <span class="keyword">if</span> self.remove_invisible:</span><br><span class="line">                visible_mask = remove_invisible_grasp_points(cloud_sampled[seg_sampled==obj_idx], points, poses[:,:,i], th=<span class="number">0.01</span>)</span><br><span class="line">                points = points[visible_mask]</span><br><span class="line">                offsets = offsets[visible_mask]</span><br><span class="line">                scores = scores[visible_mask]</span><br><span class="line">                tolerance = tolerance[visible_mask]</span><br><span class="line">                collision = collision[visible_mask]</span><br><span class="line"></span><br><span class="line">            idxs = np.random.choice(<span class="built_in">len</span>(points), <span class="built_in">min</span>(<span class="built_in">max</span>(<span class="built_in">int</span>(<span class="built_in">len</span>(points)/<span class="number">4</span>),<span class="number">300</span>),<span class="built_in">len</span>(points)), replace=<span class="literal">False</span>)</span><br><span class="line">            grasp_points_list.append(points[idxs])</span><br><span class="line">            grasp_offsets_list.append(offsets[idxs])</span><br><span class="line">            collision = collision[idxs].copy()</span><br><span class="line">            scores = scores[idxs].copy()</span><br><span class="line">            tolerance = tolerance[idxs].copy()</span><br><span class="line">            scores[collision] = <span class="number">0</span></span><br><span class="line">            tolerance[collision] = <span class="number">0</span>	<span class="comment"># 场景中存在collision的情况，我们设置Grasp分数为0</span></span><br><span class="line">            grasp_scores_list.append(scores)</span><br><span class="line">            grasp_tolerance_list.append(tolerance)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 如果设置了这个，每次__getitem__的时候得到的数据也会有不一样，会有轻微的扰动，但是这样就不需要在创建Label的时候就加随机，降低的数据集的大小和处理的复杂度</span></span><br><span class="line">        <span class="keyword">if</span> self.augment:</span><br><span class="line">            cloud_sampled, object_poses_list = self.augment_data(cloud_sampled, object_poses_list)</span><br><span class="line">        </span><br><span class="line">        ret_dict = &#123;&#125;</span><br><span class="line">        ret_dict[<span class="string">&#x27;point_clouds&#x27;</span>] = cloud_sampled.astype(np.float32)</span><br><span class="line">        ret_dict[<span class="string">&#x27;cloud_colors&#x27;</span>] = color_sampled.astype(np.float32)</span><br><span class="line">        ret_dict[<span class="string">&#x27;objectness_label&#x27;</span>] = objectness_label.astype(np.int64)</span><br><span class="line">        ret_dict[<span class="string">&#x27;object_poses_list&#x27;</span>] = object_poses_list</span><br><span class="line">        ret_dict[<span class="string">&#x27;grasp_points_list&#x27;</span>] = grasp_points_list</span><br><span class="line">        ret_dict[<span class="string">&#x27;grasp_offsets_list&#x27;</span>] = grasp_offsets_list</span><br><span class="line">        ret_dict[<span class="string">&#x27;grasp_labels_list&#x27;</span>] = grasp_scores_list</span><br><span class="line">        ret_dict[<span class="string">&#x27;grasp_tolerance_list&#x27;</span>] = grasp_tolerance_list</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ret_dict</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">augment_data</span>(<span class="params">self, point_clouds, object_poses_list</span>):</span></span><br><span class="line">   <span class="comment"># Flipping along the YZ plane</span></span><br><span class="line">    <span class="keyword">if</span> np.random.random() &gt; <span class="number">0.5</span>:</span><br><span class="line">        flip_mat = np.array([[-<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                            [ <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                            [ <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line">        point_clouds = transform_point_cloud(point_clouds, flip_mat, <span class="string">&#x27;3x3&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(object_poses_list)):</span><br><span class="line">            object_poses_list[i] = np.dot(flip_mat, object_poses_list[i]).astype(np.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 沿着z轴旋转</span></span><br><span class="line">    rot_angle = (np.random.random()*np.pi/<span class="number">3</span>) - np.pi/<span class="number">6</span> <span class="comment"># -30 ~ +30 degree</span></span><br><span class="line">    c, s = np.cos(rot_angle), np.sin(rot_angle)</span><br><span class="line">    rot_mat = np.array([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                        [<span class="number">0</span>, c,-s],</span><br><span class="line">                        [<span class="number">0</span>, s, c]])</span><br><span class="line">    point_clouds = transform_point_cloud(point_clouds, rot_mat, <span class="string">&#x27;3x3&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(object_poses_list)):</span><br><span class="line">        object_poses_list[i] = np.dot(rot_mat, object_poses_list[i]).astype(np.float32)</span><br><span class="line">    <span class="keyword">return</span> point_clouds, object_poses_list</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span>(<span class="params">batch</span>):</span></span><br><span class="line">    <span class="comment"># 在train.py创建DataLoader的时候会使用到这个函数。</span></span><br><span class="line">    <span class="comment"># collate_fn：如何取样本的，我们可以定义自己的函数来准确地实现想要的功能</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(batch[<span class="number">0</span>]).__module__ == <span class="string">&#x27;numpy&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.stack([torch.from_numpy(b) <span class="keyword">for</span> b <span class="keyword">in</span> batch], <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(batch[<span class="number">0</span>], container_abcs.Mapping):</span><br><span class="line">        <span class="keyword">return</span> &#123;key:collate_fn([d[key] <span class="keyword">for</span> d <span class="keyword">in</span> batch]) <span class="keyword">for</span> key <span class="keyword">in</span> batch[<span class="number">0</span>]&#125;</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(batch[<span class="number">0</span>], container_abcs.<span class="type">Sequence</span>):</span><br><span class="line">        <span class="keyword">return</span> [[torch.from_numpy(sample) <span class="keyword">for</span> sample <span class="keyword">in</span> b] <span class="keyword">for</span> b <span class="keyword">in</span> batch]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">raise</span> TypeError(<span class="string">&quot;batch must contain tensors, dicts or lists; found &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">type</span>(batch[<span class="number">0</span>])))</span><br></pre></td></tr></table></figure>



<h2 id="附言"><a href="#附言" class="headerlink" title="附言"></a>附言</h2><p>​    和浩树学长聊了一下，他很明确地指出这其实是一个多分类任务。并且因为论文中提到了使用离散的Bin来代替回归出一个向量的idea，我就继续请教了一下这样做的原因。因为这一点其实在VoteNet中尝试预测物体的6D pose的时候，也是有体现的。他认为回归在方法论上就不太对，因为正常的情况下Valid Approach Vector / Operations 应当是一个分布，而不是回归出来的一个值。多分类对于每个离散的bin预测出一个score，在我的理解下，就是用细粒度的均匀分布去拟合这个我们所希望预测的分布，这样训练效果就会好。听罢，感叹自己科研的路上还有很多路要走…因为各种各样的论文只会告诉你它的优点是什么，真正的硬伤都不会写在论文里。而要把握领域前沿动向，并且做出正确的选择，那就必须要在这种抽象层面上建构出自己稳定的理论体系， 但是我觉得我自己还差了很多这一块的知识积累。真是道阻且长啊。好在，我在努力开发出很多未来可以使用的方法论、视野、论文积累、代码积累等，希望可以在这条路上继续努力！</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>(CVPR2020)GraspNet-1Billion</p><p><a href="https://kami-code.com/2022/01/18/GraspNet-1Billion/">https://kami-code.com/2022/01/18/GraspNet-1Billion/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Kami-code</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2022-01-18</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2022-02-04</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/GraspNet/">GraspNet</a><a class="link-muted mr-2" rel="tag" href="/tags/CVPR/">CVPR</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2022/01/26/GIGA/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">(RSS2021)GIGA的仿真器部分</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2022/01/17/6D-GraspNet/"><span class="level-item">(ICCV2019)6D-GraspNet</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="SOHUCS" sid="2022/01/18/GraspNet-1Billion/"></div><script charset="utf-8" src="https://changyan.sohu.com/upload/changyan.js"></script><script>window.changyan.api.config({appid: 'cyvI88c19',conf: 'prod_634561f1ec380218934dcf2c12b8b70b'});</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.jpg" alt="Chen Bao"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Chen Bao</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shanghai, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">24</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">0</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">34</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Kami-code" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Kami-code"><i class="fab fa-github"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><!--!--><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-01-29T03:41:38.000Z">2022-01-29</time></p><p class="title"><a href="/2022/01/29/unigrasp/">unigrasp</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-01-26T10:59:39.000Z">2022-01-26</time></p><p class="title"><a href="/2022/01/26/GIGA/">(RSS2021)GIGA的仿真器部分</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-01-18T06:33:35.000Z">2022-01-18</time></p><p class="title"><a href="/2022/01/18/GraspNet-1Billion/">(CVPR2020)GraspNet-1Billion</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-01-17T07:07:22.000Z">2022-01-17</time></p><p class="title"><a href="/2022/01/17/6D-GraspNet/">(ICCV2019)6D-GraspNet</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-01-17T06:39:59.000Z">2022-01-17</time></p><p class="title"><a href="/2022/01/17/machine-learning-pre/">VoteNet_pre</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2022/01/"><span class="level-start"><span class="level-item">January 2022</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/12/"><span class="level-start"><span class="level-item">December 2021</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/11/"><span class="level-start"><span class="level-item">November 2021</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/10/"><span class="level-start"><span class="level-item">October 2021</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/6D-GraspNet/"><span class="tag">6D-GraspNet</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CHOMP/"><span class="tag">CHOMP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CVPR/"><span class="tag">CVPR</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Camera/"><span class="tag">Camera</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Docker/"><span class="tag">Docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GraspNet/"><span class="tag">GraspNet</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ICCV/"><span class="tag">ICCV</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ICRA/"><span class="tag">ICRA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MongoDB/"><span class="tag">MongoDB</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MySQL/"><span class="tag">MySQL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Neo4j/"><span class="tag">Neo4j</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OpenCV/"><span class="tag">OpenCV</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OpenGL/"><span class="tag">OpenGL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PointNet/"><span class="tag">PointNet</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PointNet/"><span class="tag">PointNet++</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Pybind11/"><span class="tag">Pybind11</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Pybullet/"><span class="tag">Pybullet</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ROS/"><span class="tag">ROS</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RSS/"><span class="tag">RSS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SE3353/"><span class="tag">SE3353</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ScrewNet/"><span class="tag">ScrewNet</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Search/"><span class="tag">Search</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VoteNet/"><span class="tag">VoteNet</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/WebService/"><span class="tag">WebService</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/behavior-tree/"><span class="tag">behavior tree</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/compliers/"><span class="tag">compliers</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/deeplearning/"><span class="tag">deeplearning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ikfast/"><span class="tag">ikfast</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/lab/"><span class="tag">lab</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/mmdection/"><span class="tag">mmdection</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/moveit/"><span class="tag">moveit</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/planning/"><span class="tag">planning</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pybullet/"><span class="tag">pybullet</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/segmentation/"><span class="tag">segmentation</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Ryan &#039;s website" height="28"></a><span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span><span class="post-meta-divider">|</span><span id="busuanzi_container_site_uv" style="display:none">本站访客数<span id="busuanzi_value_site_uv"></span>人</span><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><p class="is-size-7"><span>&copy; 2022 Kami-code</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>