<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>(ICCV2019)6D-GraspNet - Ryan &#039;s website</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Ryan&#039;s blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Ryan&#039;s blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="6D GraspNet是英伟达2019年提出的一篇抓取的论文。"><meta property="og:type" content="blog"><meta property="og:title" content="(ICCV2019)6D-GraspNet"><meta property="og:url" content="https://kami-code.com/2022/01/17/6D-GraspNet/"><meta property="og:site_name" content="Ryan &#039;s website"><meta property="og:description" content="6D GraspNet是英伟达2019年提出的一篇抓取的论文。"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://kami-code.com/2022/01/17/6D-GraspNet/image-20220117123346416.png"><meta property="og:image" content="https://kami-code.com/2022/01/17/6D-GraspNet/image-20220117153929206.png"><meta property="og:image" content="https://kami-code.com/2022/01/17/6D-GraspNet/image-20220117150828835.png"><meta property="og:image" content="https://kami-code.com/2022/01/17/6D-GraspNet/03808585289223ea205c3361de5de26d9104.gif"><meta property="article:published_time" content="2022-01-17T07:07:22.000Z"><meta property="article:modified_time" content="2022-02-19T04:57:55.345Z"><meta property="article:author" content="Kami-code"><meta property="article:tag" content="GraspNet"><meta property="article:tag" content="ICCV"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/2022/01/17/6D-GraspNet/image-20220117123346416.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://kami-code.com/2022/01/17/6D-GraspNet/"},"headline":"(ICCV2019)6D-GraspNet","image":["https://kami-code.com/2022/01/17/6D-GraspNet/image-20220117123346416.png","https://kami-code.com/2022/01/17/6D-GraspNet/image-20220117153929206.png","https://kami-code.com/2022/01/17/6D-GraspNet/image-20220117150828835.png","https://kami-code.com/2022/01/17/6D-GraspNet/03808585289223ea205c3361de5de26d9104.gif"],"datePublished":"2022-01-17T07:07:22.000Z","dateModified":"2022-02-19T04:57:55.345Z","author":{"@type":"Person","name":"Kami-code"},"publisher":{"@type":"Organization","name":"Ryan 's website","logo":{"@type":"ImageObject","url":"https://kami-code.com/img/logo.svg"}},"description":"6D GraspNet是英伟达2019年提出的一篇抓取的论文。"}</script><link rel="canonical" href="https://kami-code.com/2022/01/17/6D-GraspNet/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Ryan &#039;s website" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-01-17T07:07:22.000Z" title="2022/1/17 下午3:07:22">2022-01-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2022-02-19T04:57:55.345Z" title="2022/2/19 下午12:57:55">2022-02-19</time></span><span class="level-item">30 minutes read (About 4467 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">(ICCV2019)6D-GraspNet</h1><div class="content"><p>6D GraspNet是英伟达2019年提出的一篇抓取的论文。</p>
<span id="more"></span>

<p><img src="/2022/01/17/6D-GraspNet/image-20220117123346416.png"></p>
<p>​    总体结构如上图所示。</p>
<h3 id="Grasp-Sampler"><a href="#Grasp-Sampler" class="headerlink" title="Grasp Sampler"></a>Grasp Sampler</h3><p>​        传入一个部分观测的物体点云$X$，我们需要能够生成出对应的抓取proposal $G^*$。这是一个生成式模型的场景，也就是我们估计出后验分布$P(G^*|X)$，这样我们才能够根据任意输入$X$来得到$G^*$。论文使用的VAE来对底层隐变量进行建模。此处提供一个复习VAE的<a target="_blank" rel="noopener" href="https://blog.csdn.net/a312863063/article/details/87953517">博客</a>。</p>
<img src="/2022/01/17/6D-GraspNet/image-20220117153929206.png" style="zoom:67%;">

<p>​    在VAE中，因为涉及到随机变量的采样，如果按照Original Form采样会导致梯度不能回传。所以，<a target="_blank" rel="noopener" href="https://stats.stackexchange.com/questions/199605/how-does-the-reparameterization-trick-for-vaes-work-and-why-is-it-important">重参数化步骤</a>会使得$z = \mu + \sigma\odot\varepsilon $，使得<a target="_blank" rel="noopener" href="https://stats.stackexchange.com/questions/420974/backpropagation-on-variational-autoencoders">梯度可以回传</a>，而随机节点$\varepsilon \sim N(0, 1)$不需要更新。</p>
<img src="/2022/01/17/6D-GraspNet/image-20220117150828835.png" style="zoom:67%;">

<p>​    我们使用VAE来最大化$P(G|X)$的。传入一个点云$X$和隐变量$z$，Decoder部分就是一个确定性的函数来预测出一个grasp。中间的隐变量空间为$P(z)=N(0, I)$。所以，传入一个部分观测点云X以后，我们可以在这个高斯分布上多次采样来得到不同的$z$，那么我们的问题就转化成了最大化<br>$$<br>P(G|X)=\int P(G|X,z;\Theta)P(z)dz<br>$$<br>​    直接积分是不可积的，所以我们需要通过encoder$Q(z|X,g)$把正样本X和g映射到隐变量空间的一个子空间中。Encoder和Decoder都是基于PointNet++的，Encoder使用的方法就是把GT抓取$g$接在点云$X$后面，而Decoder就是把采样出的隐变量$z$接在点云$X$后面。</p>
<h3 id="Grasp-Pose-Evaluation"><a href="#Grasp-Pose-Evaluation" class="headerlink" title="Grasp Pose Evaluation"></a>Grasp Pose Evaluation</h3><p>​    因为Decoder预测出来的抓取肯定有些能成功而有些会失败。我们需要对每个预测出来的$&lt;X,\hat{g}&gt;$预测一个成功率$P(S|X,g)$，其实就是一个二分类问题。但是比起直接把16维的6D grasp接在点云后面，本文使用了能够更好利用好点云的特点的encoding方法，也就是把夹爪的点云$X_g$接在原先待抓取物体点云$X$之后，并且再使用一个指示向量来标志哪些是原物体的点云。</p>
<p>​    然后这个二分类网络的输入就是原点云和合成的夹爪点云，输出的就是抓取的成功率，使用交叉熵损失来优化此网络。在标签中1代表成功，0代表失败。</p>
<p>​    在数据增强部分，我们对于正样本$g\in G^*$做随机扰动，使得夹爪和物体点云有碰撞或者远离物体点云，得到增强的负样本$G^-$。</p>
<h3 id="Iterative-Grasp-Pose-Refinement"><a href="#Iterative-Grasp-Pose-Refinement" class="headerlink" title="Iterative Grasp Pose Refinement"></a>Iterative Grasp Pose Refinement</h3><p>​    现在我们的网络已经可以根据点云预测出一个抓点集合了，那么我们有没有办法去进一步让这些抓取更好呢？我们希望得到一个优化位移$\Delta g$，使得$P(s=1|g+\Delta g) &gt; P(s=1|g)$。这样的话我们就可以通过求成功率S对于抓取$g$的偏导数来做梯度上升来优化S，即$\frac{\partial S}{\partial g}$。为了保证刚体变换的约束，我们让夹爪点云$X_g$通过平移向量和欧拉角$R_g=(\alpha_g,\beta_g,\gamma_g)$，根据链式法则，我们有<br>$$<br>\Delta g=\frac{\partial S}{\partial g}=\eta\times\frac{\partial S}{\partial T(g;p)}\times\frac{\partial T(g;p)}{\partial g}<br>$$<br>​    这样我们就可以更新了。这一步并不需要什么新的网络，只需要Grasp Evaluator提供梯度就一切好办了。</p>
<h3 id="创建训练集"><a href="#创建训练集" class="headerlink" title="创建训练集"></a>创建训练集</h3><p>​    在仿真环境中，我们随机在物体的mesh上采样点，并且把grasp的z轴对齐到点的法向量上。夹爪和物体表面的距离是从[0, gripper_length]上随机采样的，而z轴上的旋转角度也是随机采样得到的。我们对于那些closing volume和物体重叠的grasp来做simulation，抓上来以后会做一个抖动动作，如果物体在抖动以后依旧被抓着，那么我们就认为是一个正样本。</p>
<img src="/2022/01/17/6D-GraspNet/03808585289223ea205c3361de5de26d9104.gif" style="zoom:100%;">

<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>​    From实验室学长：最主要的问题是这套proposing或者sampling后面接evaluation网络的两步法，方法论上落后。两步法慢，而且没法生成dense的grasp pose，一步能搞定的时候为什么要拆成两步呢？</p>
<h3 id="普通VAE的代码"><a href="#普通VAE的代码" class="headerlink" title="普通VAE的代码"></a>普通VAE的代码</h3><p>​    为了仔细理解本论文的代码，我们先从<a target="_blank" rel="noopener" href="https://github.com/AntixK/PyTorch-VAE/blob/master/models/vanilla_vae.py">普通的VAE代码</a>开始阅读。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseVAE</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span></span><br><span class="line">        <span class="built_in">super</span>(BaseVAE, self).__init__()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">self, <span class="built_in">input</span>: Tensor</span>) -&gt; <span class="type">List</span>[Tensor]:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, <span class="built_in">input</span>: Tensor</span>) -&gt; <span class="type">Any</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self, batch_size:<span class="built_in">int</span>, current_device: <span class="built_in">int</span>, **kwargs</span>) -&gt; Tensor:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate</span>(<span class="params">self, x: Tensor, **kwargs</span>) -&gt; Tensor:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, *inputs: Tensor</span>) -&gt; Tensor:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss_function</span>(<span class="params">self, *inputs: <span class="type">Any</span>, **kwargs</span>) -&gt; Tensor:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<p>​    相对于普通的模块，我们需要额外定义encode, decode和sample函数。</p>
<p>​    在init中，无非是对称地构造encoder和decoder。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">modules = []</span><br><span class="line">hidden_dims = [<span class="number">32</span>, <span class="number">64</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">512</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build Encoder</span></span><br><span class="line"><span class="keyword">for</span> h_dim <span class="keyword">in</span> hidden_dims:</span><br><span class="line">    modules.append(</span><br><span class="line">        nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, out_channels=h_dim,</span><br><span class="line">                      kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(h_dim),</span><br><span class="line">            nn.LeakyReLU())</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">in_channels = h_dim</span><br><span class="line">self.encoder = nn.Sequential(*modules)</span><br><span class="line">self.fc_mu = nn.Linear(hidden_dims[-<span class="number">1</span>]*<span class="number">4</span>, latent_dim)</span><br><span class="line">self.fc_var = nn.Linear(hidden_dims[-<span class="number">1</span>]*<span class="number">4</span>, latent_dim)</span><br><span class="line"><span class="comment"># Build Decoder</span></span><br><span class="line">modules = []</span><br><span class="line">self.decoder_input = nn.Linear(latent_dim, hidden_dims[-<span class="number">1</span>] * <span class="number">4</span>)</span><br><span class="line">hidden_dims.reverse()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(hidden_dims) - <span class="number">1</span>):</span><br><span class="line">    modules.append(</span><br><span class="line">        nn.Sequential(</span><br><span class="line">            nn.ConvTranspose2d(hidden_dims[i],</span><br><span class="line">                               hidden_dims[i + <span class="number">1</span>],</span><br><span class="line">                               kernel_size=<span class="number">3</span>,</span><br><span class="line">                               stride=<span class="number">2</span>,</span><br><span class="line">                               padding=<span class="number">1</span>,</span><br><span class="line">                               output_padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(hidden_dims[i + <span class="number">1</span>]),</span><br><span class="line">            nn.LeakyReLU())</span><br><span class="line">    )</span><br><span class="line">self.decoder = nn.Sequential(*modules)</span><br><span class="line">self.final_layer = nn.Sequential(</span><br><span class="line">    nn.ConvTranspose2d(hidden_dims[-<span class="number">1</span>],</span><br><span class="line">                       hidden_dims[-<span class="number">1</span>],</span><br><span class="line">                       kernel_size=<span class="number">3</span>,</span><br><span class="line">                       stride=<span class="number">2</span>,</span><br><span class="line">                       padding=<span class="number">1</span>,</span><br><span class="line">                       output_padding=<span class="number">1</span>),</span><br><span class="line">    nn.BatchNorm2d(hidden_dims[-<span class="number">1</span>]),</span><br><span class="line">    nn.LeakyReLU(),</span><br><span class="line">    nn.Conv2d(hidden_dims[-<span class="number">1</span>], out_channels=<span class="number">3</span>,</span><br><span class="line">              kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.Tanh())</span><br></pre></td></tr></table></figure>

<p>​    Encoder和Decoder的代码如下，因为<a target="_blank" rel="noopener" href="https://github.com/AntixK/PyTorch-VAE/issues/29">本仓库是按照 64 * 64 * 3的图像</a>来计算的，所以在应用不同分辨率的时候，需要计算参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">self, <span class="built_in">input</span>: Tensor</span>) -&gt; <span class="type">List</span>[Tensor]:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Encodes the input by passing through the encoder network</span></span><br><span class="line"><span class="string">        and returns the latent codes.</span></span><br><span class="line"><span class="string">        :param input: (Tensor) Input tensor to encoder [B x C x H x W]</span></span><br><span class="line"><span class="string">        :return: (Tensor) List of latent codes</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">    result = self.encoder(<span class="built_in">input</span>)</span><br><span class="line">    <span class="comment"># B * C * 64 * 64</span></span><br><span class="line">    <span class="comment"># B * 32 * 32 * 32</span></span><br><span class="line">    <span class="comment"># B * 64 * 16 * 16</span></span><br><span class="line">    <span class="comment"># B * 128 * 8 * 8</span></span><br><span class="line">    <span class="comment"># B * 256 * 4 * 4</span></span><br><span class="line">    <span class="comment"># B * 512 * 2 * 2</span></span><br><span class="line">    result = torch.flatten(result, start_dim=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># B * 2048</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Split the result into mu and var components</span></span><br><span class="line">    <span class="comment"># of the latent Gaussian distribution</span></span><br><span class="line">    mu = self.fc_mu(result)</span><br><span class="line">    <span class="comment"># B * latent_dim</span></span><br><span class="line">    log_var = self.fc_var(result)</span><br><span class="line">	<span class="comment"># B * latent_dim</span></span><br><span class="line">    <span class="keyword">return</span> [mu, log_var]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, z: Tensor</span>) -&gt; Tensor:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Maps the given latent codes</span></span><br><span class="line"><span class="string">        onto the image space.</span></span><br><span class="line"><span class="string">        :param z: (Tensor) [B x latent_dim]</span></span><br><span class="line"><span class="string">        :return: (Tensor) [B x C x H x W]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">    result = self.decoder_input(z)</span><br><span class="line">    <span class="comment"># B * latent_dim =&gt; B * 2048</span></span><br><span class="line">    result = result.view(-<span class="number">1</span>, <span class="number">512</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="comment"># B * 512 * 2 * 2</span></span><br><span class="line">    result = self.decoder(result)</span><br><span class="line">    <span class="comment"># B * 256 * 4 * 4</span></span><br><span class="line">    <span class="comment"># B * 128 * 8 * 8</span></span><br><span class="line">    <span class="comment"># B * 64 * 16 * 16</span></span><br><span class="line">    <span class="comment"># B * 32 * 32 * 32</span></span><br><span class="line">    result = self.final_layer(result)</span><br><span class="line">    <span class="comment"># B * 32 * 64 * 64</span></span><br><span class="line">    <span class="comment"># B * 3 * 64 * 64</span></span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>

<p>​        下面我们可以看到重参数化其实就是做了一个$z = \mu + \sigma\odot\varepsilon $，前向传播的时候，我们就需要采样一个z出来继续做decode操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reparameterize</span>(<span class="params">self, mu: Tensor, logvar: Tensor</span>) -&gt; Tensor:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Reparameterization trick to sample from N(mu, var) from</span></span><br><span class="line"><span class="string">        N(0,1).</span></span><br><span class="line"><span class="string">        :param mu: (Tensor) Mean of the latent Gaussian [B x D]</span></span><br><span class="line"><span class="string">        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]</span></span><br><span class="line"><span class="string">        :return: (Tensor) [B x D]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">    std = torch.exp(<span class="number">0.5</span> * logvar)</span><br><span class="line">    eps = torch.randn_like(std)</span><br><span class="line">    <span class="keyword">return</span> eps * std + mu</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>: Tensor, **kwargs</span>) -&gt; <span class="type">List</span>[Tensor]:</span></span><br><span class="line">    mu, log_var = self.encode(<span class="built_in">input</span>)</span><br><span class="line">    z = self.reparameterize(mu, log_var)</span><br><span class="line">    <span class="keyword">return</span>  [self.decode(z), <span class="built_in">input</span>, mu, log_var]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_function</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                  *args,</span></span></span><br><span class="line"><span class="params"><span class="function">                  **kwargs</span>) -&gt; <span class="built_in">dict</span>:</span></span><br><span class="line">    recons = args[<span class="number">0</span>]</span><br><span class="line">    <span class="built_in">input</span> = args[<span class="number">1</span>]</span><br><span class="line">    mu = args[<span class="number">2</span>]</span><br><span class="line">    log_var = args[<span class="number">3</span>]</span><br><span class="line">	<span class="comment">#重建误差</span></span><br><span class="line">    recons_loss =F.mse_loss(recons, <span class="built_in">input</span>)</span><br><span class="line">	<span class="comment">#计算KL散度</span></span><br><span class="line">    kld_weight = kwargs[<span class="string">&#x27;M_N&#x27;</span>] <span class="comment"># Account for the minibatch samples from the dataset</span></span><br><span class="line">    kld_loss = torch.mean(-<span class="number">0.5</span> * torch.<span class="built_in">sum</span>(<span class="number">1</span> + log_var - mu ** <span class="number">2</span> - log_var.exp(), dim = <span class="number">1</span>), dim = <span class="number">0</span>)</span><br><span class="line">    loss = recons_loss + kld_weight * kld_loss</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&#x27;loss&#x27;</span>: loss, <span class="string">&#x27;Reconstruction_Loss&#x27;</span>:recons_loss.detach(), <span class="string">&#x27;KLD&#x27;</span>:-kld_loss.detach()&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">           num_samples:<span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">           current_device: <span class="built_in">int</span>, **kwargs</span>) -&gt; Tensor:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Samples from the latent space and return the corresponding</span></span><br><span class="line"><span class="string">        image space map.</span></span><br><span class="line"><span class="string">        :param num_samples: (Int) Number of samples</span></span><br><span class="line"><span class="string">        :param current_device: (Int) Device to run the model</span></span><br><span class="line"><span class="string">        :return: (Tensor)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">    z = torch.randn(num_samples,</span><br><span class="line">                    self.latent_dim)</span><br><span class="line"></span><br><span class="line">    z = z.to(current_device)</span><br><span class="line"></span><br><span class="line">    samples = self.decode(z)</span><br><span class="line">    <span class="keyword">return</span> samples</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate</span>(<span class="params">self, x: Tensor, **kwargs</span>) -&gt; Tensor:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Given an input image x, returns the reconstructed image</span></span><br><span class="line"><span class="string">        :param x: (Tensor) [B x C x H x W]</span></span><br><span class="line"><span class="string">        :return: (Tensor) [B x C x H x W]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.forward(x)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/yutingmoran/p/8631186.html">KL散度公式</a>如下：$ KL(N(\mu, \sigma), N(0, 1)) = -\log \sigma + \frac{\sigma^2 + \mu^2}{2} - \frac{1}{2}$</p>
<h2 id="6D-GraspNet代码"><a href="#6D-GraspNet代码" class="headerlink" title="6D-GraspNet代码"></a>6D-GraspNet代码</h2><h3 id="Grasp-Sampler-1"><a href="#Grasp-Sampler-1" class="headerlink" title="Grasp Sampler"></a>Grasp Sampler</h3><p>在代码中GraspSamplerVAE和GraspSamplerGAN都继承于GraspSampler。GraspSampler提供了共用的decoder函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GraspSampler</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, latent_size, device</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(GraspSampler, self).__init__()</span><br><span class="line">        self.latent_size = latent_size</span><br><span class="line">        self.device = device</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_decoder</span>(<span class="params">self, model_scale, pointnet_radius, pointnet_nclusters,</span></span></span><br><span class="line"><span class="params"><span class="function">                       num_input_features</span>):</span></span><br><span class="line">        <span class="comment"># The number of input features for the decoder is 3+latent space where 3</span></span><br><span class="line">        <span class="comment"># represents the x, y, z position of the point-cloud</span></span><br><span class="line"></span><br><span class="line">        self.decoder = base_network(pointnet_radius, pointnet_nclusters,</span><br><span class="line">                                    model_scale, num_input_features)</span><br><span class="line">        self.q = nn.Linear(model_scale * <span class="number">1024</span>, <span class="number">4</span>)</span><br><span class="line">        self.t = nn.Linear(model_scale * <span class="number">1024</span>, <span class="number">3</span>)</span><br><span class="line">        self.confidence = nn.Linear(model_scale * <span class="number">1024</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, xyz, z</span>):</span></span><br><span class="line">        <span class="comment"># 我们输入一个点云和一个采样得到的隐变量z，我们需要预测出抓取的pos(t(x))和orn(q(x))以及这个抓取的置信度。</span></span><br><span class="line">        <span class="comment"># 把隐变量接在点云后面作为特征</span></span><br><span class="line">        xyz_features = self.concatenate_z_with_pc(xyz,</span><br><span class="line">                                                  z).transpose(-<span class="number">1</span>,</span><br><span class="line">                                                               <span class="number">1</span>).contiguous()</span><br><span class="line">        <span class="keyword">for</span> module <span class="keyword">in</span> self.decoder[<span class="number">0</span>]:</span><br><span class="line">            xyz, xyz_features = module(xyz, xyz_features)</span><br><span class="line">        <span class="comment"># 过一轮decoder，也就是先从PointNet++中提取出B * 1024(scale)的特征向量</span></span><br><span class="line">        x = self.decoder[<span class="number">1</span>](xyz_features.squeeze(-<span class="number">1</span>))</span><br><span class="line">        predicted_qt = torch.cat(</span><br><span class="line">            (F.normalize(self.q(x), p=<span class="number">2</span>, dim=-<span class="number">1</span>), self.t(x)), -<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> predicted_qt, torch.sigmoid(self.confidence(x)).squeeze()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">concatenate_z_with_pc</span>(<span class="params">self, pc, z</span>):</span></span><br><span class="line">        z.unsqueeze_(<span class="number">1</span>)</span><br><span class="line">        z = z.expand(-<span class="number">1</span>, pc.shape[<span class="number">1</span>], -<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> torch.cat((pc, z), -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_latent_size</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.latent_size</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">base_network</span>(<span class="params">pointnet_radius, pointnet_nclusters, scale, in_features</span>):</span></span><br><span class="line">    sa1_module = pointnet2.PointnetSAModule(</span><br><span class="line">        npoint=pointnet_nclusters,</span><br><span class="line">        radius=pointnet_radius,</span><br><span class="line">        nsample=<span class="number">64</span>,</span><br><span class="line">        mlp=[in_features, <span class="number">64</span> * scale, <span class="number">64</span> * scale, <span class="number">128</span> * scale])</span><br><span class="line">    sa2_module = pointnet2.PointnetSAModule(</span><br><span class="line">        npoint=<span class="number">32</span>,</span><br><span class="line">        radius=<span class="number">0.04</span>,</span><br><span class="line">        nsample=<span class="number">128</span>,</span><br><span class="line">        mlp=[<span class="number">128</span> * scale, <span class="number">128</span> * scale, <span class="number">128</span> * scale, <span class="number">256</span> * scale])</span><br><span class="line"></span><br><span class="line">    sa3_module = pointnet2.PointnetSAModule(</span><br><span class="line">        mlp=[<span class="number">256</span> * scale, <span class="number">256</span> * scale, <span class="number">256</span> * scale, <span class="number">512</span> * scale])</span><br><span class="line"></span><br><span class="line">    sa_modules = nn.ModuleList([sa1_module, sa2_module, sa3_module])</span><br><span class="line">    fc_layer = nn.Sequential(nn.Linear(<span class="number">512</span> * scale, <span class="number">1024</span> * scale),</span><br><span class="line">                             nn.BatchNorm1d(<span class="number">1024</span> * scale), nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">                             nn.Linear(<span class="number">1024</span> * scale, <span class="number">1024</span> * scale),</span><br><span class="line">                             nn.BatchNorm1d(<span class="number">1024</span> * scale), nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([sa_modules, fc_layer])</span><br></pre></td></tr></table></figure>

<p>​    如下就是类似地GraspSampleVAE的代码，相对比较容易理解。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GraspSamplerVAE</span>(<span class="params">GraspSampler</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Network for learning a generative VAE grasp-sampler</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># omit some functions</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_encoder</span>(<span class="params">self, model_scale, pointnet_radius, pointnet_nclusters</span>):</span></span><br><span class="line">        <span class="comment"># The number of input features for the encoder is 19: the x, y, z</span></span><br><span class="line">        <span class="comment"># position of the point-cloud and the flattened 4x4=16 grasp pose matrix</span></span><br><span class="line">        <span class="comment"># 其实就是创建了一个接收 N * 19, 输出1024的一个PointNet++ Encoder</span></span><br><span class="line">        self.encoder = base_network(pointnet_radius, pointnet_nclusters, model_scale, <span class="number">19</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_bottleneck</span>(<span class="params">self, input_size, latent_size</span>):</span></span><br><span class="line">        <span class="comment"># 创建了均值向量和方差向量</span></span><br><span class="line">        mu = nn.Linear(input_size, latent_size)</span><br><span class="line">        logvar = nn.Linear(input_size, latent_size)</span><br><span class="line">        self.latent_space = nn.ModuleList([mu, logvar])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">self, xyz, xyz_features</span>):</span></span><br><span class="line">        <span class="keyword">for</span> module <span class="keyword">in</span> self.encoder[<span class="number">0</span>]:</span><br><span class="line">            xyz, xyz_features = module(xyz, xyz_features)</span><br><span class="line">        <span class="keyword">return</span> self.encoder[<span class="number">1</span>](xyz_features.squeeze(-<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, pc, grasp=<span class="literal">None</span>, train=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="keyword">if</span> train:</span><br><span class="line">            <span class="keyword">return</span> self.forward_train(pc, grasp)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.forward_test(pc, grasp)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward_train</span>(<span class="params">self, pc, grasp</span>):</span></span><br><span class="line">        <span class="comment"># 在训练的时候，确实需要通过重参数化对z采样，这样梯度才能回传</span></span><br><span class="line">        input_features = torch.cat(</span><br><span class="line">            (pc, grasp.unsqueeze(<span class="number">1</span>).expand(-<span class="number">1</span>, pc.shape[<span class="number">1</span>], -<span class="number">1</span>)),</span><br><span class="line">            -<span class="number">1</span>).transpose(-<span class="number">1</span>, <span class="number">1</span>).contiguous()</span><br><span class="line">        z = self.encode(pc, input_features)</span><br><span class="line">        mu, logvar = self.bottleneck(z)</span><br><span class="line">        z = self.reparameterize(mu, logvar)</span><br><span class="line">        qt, confidence = self.decode(pc, z)</span><br><span class="line">        <span class="keyword">return</span> qt, confidence, mu, logvar</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward_test</span>(<span class="params">self, pc, grasp</span>):</span></span><br><span class="line">        <span class="comment"># 在测试的时候，可以直接用均值来代替隐变量z</span></span><br><span class="line">        input_features = torch.cat(</span><br><span class="line">            (pc, grasp.unsqueeze(<span class="number">1</span>).expand(-<span class="number">1</span>, pc.shape[<span class="number">1</span>], -<span class="number">1</span>)),</span><br><span class="line">            -<span class="number">1</span>).transpose(-<span class="number">1</span>, <span class="number">1</span>).contiguous()</span><br><span class="line">        z = self.encode(pc, input_features)</span><br><span class="line">        mu, _ = self.bottleneck(z)</span><br><span class="line">        qt, confidence = self.decode(pc, mu)</span><br><span class="line">        <span class="keyword">return</span> qt, confidence</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample_latent</span>(<span class="params">self, batch_size</span>):</span></span><br><span class="line">        <span class="keyword">return</span> torch.randn(batch_size, self.latent_size).to(self.device)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate_grasps</span>(<span class="params">self, pc, z=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># 这个就是在inference阶段用的了，传入点云和采样到的z，可以直接把对应的grasp给预测出来</span></span><br><span class="line">        <span class="keyword">if</span> z <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            z = self.sample_latent(pc.shape[<span class="number">0</span>])</span><br><span class="line">        qt, confidence = self.decode(pc, z)</span><br><span class="line">        <span class="keyword">return</span> qt, confidence, z.squeeze()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate_dense_latents</span>(<span class="params">self, resolution</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        For the VAE sampler we consider dense latents to correspond to those between -2 and 2</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        latents = torch.meshgrid(*[</span><br><span class="line">            torch.linspace(-<span class="number">2</span>, <span class="number">2</span>, resolution) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.latent_size)</span><br><span class="line">        ])</span><br><span class="line">        <span class="keyword">return</span> torch.stack([latents[i].flatten() <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(latents))],</span><br><span class="line">                           dim=-<span class="number">1</span>).to(self.device)</span><br></pre></td></tr></table></figure>

<p>​    GraspSamplerGAN因为涉及到另一篇文章的优化，此处不再扩展。</p>
<h3 id="GraspNet-Evaluator"><a href="#GraspNet-Evaluator" class="headerlink" title="GraspNet Evaluator"></a>GraspNet Evaluator</h3><p>​    其实这部分就是一个PointNet++提特征的二分类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GraspEvaluator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,</span></span></span><br><span class="line"><span class="params"><span class="function">                 model_scale=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 pointnet_radius=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 pointnet_nclusters=<span class="number">128</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 device=<span class="string">&quot;cpu&quot;</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(GraspEvaluator, self).__init__()</span><br><span class="line">        self.create_evaluator(pointnet_radius, model_scale, pointnet_nclusters)</span><br><span class="line">        self.device = device</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_evaluator</span>(<span class="params">self, pointnet_radius, model_scale,</span></span></span><br><span class="line"><span class="params"><span class="function">                         pointnet_nclusters</span>):</span></span><br><span class="line">        <span class="comment"># The number of input features for the evaluator is 4: the x, y, z</span></span><br><span class="line">        <span class="comment"># position of the concatenated gripper and object point-clouds and an</span></span><br><span class="line">        <span class="comment"># extra binary feature, which is 0 for the object and 1 for the gripper,</span></span><br><span class="line">        <span class="comment"># to tell these point-clouds apart</span></span><br><span class="line">        self.evaluator = base_network(pointnet_radius, pointnet_nclusters,</span><br><span class="line">                                      model_scale, <span class="number">4</span>)</span><br><span class="line">        self.predictions_logits = nn.Linear(<span class="number">1024</span> * model_scale, <span class="number">1</span>)</span><br><span class="line">        self.confidence = nn.Linear(<span class="number">1024</span> * model_scale, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span>(<span class="params">self, xyz, xyz_features</span>):</span></span><br><span class="line">        <span class="keyword">for</span> module <span class="keyword">in</span> self.evaluator[<span class="number">0</span>]:</span><br><span class="line">            xyz, xyz_features = module(xyz, xyz_features)</span><br><span class="line">        <span class="keyword">return</span> self.evaluator[<span class="number">1</span>](xyz_features.squeeze(-<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, pc, gripper_pc, train=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="comment"># 把原始点云和夹爪点云融合在一起</span></span><br><span class="line">        pc, pc_features = self.merge_pc_and_gripper_pc(pc, gripper_pc)</span><br><span class="line">        x = self.evaluate(pc, pc_features.contiguous())</span><br><span class="line">        <span class="comment"># 过</span></span><br><span class="line">        <span class="keyword">return</span> self.predictions_logits(x), torch.sigmoid(self.confidence(x))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">merge_pc_and_gripper_pc</span>(<span class="params">self, pc, gripper_pc</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Merges the object point cloud and gripper point cloud and</span></span><br><span class="line"><span class="string">        adds a binary auxiliary feature that indicates whether each point</span></span><br><span class="line"><span class="string">        belongs to the object or to the gripper.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        pc_shape = pc.shape</span><br><span class="line">        gripper_shape = gripper_pc.shape</span><br><span class="line">        <span class="keyword">assert</span> (<span class="built_in">len</span>(pc_shape) == <span class="number">3</span>)</span><br><span class="line">        <span class="keyword">assert</span> (<span class="built_in">len</span>(gripper_shape) == <span class="number">3</span>)</span><br><span class="line">        <span class="keyword">assert</span> (pc_shape[<span class="number">0</span>] == gripper_shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">        npoints = pc_shape[<span class="number">1</span>]</span><br><span class="line">        batch_size = pc_shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        l0_xyz = torch.cat((pc, gripper_pc), <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 先把两个点云接在一起</span></span><br><span class="line">        labels = [</span><br><span class="line">            torch.ones(pc.shape[<span class="number">1</span>], <span class="number">1</span>, dtype=torch.float32),</span><br><span class="line">            torch.zeros(gripper_pc.shape[<span class="number">1</span>], <span class="number">1</span>, dtype=torch.float32)</span><br><span class="line">        ]</span><br><span class="line">        labels = torch.cat(labels, <span class="number">0</span>)</span><br><span class="line">        labels.unsqueeze_(<span class="number">0</span>)</span><br><span class="line">        labels = labels.repeat(batch_size, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">		<span class="comment"># 把标志着是否是原点云的特征接在点云后</span></span><br><span class="line">        l0_points = torch.cat([l0_xyz, labels.to(self.device)],</span><br><span class="line">                              -<span class="number">1</span>).transpose(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> l0_xyz, l0_points</span><br></pre></td></tr></table></figure>

<h3 id="Iterative-Grasp-Pose-Refinement-1"><a href="#Iterative-Grasp-Pose-Refinement-1" class="headerlink" title="Iterative Grasp Pose Refinement"></a>Iterative Grasp Pose Refinement</h3><p>​    这部分代码比较复杂，也非常重要。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GraspEstimator</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">      Includes the code used for running the inference.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, grasp_sampler_opt, grasp_evaluator_opt, opt</span>):</span></span><br><span class="line">        self.grasp_sampler_opt = grasp_sampler_opt</span><br><span class="line">        self.grasp_evaluator_opt = grasp_evaluator_opt</span><br><span class="line">        self.opt = opt</span><br><span class="line">        self.target_pc_size = opt.target_pc_size</span><br><span class="line">        self.num_refine_steps = opt.refine_steps</span><br><span class="line">        self.refine_method = opt.refinement_method</span><br><span class="line">        self.threshold = opt.threshold</span><br><span class="line">        self.batch_size = opt.batch_size</span><br><span class="line">        self.generate_dense_grasps = opt.generate_dense_grasps</span><br><span class="line">        <span class="keyword">if</span> self.generate_dense_grasps:</span><br><span class="line">            self.num_grasps_per_dim = opt.num_grasp_samples</span><br><span class="line">            self.num_grasp_samples = opt.num_grasp_samples * opt.num_grasp_samples</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.num_grasp_samples = opt.num_grasp_samples</span><br><span class="line">        self.choose_fn = opt.choose_fn</span><br><span class="line">        self.choose_fns = &#123;</span><br><span class="line">            <span class="string">&quot;all&quot;</span>:</span><br><span class="line">            <span class="literal">None</span>,</span><br><span class="line">            <span class="string">&quot;better_than_threshold&quot;</span>:</span><br><span class="line">            utils.choose_grasps_better_than_threshold,</span><br><span class="line">            <span class="string">&quot;better_than_threshold_in_sequence&quot;</span>:</span><br><span class="line">            utils.choose_grasps_better_than_threshold_in_sequence,</span><br><span class="line">        &#125;</span><br><span class="line">        self.device = torch.device(<span class="string">&quot;cuda:0&quot;</span>)</span><br><span class="line">        self.grasp_evaluator = create_model(grasp_evaluator_opt)</span><br><span class="line">        self.grasp_sampler = create_model(grasp_sampler_opt)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">keep_inliers</span>(<span class="params">self, grasps, confidences, z, pc, inlier_indices_list</span>):</span></span><br><span class="line">        <span class="keyword">for</span> i, inlier_indices <span class="keyword">in</span> <span class="built_in">enumerate</span>(inlier_indices_list):</span><br><span class="line">            grasps[i] = grasps[i][inlier_indices]</span><br><span class="line">            confidences[i] = confidences[i][inlier_indices]</span><br><span class="line">            z[i] = z[i][inlier_indices]</span><br><span class="line">            pc[i] = pc[i][inlier_indices]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate_and_refine_grasps</span>(<span class="params">self, pc</span>):</span></span><br><span class="line">        pc_list, pc_mean = self.prepare_pc(pc)</span><br><span class="line">        grasps_list, confidence_list, z_list = self.generate_grasps(pc_list)</span><br><span class="line">        inlier_indices = utils.get_inlier_grasp_indices(grasps_list,</span><br><span class="line">                                                        torch.zeros(<span class="number">1</span>, <span class="number">3</span>).to(</span><br><span class="line">                                                            self.device),</span><br><span class="line">                                                        threshold=<span class="number">1.0</span>,</span><br><span class="line">                                                        device=self.device)</span><br><span class="line">        self.keep_inliers(grasps_list, confidence_list, z_list, pc_list, inlier_indices)</span><br><span class="line">        improved_eulers, improved_ts, improved_success = [], [], []</span><br><span class="line">        <span class="keyword">for</span> pc, grasps <span class="keyword">in</span> <span class="built_in">zip</span>(pc_list, grasps_list):</span><br><span class="line">            out = self.refine_grasps(pc, grasps, self.refine_method,</span><br><span class="line">                                     self.num_refine_steps)</span><br><span class="line">            improved_eulers.append(out[<span class="number">0</span>])</span><br><span class="line">            improved_ts.append(out[<span class="number">1</span>])</span><br><span class="line">            improved_success.append(out[<span class="number">2</span>])</span><br><span class="line">        improved_eulers = np.hstack(improved_eulers)</span><br><span class="line">        improved_ts = np.hstack(improved_ts)</span><br><span class="line">        improved_success = np.hstack(improved_success)</span><br><span class="line">        <span class="keyword">if</span> self.choose_fn <span class="keyword">is</span> <span class="string">&quot;all&quot;</span>:</span><br><span class="line">            selection_mask = np.ones(improved_success.shape, dtype=np.float32)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            selection_mask = self.choose_fns[self.choose_fn](improved_eulers,</span><br><span class="line">                                                             improved_ts,</span><br><span class="line">                                                             improved_success,</span><br><span class="line">                                                             self.threshold)</span><br><span class="line">        grasps = utils.rot_and_trans_to_grasps(improved_eulers, improved_ts,</span><br><span class="line">                                               selection_mask)</span><br><span class="line">        utils.denormalize_grasps(grasps, pc_mean)</span><br><span class="line">        refine_indexes, sample_indexes = np.where(selection_mask)</span><br><span class="line">        success_prob = improved_success[refine_indexes,</span><br><span class="line">                                        sample_indexes].tolist()</span><br><span class="line">        <span class="keyword">return</span> grasps, success_prob</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prepare_pc</span>(<span class="params">self, pc</span>):</span></span><br><span class="line">        <span class="keyword">if</span> pc.shape[<span class="number">0</span>] &gt; self.target_pc_size:</span><br><span class="line">            pc = utils.regularize_pc_point_count(pc, self.target_pc_size)</span><br><span class="line">        pc_mean = np.mean(pc, <span class="number">0</span>)</span><br><span class="line">        pc -= np.expand_dims(pc_mean, <span class="number">0</span>)</span><br><span class="line">        pc = np.tile(pc, (self.num_grasp_samples, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        pc = torch.from_numpy(pc).<span class="built_in">float</span>().to(self.device)</span><br><span class="line">        pcs = []</span><br><span class="line">        pcs = utils.partition_array_into_subarrays(pc, self.batch_size)</span><br><span class="line">        <span class="keyword">return</span> pcs, pc_mean</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate_grasps</span>(<span class="params">self, pcs</span>):</span></span><br><span class="line">        all_grasps = []</span><br><span class="line">        all_confidence = []</span><br><span class="line">        all_z = []</span><br><span class="line">        <span class="keyword">if</span> self.generate_dense_grasps:</span><br><span class="line">            latent_samples = self.grasp_sampler.net.module.generate_dense_latents(</span><br><span class="line">                self.num_grasps_per_dim) <span class="comment"># 对标准高斯分布采样</span></span><br><span class="line">            latent_samples = utils.partition_array_into_subarrays(</span><br><span class="line">                latent_samples, self.batch_size)</span><br><span class="line">            <span class="keyword">for</span> latent_sample, pc <span class="keyword">in</span> <span class="built_in">zip</span>(latent_samples, pcs):	</span><br><span class="line">                grasps, confidence, z = self.grasp_sampler.generate_grasps(</span><br><span class="line">                    pc, latent_sample)</span><br><span class="line">                all_grasps.append(grasps)</span><br><span class="line">                all_confidence.append(confidence)</span><br><span class="line">                all_z.append(z)</span><br><span class="line">                <span class="comment"># 对每个点云获得对应的抓取</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> pc <span class="keyword">in</span> pcs:</span><br><span class="line">                grasps, confidence, z = self.grasp_sampler.generate_grasps(pc)</span><br><span class="line">                all_grasps.append(grasps)</span><br><span class="line">                all_confidence.append(confidence)</span><br><span class="line">                all_z.append(z)</span><br><span class="line">        <span class="keyword">return</span> all_grasps, all_confidence, all_z</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">refine_grasps</span>(<span class="params">self, pc, grasps, refine_method, num_refine_steps=<span class="number">10</span></span>):</span></span><br><span class="line"></span><br><span class="line">        grasp_eulers, grasp_translations = utils.convert_qt_to_rt(grasps)</span><br><span class="line">        <span class="keyword">if</span> refine_method == <span class="string">&quot;gradient&quot;</span>:</span><br><span class="line">            improve_fun = self.improve_grasps_gradient_based</span><br><span class="line">            grasp_eulers = torch.autograd.Variable(grasp_eulers.to(</span><br><span class="line">                self.device),requires_grad=<span class="literal">True</span>)</span><br><span class="line">            grasp_translations = torch.autograd.Variable(grasp_translations.to(</span><br><span class="line">                self.device),requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            improve_fun = self.improve_grasps_sampling_based</span><br><span class="line"></span><br><span class="line">        improved_success = []</span><br><span class="line">        improved_eulers = []</span><br><span class="line">        improved_ts = []</span><br><span class="line">        improved_eulers.append(grasp_eulers.cpu().data.numpy())</span><br><span class="line">        improved_ts.append(grasp_translations.cpu().data.numpy())</span><br><span class="line">        last_success = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_refine_steps):</span><br><span class="line">            <span class="comment"># 对于每个grasp都通过improve_fun来提高成功率</span></span><br><span class="line">            success_prob, last_success = improve_fun(pc, grasp_eulers,</span><br><span class="line">                                                     grasp_translations,</span><br><span class="line">                                                     last_success)</span><br><span class="line">            improved_success.append(success_prob.cpu().data.numpy())</span><br><span class="line">            improved_eulers.append(grasp_eulers.cpu().data.numpy())</span><br><span class="line">            improved_ts.append(grasp_translations.cpu().data.numpy())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># we need to run the success on the final improved grasps</span></span><br><span class="line">        grasp_pcs = utils.control_points_from_rot_and_trans(</span><br><span class="line">            grasp_eulers, grasp_translations, self.device)</span><br><span class="line">        improved_success.append(</span><br><span class="line">            self.grasp_evaluator.evaluate_grasps(</span><br><span class="line">                pc, grasp_pcs).squeeze().cpu().data.numpy())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> np.asarray(improved_eulers), np.asarray(</span><br><span class="line">            improved_ts), np.asarray(improved_success)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">improve_grasps_gradient_based</span>(<span class="params">self, pcs, grasp_eulers, grasp_trans, last_success</span>):</span>  <span class="comment">#euler_angles, translation, eval_and_improve, metadata):</span></span><br><span class="line">        grasp_pcs = utils.control_points_from_rot_and_trans(</span><br><span class="line">            grasp_eulers, grasp_trans, self.device)</span><br><span class="line"></span><br><span class="line">        success = self.grasp_evaluator.evaluate_grasps(pcs, grasp_pcs)</span><br><span class="line">        success.squeeze().backward(</span><br><span class="line">            torch.ones(success.shape[<span class="number">0</span>]).to(self.device))</span><br><span class="line">        delta_t = grasp_trans.grad</span><br><span class="line">        norm_t = torch.norm(delta_t, p=<span class="number">2</span>, dim=-<span class="number">1</span>).to(self.device)</span><br><span class="line">        <span class="comment"># Adjust the alpha so that it won&#x27;t update more than 1 cm. Gradient is only valid</span></span><br><span class="line">        <span class="comment"># in small neighborhood.</span></span><br><span class="line">        alpha = torch.<span class="built_in">min</span>(<span class="number">0.01</span> / norm_t, torch.tensor(<span class="number">1.0</span>).to(self.device))</span><br><span class="line">        <span class="comment"># 这里就直接对grasp的pos和orn做梯度上升</span></span><br><span class="line">        grasp_trans.data += grasp_trans.grad * alpha[:, <span class="literal">None</span>]</span><br><span class="line">        temp = grasp_eulers.clone()</span><br><span class="line">        grasp_eulers.data += grasp_eulers.grad * alpha[:, <span class="literal">None</span>]</span><br><span class="line">        <span class="keyword">return</span> success.squeeze(), <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">improve_grasps_sampling_based</span>(<span class="params">self, pcs, grasp_eulers, grasp_trans, last_success=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="keyword">if</span> last_success <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                grasp_pcs = utils.control_points_from_rot_and_trans(</span><br><span class="line">                    grasp_eulers, grasp_trans, self.device)</span><br><span class="line">                last_success = self.grasp_evaluator.evaluate_grasps(</span><br><span class="line">                    pcs, grasp_pcs)</span><br><span class="line"></span><br><span class="line">            delta_t = <span class="number">2</span> * (torch.rand(grasp_trans.shape).to(self.device) - <span class="number">0.5</span>)</span><br><span class="line">            delta_t *= <span class="number">0.02</span></span><br><span class="line">            delta_euler_angles = (torch.rand(grasp_eulers.shape).to(self.device) - <span class="number">0.5</span>) * <span class="number">2</span></span><br><span class="line">            <span class="comment"># 基于采样的算法就要如上进行采样，然后计算出优化以后的grasp</span></span><br><span class="line">            </span><br><span class="line">            perturbed_translation = grasp_trans + delta_t</span><br><span class="line">            perturbed_euler_angles = grasp_eulers + delta_euler_angles</span><br><span class="line">            grasp_pcs = utils.control_points_from_rot_and_trans(</span><br><span class="line">                perturbed_euler_angles, perturbed_translation, self.device)</span><br><span class="line"></span><br><span class="line">            perturbed_success = self.grasp_evaluator.evaluate_grasps(pcs, grasp_pcs)</span><br><span class="line">            ratio = perturbed_success / torch.<span class="built_in">max</span>(last_success, torch.tensor(<span class="number">0.0001</span>).to(self.device))</span><br><span class="line">            <span class="comment"># 丢到Estimator里看看有没有增加成功率</span></span><br><span class="line">            mask = torch.rand(ratio.shape).to(self.device) &lt;= ratio</span><br><span class="line">            next_success = last_success</span><br><span class="line">            ind = torch.where(mask)[<span class="number">0</span>]</span><br><span class="line">            next_success[ind] = perturbed_success[ind]</span><br><span class="line">            grasp_trans[ind].data = perturbed_translation.data[ind]</span><br><span class="line">            grasp_eulers[ind].data = perturbed_euler_angles.data[ind]</span><br><span class="line">            <span class="keyword">return</span> last_success.squeeze(), next_success</span><br></pre></td></tr></table></figure>

</div><div class="article-licensing box"><div class="licensing-title"><p>(ICCV2019)6D-GraspNet</p><p><a href="https://kami-code.com/2022/01/17/6D-GraspNet/">https://kami-code.com/2022/01/17/6D-GraspNet/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Kami-code</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2022-01-17</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2022-02-19</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/GraspNet/">GraspNet</a><a class="link-muted mr-2" rel="tag" href="/tags/ICCV/">ICCV</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2022/01/18/GraspNet-1Billion/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">(CVPR2020)GraspNet-1Billion</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2022/01/17/machine-learning-pre/"><span class="level-item">VoteNet presentation</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div id="SOHUCS" sid="2022/01/17/6D-GraspNet/"></div><script charset="utf-8" src="https://changyan.sohu.com/upload/changyan.js"></script><script>window.changyan.api.config({appid: 'cyvI88c19',conf: 'prod_634561f1ec380218934dcf2c12b8b70b'});</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.jpg" alt="Chen Bao"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Chen Bao</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shanghai, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">32</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">0</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">39</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Kami-code" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Kami-code"><i class="fab fa-github"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><!--!--><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-03-16T15:28:44.000Z">2022-03-16</time></p><p class="title"><a href="/2022/03/16/DEXMV/">DEXMV</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-03-16T15:28:35.000Z">2022-03-16</time></p><p class="title"><a href="/2022/03/16/SAPIEN-tutorial/">SAPIEN_tutorial</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-03-13T08:18:21.000Z">2022-03-13</time></p><p class="title"><a href="/2022/03/13/GraspTTA/">(ICCV2021)Hand-Object Contact Consistency Reasoning for Human Grasps Generation</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-03-02T06:24:03.000Z">2022-03-02</time></p><p class="title"><a href="/2022/03/02/NOCS/">(CVPR2019)NOCS</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-02-20T14:35:48.000Z">2022-02-20</time></p><p class="title"><a href="/2022/02/20/A-SDF/">(ICCV2021)A-SDF</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2022/03/"><span class="level-start"><span class="level-item">March 2022</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/02/"><span class="level-start"><span class="level-item">February 2022</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/01/"><span class="level-start"><span class="level-item">January 2022</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/12/"><span class="level-start"><span class="level-item">December 2021</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/11/"><span class="level-start"><span class="level-item">November 2021</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/10/"><span class="level-start"><span class="level-item">October 2021</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Affordance/"><span class="tag">Affordance</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/BCO/"><span class="tag">BCO</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CHOMP/"><span class="tag">CHOMP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CVPR/"><span class="tag">CVPR</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Camera/"><span class="tag">Camera</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Docker/"><span class="tag">Docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GraspNet/"><span class="tag">GraspNet</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ICCV/"><span class="tag">ICCV</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ICRA/"><span class="tag">ICRA</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/IJCAI/"><span class="tag">IJCAI</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/IROS/"><span class="tag">IROS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MongoDB/"><span class="tag">MongoDB</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MySQL/"><span class="tag">MySQL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Neo4j/"><span class="tag">Neo4j</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OpenCV/"><span class="tag">OpenCV</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/OpenGL/"><span class="tag">OpenGL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PointNet/"><span class="tag">PointNet</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/PointNet/"><span class="tag">PointNet++</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Pybind11/"><span class="tag">Pybind11</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Pybullet/"><span class="tag">Pybullet</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ROS/"><span class="tag">ROS</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RSS/"><span class="tag">RSS</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SE3353/"><span class="tag">SE3353</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SOIL/"><span class="tag">SOIL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ScrewNet/"><span class="tag">ScrewNet</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Search/"><span class="tag">Search</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VoteNet/"><span class="tag">VoteNet</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/WebService/"><span class="tag">WebService</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/behavior-tree/"><span class="tag">behavior tree</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/compliers/"><span class="tag">compliers</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/deeplearning/"><span class="tag">deeplearning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hidden/"><span class="tag">hidden</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ikfast/"><span class="tag">ikfast</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/lab/"><span class="tag">lab</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/mmdection/"><span class="tag">mmdection</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/moveit/"><span class="tag">moveit</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/planning/"><span class="tag">planning</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pybullet/"><span class="tag">pybullet</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/segmentation/"><span class="tag">segmentation</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Ryan &#039;s website" height="28"></a><span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span><span class="post-meta-divider">|</span><span id="busuanzi_container_site_uv" style="display:none">本站访客数<span id="busuanzi_value_site_uv"></span>人</span><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><p class="is-size-7"><span>&copy; 2022 Kami-code</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>